{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Προχωρημένα Θέματα Βάσεων Δεδομένων\n",
    "\n",
    "**Ονοματεπώνυμο:** Κωνσταντίνος Διβριώτης\n",
    "\n",
    "**ΑΜ:** 03114140\n",
    "\n",
    "## Query 3: \n",
    "\n",
    "Χρησιμοποιώντας ως αναφορά τα δεδομένα της απογραφής 2010 για τον πληθυσμό και εκείνα της απογραφής του 2015 για το εισόδημα ανα νοικοκυριό, να υπολογίσετε για κάθε περιοχή του Los Angeles τα παρακάτω:\n",
    "- Το μέσο ετήσιο εισόδημα ανά άτομο\n",
    "- Την αναλογία συνολικού αριθμού εγκλημάτων ανά άτομο"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2uaG6GoZthpT",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2499</td><td>application_1732639283265_2458</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2458/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-166.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2458_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.spark import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"CensusDataAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_BUCKET = \"s3://initial-notebook-data-bucket-dblab-905418150721\"\n",
    "GROUP_BUCKET = \"s3://groups-bucket-dblab-905418150721/group15\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Διάβασμα και Επισκόπηση αρχείων εισόδου"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+\n",
      "|Zip Code|Estimated Median Income|\n",
      "+--------+-----------------------+\n",
      "|   90001|                33887.0|\n",
      "|   90002|                30413.0|\n",
      "|   90003|                30805.0|\n",
      "|   90004|                40612.0|\n",
      "|   90005|                31142.0|\n",
      "+--------+-----------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "income_schema = StructType([\n",
    "    StructField(\"Zip Code\", StringType()),\n",
    "    StructField(\"Community\", StringType()),\n",
    "    StructField(\"Estimated Median Income\", StringType())\n",
    "])\n",
    "\n",
    "income_data = spark.read.csv(f\"{DATA_BUCKET}/LA_income_2015.csv\", header=True, schema=income_schema)\n",
    "\n",
    "# Μετατροπή του Estimated Median Income σε αριθμητική μορφή\n",
    "income_data = income_data \\\n",
    "    .withColumn(\n",
    "        \"Estimated Median Income\",\n",
    "        regexp_replace(col(\"Estimated Median Income\"), \"[$,]\", \"\").cast(\"float\")\n",
    "    ) \\\n",
    "    .select(\"Zip Code\", \"Estimated Median Income\")\n",
    "\n",
    "income_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|    DR_NO|                geom|\n",
      "+---------+--------------------+\n",
      "|001307355|POINT (-118.2695 ...|\n",
      "|011401303|POINT (-118.3962 ...|\n",
      "|070309629|POINT (-118.2524 ...|\n",
      "|090631215|POINT (-118.3295 ...|\n",
      "|100100501|POINT (-118.2488 ...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, DoubleType\n",
    "\n",
    "# Ορισμός του schema των dataset\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", IntegerType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", IntegerType()),\n",
    "    StructField(\"Crm Cd\", IntegerType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", StringType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", IntegerType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", IntegerType()),\n",
    "    StructField(\"Crm Cd 2\", IntegerType()),\n",
    "    StructField(\"Crm Cd 3\", IntegerType()),\n",
    "    StructField(\"Crm Cd 4\", IntegerType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", DoubleType()),\n",
    "    StructField(\"LON\", DoubleType())\n",
    "])\n",
    "\n",
    "# Διαβάζουμε τα 2 datasets (2010-2019 και 2020-σήμερα) και τα συνενώνουμε σε 1\n",
    "crime_data_2010_2019 = spark.read.csv(f\"{DATA_BUCKET}/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, schema=crimes_schema)\n",
    "crime_data_2020_present = spark.read.csv(f\"{DATA_BUCKET}/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", header=True, schema=crimes_schema)\n",
    "crime_data = crime_data_2010_2019.union(crime_data_2020_present)\n",
    "\n",
    "# Μετατρέπουμε τις στήλες LAT, LON σε geometry με το ST_POINT\n",
    "crime_data = crime_data \\\n",
    "                .withColumn(\"geom\", ST_Point(\"LON\", \"LAT\")) \\\n",
    "                .filter(col(\"geom\") != ST_Point(0, 0)) \\\n",
    "                .select(\"DR_NO\", \"geom\")\n",
    "\n",
    "crime_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---------+--------+--------------------+\n",
      "|     COMM|ZCTA10|HOUSING10|POP_2010|            geometry|\n",
      "+---------+------+---------+--------+--------------------+\n",
      "|San Pedro| 90732|       26|      69|POLYGON ((-118.31...|\n",
      "|San Pedro| 90731|       70|     120|POLYGON ((-118.28...|\n",
      "|San Pedro| 90731|       86|     240|POLYGON ((-118.29...|\n",
      "|San Pedro| 90732|       29|      75|POLYGON ((-118.31...|\n",
      "|San Pedro| 90731|       80|     246|POLYGON ((-118.28...|\n",
      "+---------+------+---------+--------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\") \\\n",
    "            .load(f\"{DATA_BUCKET}/2010_Census_Blocks.geojson\") \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "blocks_data = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                    blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\") \\\n",
    "            .filter(\n",
    "                col(\"COMM\").isNotNull() & (col(\"CITY\") == \"Los Angeles\") &\n",
    "                (col(\"HOUSING10\") > 0) & (col(\"POP_2010\") > 0)\n",
    "            ) \\\n",
    "            .select(\"COMM\", \"ZCTA10\", \"HOUSING10\", \"POP_2010\", \"geometry\")\n",
    "\n",
    "blocks_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- COMM: string (nullable = true)\n",
      " |-- ZCTA10: string (nullable = true)\n",
      " |-- HOUSING10: long (nullable = true)\n",
      " |-- POP_2010: long (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)"
     ]
    }
   ],
   "source": [
    "blocks_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------------------------------------------------------------------+\n",
      "|field    |type    |meaning                                                                  |\n",
      "+---------+--------+-------------------------------------------------------------------------+\n",
      "|COMM     |string  |Unincorporated area community name and LA City neighborhood              |\n",
      "|HOUSING10|long    |2010 housing (PL 94-171 Redistricting Data Summary File - Total Housing) |\n",
      "|POP_2010 |long    |Population (PL 94-171 Redistricting Data Summary File - Total Population)|\n",
      "|ZCTA10   |string  |Zip Code Tabulation Area                                                 |\n",
      "|geometry |geometry|Geometry of the block                                                    |\n",
      "+---------+--------+-------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "blocks_data_description_schema = StructType([\n",
    "    StructField(\"field\", StringType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"meaning\", StringType())\n",
    "])\n",
    "\n",
    "blocks_data_description = spark.read.csv(f\"{DATA_BUCKET}/2010_Census_Blocks_fields.csv\", header=True, schema=blocks_data_description_schema)\n",
    "\n",
    "blocks_data_description \\\n",
    "        .filter(col(\"field\").isin(\"COMM\", \"ZCTA10\", \"HOUSING10\", \"POP_2010\", \"geometry\")) \\\n",
    "        .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Υλοποίηση με DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------+-------------------+\n",
      "|COMM                   |Income per Person |Crimes per Person  |\n",
      "+-----------------------+------------------+-------------------+\n",
      "|Adams-Normandie        |8791.458301453711 |0.7148686559551135 |\n",
      "|Alsace                 |11239.50136425648 |0.5416098226466576 |\n",
      "|Angeles National Forest|33079.58823529412 |0.4117647058823529 |\n",
      "|Angelino Heights       |18427.059814658805|0.5732940185341197 |\n",
      "|Arleta                 |12110.779474388612|0.4264509064363061 |\n",
      "|Atwater Village        |28481.236967160792|0.5288318320448259 |\n",
      "|Baldwin Hills          |17303.906408241663|0.9950061114021302 |\n",
      "|Bel Air                |63041.33942621959 |0.39922527539038855|\n",
      "|Beverly Crest          |60947.48978754819 |0.3689607087195472 |\n",
      "|Beverlywood            |29267.82118405155 |0.5084977849375755 |\n",
      "|Boyle Heights          |8494.108286861341 |0.6171887393378809 |\n",
      "|Brentwood              |60846.854461055365|0.4058638814936173 |\n",
      "|Brookside              |18138.626409017714|0.8856682769726248 |\n",
      "|Cadillac-Corning       |19572.784696174043|0.581695423855964  |\n",
      "|Canoga Park            |19660.29173807544 |0.5506083179203503 |\n",
      "|Carthay                |49848.73592646053 |0.7628959963534149 |\n",
      "|Central                |6973.305663786775 |0.6593822350217403 |\n",
      "|Century City           |45617.760134566866|0.632968881412952  |\n",
      "|Century Palms/Cove     |8610.314655031003 |1.1446474853187232 |\n",
      "|Chatsworth             |30694.606443388573|0.5281029087959207 |\n",
      "+-----------------------+------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, col, max\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Για κάθε περιοχή του στο Census, υπάρχουν πολλαπλές\n",
    "# εγγραφές για COMM, ZCTA10. Συνεπώς, πριν κάνουμε το\n",
    "# join με το Income, πρέπει να κάνουμε group by για να\n",
    "# υπολογίσουμε το συνολικό πληθυσμό ανά COMM, ZIP Code\n",
    "result = blocks_data \\\n",
    "            .groupBy(\"COMM\", \"ZCTA10\") \\\n",
    "            .agg(\n",
    "                sum(\"POP_2010\").alias(\"Population\"),\n",
    "                sum(\"HOUSING10\").alias(\"Households\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "            )\n",
    "\n",
    "# Έπειτα κάνουμε join με το Income με βάση το ZIP Code,\n",
    "# οπότε έχουμε το μέσο ετήσιο εισόδημα ανά νοικοκυριό\n",
    "# για κάθε COMM, ZIP Code. Κάνουμε group by με το\n",
    "# COMM για να βρούμε το συνολικό εισόδημα και το\n",
    "# συνολικό πληθυσμό ανά COMM.\n",
    "result = result \\\n",
    "            .join(income_data, blocks_data[\"ZCTA10\"] == income_data[\"Zip Code\"]) \\\n",
    "            .groupBy(\"COMM\") \\\n",
    "            .agg(\n",
    "                sum(\"Population\").alias(\"Population\"),\n",
    "                sum(col(\"Estimated Median Income\") * col(\"Households\")).alias(\"Total Income\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "            )\n",
    "\n",
    "# Τέλος, κάνουμε join με τα εγκλήματα με βάση το geometry, δηλαδή\n",
    "# το POINT του εγκλήματος βρίσκεται εντός του POLYGON της περιοχής.\n",
    "# Κάνουμε group by με το COMM (και τα Population, Total Income τα οποία είναι\n",
    "# ίδια ανά εγγραφή) και υπολογίζουμε το συνολικό πλήθος των εγκλημάτων.\n",
    "# Μετά διαιρούμε με τον πληθυσμό για να βρούμε το εισόδημα ανά άτομο\n",
    "# και το πλήθος εγκλημάτων ανά άτομο.\n",
    "result = result \\\n",
    "            .join(crime_data, ST_Within(crime_data[\"geom\"], result[\"geometry\"]), \"inner\") \\\n",
    "            .groupBy(\"COMM\", \"Population\", \"Total Income\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"Total Crimes\")\n",
    "            ) \\\n",
    "            .withColumn(\"Income per Person\", col(\"Total Income\") / col(\"Population\")) \\\n",
    "            .withColumn(\"Crimes per Person\", col(\"Total Crimes\") / col(\"Population\")) \\\n",
    "            .orderBy(\"COMM\")\n",
    "\n",
    "result \\\n",
    "    .select(\"COMM\", \"Income per Person\", \"Crimes per Person\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 32.56 seconds"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα δοκιμάσουμε να αναγκάσουμε το Spark να χρησιμοποιήσει διαφορετικές στρατηγικές, ώστε να συγκρίνουμε την απόδοσή τους."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. BROADCAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------+-------------------+\n",
      "|COMM                   |Income per Person |Crimes per Person  |\n",
      "+-----------------------+------------------+-------------------+\n",
      "|Adams-Normandie        |8791.458301453711 |0.7148686559551135 |\n",
      "|Alsace                 |11239.50136425648 |0.5416098226466576 |\n",
      "|Angeles National Forest|33079.58823529412 |0.4117647058823529 |\n",
      "|Angelino Heights       |18427.059814658805|0.5732940185341197 |\n",
      "|Arleta                 |12110.779474388612|0.4264509064363061 |\n",
      "|Atwater Village        |28481.236967160792|0.5288318320448259 |\n",
      "|Baldwin Hills          |17303.906408241663|0.9950061114021302 |\n",
      "|Bel Air                |63041.33942621959 |0.39922527539038855|\n",
      "|Beverly Crest          |60947.48978754819 |0.3689607087195472 |\n",
      "|Beverlywood            |29267.82118405155 |0.5084977849375755 |\n",
      "|Boyle Heights          |8494.108286861341 |0.6171887393378809 |\n",
      "|Brentwood              |60846.854461055365|0.4058638814936173 |\n",
      "|Brookside              |18138.626409017714|0.8856682769726248 |\n",
      "|Cadillac-Corning       |19572.784696174043|0.581695423855964  |\n",
      "|Canoga Park            |19660.29173807544 |0.5506083179203503 |\n",
      "|Carthay                |49848.73592646053 |0.7628959963534149 |\n",
      "|Central                |6973.305663786775 |0.6593822350217403 |\n",
      "|Century City           |45617.760134566866|0.632968881412952  |\n",
      "|Century Palms/Cove     |8610.314655031003 |1.1446474853187232 |\n",
      "|Chatsworth             |30694.606443388573|0.5281029087959207 |\n",
      "+-----------------------+------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, col\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Για κάθε περιοχή του στο Census, υπάρχουν πολλαπλές\n",
    "# εγγραφές για COMM, ZCTA10. Συνεπώς, πριν κάνουμε το\n",
    "# join με το Income, πρέπει να κάνουμε group by για να\n",
    "# υπολογίσουμε το συνολικό πληθυσμό ανά COMM, ZIP Code\n",
    "result = blocks_data \\\n",
    "            .groupBy(\"COMM\", \"ZCTA10\") \\\n",
    "            .agg(\n",
    "                sum(\"POP_2010\").alias(\"Population\"),\n",
    "                sum(\"HOUSING10\").alias(\"Households\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "            )\n",
    "\n",
    "# Έπειτα κάνουμε join με το Income με βάση το ZIP Code,\n",
    "# οπότε έχουμε το μέσο ετήσιο εισόδημα ανά νοικοκυριό\n",
    "# για κάθε COMM, ZIP Code. Κάνουμε group by με το\n",
    "# COMM για να βρούμε το συνολικό εισόδημα και το\n",
    "# συνολικό πληθυσμό ανά COMM.\n",
    "result = result \\\n",
    "            .hint(\"BROADCAST\") \\\n",
    "            .join(income_data, blocks_data[\"ZCTA10\"] == income_data[\"Zip Code\"]) \\\n",
    "            .groupBy(\"COMM\") \\\n",
    "            .agg(\n",
    "                sum(\"Population\").alias(\"Population\"),\n",
    "                sum(col(\"Estimated Median Income\") * col(\"Households\")).alias(\"Total Income\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "            )\n",
    "\n",
    "# Τέλος, κάνουμε join με τα εγκλήματα με βάση το geometry, δηλαδή\n",
    "# το POINT του εγκλήματος βρίσκεται εντός του POLYGON της περιοχής.\n",
    "# Κάνουμε group by με το COMM (και τα Population, Total Income τα οποία είναι\n",
    "# ίδια ανά εγγραφή) και υπολογίζουμε το συνολικό πλήθος των εγκλημάτων.\n",
    "# Μετά διαιρούμε με τον πληθυσμό για να βρούμε το εισόδημα ανά άτομο\n",
    "# και το πλήθος εγκλημάτων ανά άτομο.\n",
    "result = result \\\n",
    "            .hint(\"BROADCAST\") \\\n",
    "            .join(crime_data, ST_Within(crime_data[\"geom\"], result[\"geometry\"]), \"inner\") \\\n",
    "            .groupBy(\"COMM\", \"Population\", \"Total Income\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"Total Crimes\")\n",
    "            ) \\\n",
    "            .withColumn(\"Income per Person\", col(\"Total Income\") / col(\"Population\")) \\\n",
    "            .withColumn(\"Crimes per Person\", col(\"Total Crimes\") / col(\"Population\")) \\\n",
    "            .orderBy(\"COMM\")\n",
    "\n",
    "result \\\n",
    "    .select(\"COMM\", \"Income per Person\", \"Crimes per Person\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 26.90 seconds"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (35)\n",
      "+- Sort (34)\n",
      "   +- Exchange (33)\n",
      "      +- Project (32)\n",
      "         +- HashAggregate (31)\n",
      "            +- Exchange (30)\n",
      "               +- HashAggregate (29)\n",
      "                  +- Project (28)\n",
      "                     +- BroadcastIndexJoin (27)\n",
      "                        :- SpatialIndex (19)\n",
      "                        :  +- Filter (18)\n",
      "                        :     +- ObjectHashAggregate (17)\n",
      "                        :        +- Exchange (16)\n",
      "                        :           +- ObjectHashAggregate (15)\n",
      "                        :              +- Project (14)\n",
      "                        :                 +- BroadcastHashJoin Inner BuildLeft (13)\n",
      "                        :                    :- BroadcastExchange (9)\n",
      "                        :                    :  +- ObjectHashAggregate (8)\n",
      "                        :                    :     +- Exchange (7)\n",
      "                        :                    :        +- ObjectHashAggregate (6)\n",
      "                        :                    :           +- Project (5)\n",
      "                        :                    :              +- Filter (4)\n",
      "                        :                    :                 +- Generate (3)\n",
      "                        :                    :                    +- Filter (2)\n",
      "                        :                    :                       +- Scan geojson  (1)\n",
      "                        :                    +- Project (12)\n",
      "                        :                       +- Filter (11)\n",
      "                        :                          +- Scan csv  (10)\n",
      "                        +- Union (26)\n",
      "                           :- Project (22)\n",
      "                           :  +- Filter (21)\n",
      "                           :     +- Scan csv  (20)\n",
      "                           +- Project (25)\n",
      "                              +- Filter (24)\n",
      "                                 +- Scan csv  (23)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#239]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#239]\n",
      "Condition : ((size(features#239, true) > 0) AND isnotnull(features#239))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#239]\n",
      "Arguments: explode(features#239), false, [features#247]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : ((((isnotnull(features#247.properties.CITY) AND isnotnull(features#247.properties.HOUSING10)) AND isnotnull(features#247.properties.POP_2010)) AND (((isnotnull(features#247.properties.COMM) AND (features#247.properties.CITY = Los Angeles)) AND (features#247.properties.HOUSING10 > 0)) AND (features#247.properties.POP_2010 > 0))) AND isnotnull(features#247.properties.ZCTA10))\n",
      "\n",
      "(5) Project\n",
      "Output [5]: [features#247.properties.COMM AS COMM#263, features#247.properties.ZCTA10 AS ZCTA10#280, features#247.properties.HOUSING10 AS HOUSING10#269L, features#247.properties.POP_2010 AS POP_2010#272L, features#247.geometry AS geometry#250]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(6) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, ZCTA10#280, HOUSING10#269L, POP_2010#272L, geometry#250]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [partial_sum(POP_2010#272L), partial_sum(HOUSING10#269L), partial_st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@2b93f53c, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#991L, sum#993L, buf#995]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum#992L, sum#994L, buf#996]\n",
      "\n",
      "(7) Exchange\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#992L, sum#994L, buf#996]\n",
      "Arguments: hashpartitioning(COMM#263, ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=2226]\n",
      "\n",
      "(8) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#992L, sum#994L, buf#996]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [sum(POP_2010#272L), sum(HOUSING10#269L), st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@2b93f53c, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(POP_2010#272L)#854L, sum(HOUSING10#269L)#856L, ST_Union_Aggr(geometry#250)#861]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum(POP_2010#272L)#854L AS Population#855L, sum(HOUSING10#269L)#856L AS Households#857L, ST_Union_Aggr(geometry#250)#861 AS geometry#862]\n",
      "\n",
      "(9) BroadcastExchange\n",
      "Input [5]: [COMM#263, ZCTA10#280, Population#855L, Households#857L, geometry#862]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=2229]\n",
      "\n",
      "(10) Scan csv \n",
      "Output [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "(11) Filter\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Condition : isnotnull(Zip Code#24)\n",
      "\n",
      "(12) Project\n",
      "Output [2]: [Zip Code#24, cast(regexp_replace(Estimated Median Income#26, [$,], , 1) as float) AS Estimated Median Income#30]\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "\n",
      "(13) BroadcastHashJoin\n",
      "Left keys [1]: [ZCTA10#280]\n",
      "Right keys [1]: [Zip Code#24]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(14) Project\n",
      "Output [5]: [COMM#263, Population#855L, Households#857L, geometry#862, Estimated Median Income#30]\n",
      "Input [7]: [COMM#263, ZCTA10#280, Population#855L, Households#857L, geometry#862, Zip Code#24, Estimated Median Income#30]\n",
      "\n",
      "(15) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, Population#855L, Households#857L, geometry#862, Estimated Median Income#30]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [partial_sum(Population#855L), partial_sum((Estimated Median Income#30 * cast(Households#857L as float))), partial_st_union_aggr(geometry#862, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@11e3f5b2, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#985L, sum#987, buf#989]\n",
      "Results [4]: [COMM#263, sum#986L, sum#988, buf#990]\n",
      "\n",
      "(16) Exchange\n",
      "Input [4]: [COMM#263, sum#986L, sum#988, buf#990]\n",
      "Arguments: hashpartitioning(COMM#263, 1000), ENSURE_REQUIREMENTS, [plan_id=2234]\n",
      "\n",
      "(17) ObjectHashAggregate\n",
      "Input [4]: [COMM#263, sum#986L, sum#988, buf#990]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [sum(Population#855L), sum((Estimated Median Income#30 * cast(Households#857L as float))), st_union_aggr(geometry#862, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@11e3f5b2, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(Population#855L)#901L, sum((Estimated Median Income#30 * cast(Households#857L as float)))#903, ST_Union_Aggr(geometry#862)#908]\n",
      "Results [4]: [COMM#263, sum(Population#855L)#901L AS Population#902L, sum((Estimated Median Income#30 * cast(Households#857L as float)))#903 AS Total Income#904, ST_Union_Aggr(geometry#862)#908 AS geometry#909]\n",
      "\n",
      "(18) Filter\n",
      "Input [4]: [COMM#263, Population#902L, Total Income#904, geometry#909]\n",
      "Condition : isnotnull(geometry#909)\n",
      "\n",
      "(19) SpatialIndex\n",
      "Arguments: geometry#909: geometry, RTREE, false, false\n",
      "\n",
      "(20) Scan csv \n",
      "Output [2]: [LAT#74, LON#75]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(21) Filter\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(22) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#188]\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "\n",
      "(23) Scan csv \n",
      "Output [2]: [LAT#130, LON#131]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(24) Filter\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(25) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#1045]\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "\n",
      "(26) Union\n",
      "\n",
      "(27) BroadcastIndexJoin\n",
      "Arguments: geom#188: geometry, LeftSide, RightSide, Inner, WITHIN\n",
      "\n",
      "(28) Project\n",
      "Output [3]: [COMM#263, Population#902L, Total Income#904]\n",
      "Input [5]: [COMM#263, Population#902L, Total Income#904, geometry#909, geom#188]\n",
      "\n",
      "(29) HashAggregate\n",
      "Input [3]: [COMM#263, Population#902L, Total Income#904]\n",
      "Keys [3]: [COMM#263, Population#902L, knownfloatingpointnormalized(normalizenanandzero(Total Income#904)) AS Total Income#904]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#983L]\n",
      "Results [4]: [COMM#263, Population#902L, Total Income#904, count#984L]\n",
      "\n",
      "(30) Exchange\n",
      "Input [4]: [COMM#263, Population#902L, Total Income#904, count#984L]\n",
      "Arguments: hashpartitioning(COMM#263, Population#902L, Total Income#904, 1000), ENSURE_REQUIREMENTS, [plan_id=2242]\n",
      "\n",
      "(31) HashAggregate\n",
      "Input [4]: [COMM#263, Population#902L, Total Income#904, count#984L]\n",
      "Keys [3]: [COMM#263, Population#902L, Total Income#904]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#950L]\n",
      "Results [5]: [COMM#263, Population#902L, Total Income#904, count(1)#950L AS Total Crimes#951L, (Total Income#904 / cast(Population#902L as double)) AS Income per Person#956]\n",
      "\n",
      "(32) Project\n",
      "Output [6]: [COMM#263, Population#902L, Total Income#904, Total Crimes#951L, Income per Person#956, (cast(Total Crimes#951L as double) / cast(Population#902L as double)) AS Crimes per Person#962]\n",
      "Input [5]: [COMM#263, Population#902L, Total Income#904, Total Crimes#951L, Income per Person#956]\n",
      "\n",
      "(33) Exchange\n",
      "Input [6]: [COMM#263, Population#902L, Total Income#904, Total Crimes#951L, Income per Person#956, Crimes per Person#962]\n",
      "Arguments: rangepartitioning(COMM#263 ASC NULLS FIRST, 1000), ENSURE_REQUIREMENTS, [plan_id=2246]\n",
      "\n",
      "(34) Sort\n",
      "Input [6]: [COMM#263, Population#902L, Total Income#904, Total Crimes#951L, Income per Person#956, Crimes per Person#962]\n",
      "Arguments: [COMM#263 ASC NULLS FIRST], true, 0\n",
      "\n",
      "(35) AdaptiveSparkPlan\n",
      "Output [6]: [COMM#263, Population#902L, Total Income#904, Total Crimes#951L, Income per Person#956, Crimes per Person#962]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "result.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------+-------------------+\n",
      "|COMM                   |Income per Person |Crimes per Person  |\n",
      "+-----------------------+------------------+-------------------+\n",
      "|Adams-Normandie        |8791.458301453711 |0.7148686559551135 |\n",
      "|Alsace                 |11239.50136425648 |0.5416098226466576 |\n",
      "|Angeles National Forest|33079.58823529412 |0.4117647058823529 |\n",
      "|Angelino Heights       |18427.059814658805|0.5732940185341197 |\n",
      "|Arleta                 |12110.779474388612|0.4264509064363061 |\n",
      "|Atwater Village        |28481.236967160792|0.5288318320448259 |\n",
      "|Baldwin Hills          |17303.906408241663|0.9950061114021302 |\n",
      "|Bel Air                |63041.33942621959 |0.39922527539038855|\n",
      "|Beverly Crest          |60947.48978754819 |0.3689607087195472 |\n",
      "|Beverlywood            |29267.82118405155 |0.5084977849375755 |\n",
      "|Boyle Heights          |8494.108286861341 |0.6171887393378809 |\n",
      "|Brentwood              |60846.854461055365|0.4058638814936173 |\n",
      "|Brookside              |18138.626409017714|0.8856682769726248 |\n",
      "|Cadillac-Corning       |19572.784696174043|0.581695423855964  |\n",
      "|Canoga Park            |19660.29173807544 |0.5506083179203503 |\n",
      "|Carthay                |49848.73592646053 |0.7628959963534149 |\n",
      "|Central                |6973.305663786775 |0.6593822350217403 |\n",
      "|Century City           |45617.760134566866|0.632968881412952  |\n",
      "|Century Palms/Cove     |8610.314655031003 |1.1446474853187232 |\n",
      "|Chatsworth             |30694.606443388573|0.5281029087959207 |\n",
      "+-----------------------+------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, col\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Για κάθε περιοχή του στο Census, υπάρχουν πολλαπλές\n",
    "# εγγραφές για COMM, ZCTA10. Συνεπώς, πριν κάνουμε το\n",
    "# join με το Income, πρέπει να κάνουμε group by για να\n",
    "# υπολογίσουμε το συνολικό πληθυσμό ανά COMM, ZIP Code\n",
    "result = blocks_data \\\n",
    "            .groupBy(\"COMM\", \"ZCTA10\") \\\n",
    "            .agg(\n",
    "                sum(\"POP_2010\").alias(\"Population\"),\n",
    "                sum(\"HOUSING10\").alias(\"Households\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "            )\n",
    "\n",
    "# Έπειτα κάνουμε join με το Income με βάση το ZIP Code,\n",
    "# οπότε έχουμε το μέσο ετήσιο εισόδημα ανά νοικοκυριό\n",
    "# για κάθε COMM, ZIP Code. Κάνουμε group by με το\n",
    "# COMM για να βρούμε το συνολικό εισόδημα και το\n",
    "# συνολικό πληθυσμό ανά COMM.\n",
    "result = result \\\n",
    "            .hint(\"MERGE\") \\\n",
    "            .join(income_data, blocks_data[\"ZCTA10\"] == income_data[\"Zip Code\"]) \\\n",
    "            .groupBy(\"COMM\") \\\n",
    "            .agg(\n",
    "                sum(\"Population\").alias(\"Population\"),\n",
    "                sum(col(\"Estimated Median Income\") * col(\"Households\")).alias(\"Total Income\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "            )\n",
    "\n",
    "# Τέλος, κάνουμε join με τα εγκλήματα με βάση το geometry, δηλαδή\n",
    "# το POINT του εγκλήματος βρίσκεται εντός του POLYGON της περιοχής.\n",
    "# Κάνουμε group by με το COMM (και τα Population, Total Income τα οποία είναι\n",
    "# ίδια ανά εγγραφή) και υπολογίζουμε το συνολικό πλήθος των εγκλημάτων.\n",
    "# Μετά διαιρούμε με τον πληθυσμό για να βρούμε το εισόδημα ανά άτομο\n",
    "# και το πλήθος εγκλημάτων ανά άτομο.\n",
    "result = result \\\n",
    "            .hint(\"MERGE\") \\\n",
    "            .join(crime_data, ST_Within(crime_data[\"geom\"], result[\"geometry\"]), \"inner\") \\\n",
    "            .groupBy(\"COMM\", \"Population\", \"Total Income\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"Total Crimes\")\n",
    "            ) \\\n",
    "            .withColumn(\"Income per Person\", col(\"Total Income\") / col(\"Population\")) \\\n",
    "            .withColumn(\"Crimes per Person\", col(\"Total Crimes\") / col(\"Population\")) \\\n",
    "            .orderBy(\"COMM\")\n",
    "\n",
    "result \\\n",
    "    .select(\"COMM\", \"Income per Person\", \"Crimes per Person\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 25.10 seconds"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (37)\n",
      "+- Sort (36)\n",
      "   +- Exchange (35)\n",
      "      +- Project (34)\n",
      "         +- HashAggregate (33)\n",
      "            +- Exchange (32)\n",
      "               +- HashAggregate (31)\n",
      "                  +- Project (30)\n",
      "                     +- RangeJoin (29)\n",
      "                        :- Filter (21)\n",
      "                        :  +- ObjectHashAggregate (20)\n",
      "                        :     +- Exchange (19)\n",
      "                        :        +- ObjectHashAggregate (18)\n",
      "                        :           +- Project (17)\n",
      "                        :              +- SortMergeJoin Inner (16)\n",
      "                        :                 :- Sort (10)\n",
      "                        :                 :  +- Exchange (9)\n",
      "                        :                 :     +- ObjectHashAggregate (8)\n",
      "                        :                 :        +- Exchange (7)\n",
      "                        :                 :           +- ObjectHashAggregate (6)\n",
      "                        :                 :              +- Project (5)\n",
      "                        :                 :                 +- Filter (4)\n",
      "                        :                 :                    +- Generate (3)\n",
      "                        :                 :                       +- Filter (2)\n",
      "                        :                 :                          +- Scan geojson  (1)\n",
      "                        :                 +- Sort (15)\n",
      "                        :                    +- Exchange (14)\n",
      "                        :                       +- Project (13)\n",
      "                        :                          +- Filter (12)\n",
      "                        :                             +- Scan csv  (11)\n",
      "                        +- Union (28)\n",
      "                           :- Project (24)\n",
      "                           :  +- Filter (23)\n",
      "                           :     +- Scan csv  (22)\n",
      "                           +- Project (27)\n",
      "                              +- Filter (26)\n",
      "                                 +- Scan csv  (25)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#239]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#239]\n",
      "Condition : ((size(features#239, true) > 0) AND isnotnull(features#239))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#239]\n",
      "Arguments: explode(features#239), false, [features#247]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : ((((isnotnull(features#247.properties.CITY) AND isnotnull(features#247.properties.HOUSING10)) AND isnotnull(features#247.properties.POP_2010)) AND (((isnotnull(features#247.properties.COMM) AND (features#247.properties.CITY = Los Angeles)) AND (features#247.properties.HOUSING10 > 0)) AND (features#247.properties.POP_2010 > 0))) AND isnotnull(features#247.properties.ZCTA10))\n",
      "\n",
      "(5) Project\n",
      "Output [5]: [features#247.properties.COMM AS COMM#263, features#247.properties.ZCTA10 AS ZCTA10#280, features#247.properties.HOUSING10 AS HOUSING10#269L, features#247.properties.POP_2010 AS POP_2010#272L, features#247.geometry AS geometry#250]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(6) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, ZCTA10#280, HOUSING10#269L, POP_2010#272L, geometry#250]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [partial_sum(POP_2010#272L), partial_sum(HOUSING10#269L), partial_st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@2155b123, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#1206L, sum#1208L, buf#1210]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum#1207L, sum#1209L, buf#1211]\n",
      "\n",
      "(7) Exchange\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#1207L, sum#1209L, buf#1211]\n",
      "Arguments: hashpartitioning(COMM#263, ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=3162]\n",
      "\n",
      "(8) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#1207L, sum#1209L, buf#1211]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [sum(POP_2010#272L), sum(HOUSING10#269L), st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@2155b123, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(POP_2010#272L)#1070L, sum(HOUSING10#269L)#1072L, ST_Union_Aggr(geometry#250)#1077]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum(POP_2010#272L)#1070L AS Population#1071L, sum(HOUSING10#269L)#1072L AS Households#1073L, ST_Union_Aggr(geometry#250)#1077 AS geometry#1078]\n",
      "\n",
      "(9) Exchange\n",
      "Input [5]: [COMM#263, ZCTA10#280, Population#1071L, Households#1073L, geometry#1078]\n",
      "Arguments: hashpartitioning(ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=3166]\n",
      "\n",
      "(10) Sort\n",
      "Input [5]: [COMM#263, ZCTA10#280, Population#1071L, Households#1073L, geometry#1078]\n",
      "Arguments: [ZCTA10#280 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(11) Scan csv \n",
      "Output [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "(12) Filter\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Condition : isnotnull(Zip Code#24)\n",
      "\n",
      "(13) Project\n",
      "Output [2]: [Zip Code#24, cast(regexp_replace(Estimated Median Income#26, [$,], , 1) as float) AS Estimated Median Income#30]\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "\n",
      "(14) Exchange\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#30]\n",
      "Arguments: hashpartitioning(Zip Code#24, 1000), ENSURE_REQUIREMENTS, [plan_id=3167]\n",
      "\n",
      "(15) Sort\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#30]\n",
      "Arguments: [Zip Code#24 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(16) SortMergeJoin\n",
      "Left keys [1]: [ZCTA10#280]\n",
      "Right keys [1]: [Zip Code#24]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(17) Project\n",
      "Output [5]: [COMM#263, Population#1071L, Households#1073L, geometry#1078, Estimated Median Income#30]\n",
      "Input [7]: [COMM#263, ZCTA10#280, Population#1071L, Households#1073L, geometry#1078, Zip Code#24, Estimated Median Income#30]\n",
      "\n",
      "(18) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, Population#1071L, Households#1073L, geometry#1078, Estimated Median Income#30]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [partial_sum(Population#1071L), partial_sum((Estimated Median Income#30 * cast(Households#1073L as float))), partial_st_union_aggr(geometry#1078, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@66abdc86, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#1200L, sum#1202, buf#1204]\n",
      "Results [4]: [COMM#263, sum#1201L, sum#1203, buf#1205]\n",
      "\n",
      "(19) Exchange\n",
      "Input [4]: [COMM#263, sum#1201L, sum#1203, buf#1205]\n",
      "Arguments: hashpartitioning(COMM#263, 1000), ENSURE_REQUIREMENTS, [plan_id=3174]\n",
      "\n",
      "(20) ObjectHashAggregate\n",
      "Input [4]: [COMM#263, sum#1201L, sum#1203, buf#1205]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [sum(Population#1071L), sum((Estimated Median Income#30 * cast(Households#1073L as float))), st_union_aggr(geometry#1078, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@66abdc86, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(Population#1071L)#1117L, sum((Estimated Median Income#30 * cast(Households#1073L as float)))#1119, ST_Union_Aggr(geometry#1078)#1124]\n",
      "Results [4]: [COMM#263, sum(Population#1071L)#1117L AS Population#1118L, sum((Estimated Median Income#30 * cast(Households#1073L as float)))#1119 AS Total Income#1120, ST_Union_Aggr(geometry#1078)#1124 AS geometry#1125]\n",
      "\n",
      "(21) Filter\n",
      "Input [4]: [COMM#263, Population#1118L, Total Income#1120, geometry#1125]\n",
      "Condition : isnotnull(geometry#1125)\n",
      "\n",
      "(22) Scan csv \n",
      "Output [2]: [LAT#74, LON#75]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(23) Filter\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(24) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#188]\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "\n",
      "(25) Scan csv \n",
      "Output [2]: [LAT#130, LON#131]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(26) Filter\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(27) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#1260]\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "\n",
      "(28) Union\n",
      "\n",
      "(29) RangeJoin\n",
      "Arguments: geometry#1125: geometry, geom#188: geometry, CONTAINS\n",
      "\n",
      "(30) Project\n",
      "Output [3]: [COMM#263, Population#1118L, Total Income#1120]\n",
      "Input [5]: [COMM#263, Population#1118L, Total Income#1120, geometry#1125, geom#188]\n",
      "\n",
      "(31) HashAggregate\n",
      "Input [3]: [COMM#263, Population#1118L, Total Income#1120]\n",
      "Keys [3]: [COMM#263, Population#1118L, knownfloatingpointnormalized(normalizenanandzero(Total Income#1120)) AS Total Income#1120]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#1198L]\n",
      "Results [4]: [COMM#263, Population#1118L, Total Income#1120, count#1199L]\n",
      "\n",
      "(32) Exchange\n",
      "Input [4]: [COMM#263, Population#1118L, Total Income#1120, count#1199L]\n",
      "Arguments: hashpartitioning(COMM#263, Population#1118L, Total Income#1120, 1000), ENSURE_REQUIREMENTS, [plan_id=3181]\n",
      "\n",
      "(33) HashAggregate\n",
      "Input [4]: [COMM#263, Population#1118L, Total Income#1120, count#1199L]\n",
      "Keys [3]: [COMM#263, Population#1118L, Total Income#1120]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#1166L]\n",
      "Results [5]: [COMM#263, Population#1118L, Total Income#1120, count(1)#1166L AS Total Crimes#1167L, (Total Income#1120 / cast(Population#1118L as double)) AS Income per Person#1172]\n",
      "\n",
      "(34) Project\n",
      "Output [6]: [COMM#263, Population#1118L, Total Income#1120, Total Crimes#1167L, Income per Person#1172, (cast(Total Crimes#1167L as double) / cast(Population#1118L as double)) AS Crimes per Person#1178]\n",
      "Input [5]: [COMM#263, Population#1118L, Total Income#1120, Total Crimes#1167L, Income per Person#1172]\n",
      "\n",
      "(35) Exchange\n",
      "Input [6]: [COMM#263, Population#1118L, Total Income#1120, Total Crimes#1167L, Income per Person#1172, Crimes per Person#1178]\n",
      "Arguments: rangepartitioning(COMM#263 ASC NULLS FIRST, 1000), ENSURE_REQUIREMENTS, [plan_id=3185]\n",
      "\n",
      "(36) Sort\n",
      "Input [6]: [COMM#263, Population#1118L, Total Income#1120, Total Crimes#1167L, Income per Person#1172, Crimes per Person#1178]\n",
      "Arguments: [COMM#263 ASC NULLS FIRST], true, 0\n",
      "\n",
      "(37) AdaptiveSparkPlan\n",
      "Output [6]: [COMM#263, Population#1118L, Total Income#1120, Total Crimes#1167L, Income per Person#1172, Crimes per Person#1178]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "result.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SHUFFLE HASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------+-------------------+\n",
      "|COMM                   |Income per Person |Crimes per Person  |\n",
      "+-----------------------+------------------+-------------------+\n",
      "|Adams-Normandie        |8791.458301453711 |0.7148686559551135 |\n",
      "|Alsace                 |11239.50136425648 |0.5416098226466576 |\n",
      "|Angeles National Forest|33079.58823529412 |0.4117647058823529 |\n",
      "|Angelino Heights       |18427.059814658805|0.5732940185341197 |\n",
      "|Arleta                 |12110.779474388612|0.4264509064363061 |\n",
      "|Atwater Village        |28481.236967160792|0.5288318320448259 |\n",
      "|Baldwin Hills          |17303.906408241663|0.9950061114021302 |\n",
      "|Bel Air                |63041.33942621959 |0.39922527539038855|\n",
      "|Beverly Crest          |60947.48978754819 |0.3689607087195472 |\n",
      "|Beverlywood            |29267.82118405155 |0.5084977849375755 |\n",
      "|Boyle Heights          |8494.108286861341 |0.6171887393378809 |\n",
      "|Brentwood              |60846.854461055365|0.4058638814936173 |\n",
      "|Brookside              |18138.626409017714|0.8856682769726248 |\n",
      "|Cadillac-Corning       |19572.784696174043|0.581695423855964  |\n",
      "|Canoga Park            |19660.29173807544 |0.5506083179203503 |\n",
      "|Carthay                |49848.73592646053 |0.7628959963534149 |\n",
      "|Central                |6973.305663786775 |0.6593822350217403 |\n",
      "|Century City           |45617.760134566866|0.632968881412952  |\n",
      "|Century Palms/Cove     |8610.314655031003 |1.1446474853187232 |\n",
      "|Chatsworth             |30694.606443388573|0.5281029087959207 |\n",
      "+-----------------------+------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, col\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Για κάθε περιοχή του στο Census, υπάρχουν πολλαπλές\n",
    "# εγγραφές για COMM, ZCTA10. Συνεπώς, πριν κάνουμε το\n",
    "# join με το Income, πρέπει να κάνουμε group by για να\n",
    "# υπολογίσουμε το συνολικό πληθυσμό ανά COMM, ZIP Code\n",
    "result = blocks_data \\\n",
    "            .groupBy(\"COMM\", \"ZCTA10\") \\\n",
    "            .agg(\n",
    "                sum(\"POP_2010\").alias(\"Population\"),\n",
    "                sum(\"HOUSING10\").alias(\"Households\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "            )\n",
    "\n",
    "# Έπειτα κάνουμε join με το Income με βάση το ZIP Code,\n",
    "# οπότε έχουμε το μέσο ετήσιο εισόδημα ανά νοικοκυριό\n",
    "# για κάθε COMM, ZIP Code. Κάνουμε group by με το\n",
    "# COMM για να βρούμε το συνολικό εισόδημα και το\n",
    "# συνολικό πληθυσμό ανά COMM.\n",
    "result = result \\\n",
    "            .hint(\"SHUFFLE_HASH\") \\\n",
    "            .join(income_data, blocks_data[\"ZCTA10\"] == income_data[\"Zip Code\"]) \\\n",
    "            .groupBy(\"COMM\") \\\n",
    "            .agg(\n",
    "                sum(\"Population\").alias(\"Population\"),\n",
    "                sum(col(\"Estimated Median Income\") * col(\"Households\")).alias(\"Total Income\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "            )\n",
    "\n",
    "# Τέλος, κάνουμε join με τα εγκλήματα με βάση το geometry, δηλαδή\n",
    "# το POINT του εγκλήματος βρίσκεται εντός του POLYGON της περιοχής.\n",
    "# Κάνουμε group by με το COMM (και τα Population, Total Income τα οποία είναι\n",
    "# ίδια ανά εγγραφή) και υπολογίζουμε το συνολικό πλήθος των εγκλημάτων.\n",
    "# Μετά διαιρούμε με τον πληθυσμό για να βρούμε το εισόδημα ανά άτομο\n",
    "# και το πλήθος εγκλημάτων ανά άτομο.\n",
    "result = result \\\n",
    "            .hint(\"SHUFFLE_HASH\") \\\n",
    "            .join(crime_data, ST_Within(crime_data[\"geom\"], result[\"geometry\"]), \"inner\") \\\n",
    "            .groupBy(\"COMM\", \"Population\", \"Total Income\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"Total Crimes\")\n",
    "            ) \\\n",
    "            .withColumn(\"Income per Person\", col(\"Total Income\") / col(\"Population\")) \\\n",
    "            .withColumn(\"Crimes per Person\", col(\"Total Crimes\") / col(\"Population\")) \\\n",
    "            .orderBy(\"COMM\")\n",
    "\n",
    "result \\\n",
    "    .select(\"COMM\", \"Income per Person\", \"Crimes per Person\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 22.81 seconds"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (35)\n",
      "+- Sort (34)\n",
      "   +- Exchange (33)\n",
      "      +- Project (32)\n",
      "         +- HashAggregate (31)\n",
      "            +- Exchange (30)\n",
      "               +- HashAggregate (29)\n",
      "                  +- Project (28)\n",
      "                     +- RangeJoin (27)\n",
      "                        :- Filter (19)\n",
      "                        :  +- ObjectHashAggregate (18)\n",
      "                        :     +- Exchange (17)\n",
      "                        :        +- ObjectHashAggregate (16)\n",
      "                        :           +- Project (15)\n",
      "                        :              +- ShuffledHashJoin Inner BuildLeft (14)\n",
      "                        :                 :- Exchange (9)\n",
      "                        :                 :  +- ObjectHashAggregate (8)\n",
      "                        :                 :     +- Exchange (7)\n",
      "                        :                 :        +- ObjectHashAggregate (6)\n",
      "                        :                 :           +- Project (5)\n",
      "                        :                 :              +- Filter (4)\n",
      "                        :                 :                 +- Generate (3)\n",
      "                        :                 :                    +- Filter (2)\n",
      "                        :                 :                       +- Scan geojson  (1)\n",
      "                        :                 +- Exchange (13)\n",
      "                        :                    +- Project (12)\n",
      "                        :                       +- Filter (11)\n",
      "                        :                          +- Scan csv  (10)\n",
      "                        +- Union (26)\n",
      "                           :- Project (22)\n",
      "                           :  +- Filter (21)\n",
      "                           :     +- Scan csv  (20)\n",
      "                           +- Project (25)\n",
      "                              +- Filter (24)\n",
      "                                 +- Scan csv  (23)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#239]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#239]\n",
      "Condition : ((size(features#239, true) > 0) AND isnotnull(features#239))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#239]\n",
      "Arguments: explode(features#239), false, [features#247]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : ((((isnotnull(features#247.properties.CITY) AND isnotnull(features#247.properties.HOUSING10)) AND isnotnull(features#247.properties.POP_2010)) AND (((isnotnull(features#247.properties.COMM) AND (features#247.properties.CITY = Los Angeles)) AND (features#247.properties.HOUSING10 > 0)) AND (features#247.properties.POP_2010 > 0))) AND isnotnull(features#247.properties.ZCTA10))\n",
      "\n",
      "(5) Project\n",
      "Output [5]: [features#247.properties.COMM AS COMM#263, features#247.properties.ZCTA10 AS ZCTA10#280, features#247.properties.HOUSING10 AS HOUSING10#269L, features#247.properties.POP_2010 AS POP_2010#272L, features#247.geometry AS geometry#250]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(6) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, ZCTA10#280, HOUSING10#269L, POP_2010#272L, geometry#250]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [partial_sum(POP_2010#272L), partial_sum(HOUSING10#269L), partial_st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@5f48ef1b, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#1420L, sum#1422L, buf#1424]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum#1421L, sum#1423L, buf#1425]\n",
      "\n",
      "(7) Exchange\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#1421L, sum#1423L, buf#1425]\n",
      "Arguments: hashpartitioning(COMM#263, ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=4059]\n",
      "\n",
      "(8) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#1421L, sum#1423L, buf#1425]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [sum(POP_2010#272L), sum(HOUSING10#269L), st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@5f48ef1b, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(POP_2010#272L)#1284L, sum(HOUSING10#269L)#1286L, ST_Union_Aggr(geometry#250)#1291]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum(POP_2010#272L)#1284L AS Population#1285L, sum(HOUSING10#269L)#1286L AS Households#1287L, ST_Union_Aggr(geometry#250)#1291 AS geometry#1292]\n",
      "\n",
      "(9) Exchange\n",
      "Input [5]: [COMM#263, ZCTA10#280, Population#1285L, Households#1287L, geometry#1292]\n",
      "Arguments: hashpartitioning(ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=4063]\n",
      "\n",
      "(10) Scan csv \n",
      "Output [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "(11) Filter\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Condition : isnotnull(Zip Code#24)\n",
      "\n",
      "(12) Project\n",
      "Output [2]: [Zip Code#24, cast(regexp_replace(Estimated Median Income#26, [$,], , 1) as float) AS Estimated Median Income#30]\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "\n",
      "(13) Exchange\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#30]\n",
      "Arguments: hashpartitioning(Zip Code#24, 1000), ENSURE_REQUIREMENTS, [plan_id=4064]\n",
      "\n",
      "(14) ShuffledHashJoin\n",
      "Left keys [1]: [ZCTA10#280]\n",
      "Right keys [1]: [Zip Code#24]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(15) Project\n",
      "Output [5]: [COMM#263, Population#1285L, Households#1287L, geometry#1292, Estimated Median Income#30]\n",
      "Input [7]: [COMM#263, ZCTA10#280, Population#1285L, Households#1287L, geometry#1292, Zip Code#24, Estimated Median Income#30]\n",
      "\n",
      "(16) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, Population#1285L, Households#1287L, geometry#1292, Estimated Median Income#30]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [partial_sum(Population#1285L), partial_sum((Estimated Median Income#30 * cast(Households#1287L as float))), partial_st_union_aggr(geometry#1292, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@d09fc25, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#1414L, sum#1416, buf#1418]\n",
      "Results [4]: [COMM#263, sum#1415L, sum#1417, buf#1419]\n",
      "\n",
      "(17) Exchange\n",
      "Input [4]: [COMM#263, sum#1415L, sum#1417, buf#1419]\n",
      "Arguments: hashpartitioning(COMM#263, 1000), ENSURE_REQUIREMENTS, [plan_id=4069]\n",
      "\n",
      "(18) ObjectHashAggregate\n",
      "Input [4]: [COMM#263, sum#1415L, sum#1417, buf#1419]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [sum(Population#1285L), sum((Estimated Median Income#30 * cast(Households#1287L as float))), st_union_aggr(geometry#1292, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@d09fc25, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(Population#1285L)#1331L, sum((Estimated Median Income#30 * cast(Households#1287L as float)))#1333, ST_Union_Aggr(geometry#1292)#1338]\n",
      "Results [4]: [COMM#263, sum(Population#1285L)#1331L AS Population#1332L, sum((Estimated Median Income#30 * cast(Households#1287L as float)))#1333 AS Total Income#1334, ST_Union_Aggr(geometry#1292)#1338 AS geometry#1339]\n",
      "\n",
      "(19) Filter\n",
      "Input [4]: [COMM#263, Population#1332L, Total Income#1334, geometry#1339]\n",
      "Condition : isnotnull(geometry#1339)\n",
      "\n",
      "(20) Scan csv \n",
      "Output [2]: [LAT#74, LON#75]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(21) Filter\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(22) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#188]\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "\n",
      "(23) Scan csv \n",
      "Output [2]: [LAT#130, LON#131]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(24) Filter\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(25) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#1474]\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "\n",
      "(26) Union\n",
      "\n",
      "(27) RangeJoin\n",
      "Arguments: geometry#1339: geometry, geom#188: geometry, CONTAINS\n",
      "\n",
      "(28) Project\n",
      "Output [3]: [COMM#263, Population#1332L, Total Income#1334]\n",
      "Input [5]: [COMM#263, Population#1332L, Total Income#1334, geometry#1339, geom#188]\n",
      "\n",
      "(29) HashAggregate\n",
      "Input [3]: [COMM#263, Population#1332L, Total Income#1334]\n",
      "Keys [3]: [COMM#263, Population#1332L, knownfloatingpointnormalized(normalizenanandzero(Total Income#1334)) AS Total Income#1334]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#1412L]\n",
      "Results [4]: [COMM#263, Population#1332L, Total Income#1334, count#1413L]\n",
      "\n",
      "(30) Exchange\n",
      "Input [4]: [COMM#263, Population#1332L, Total Income#1334, count#1413L]\n",
      "Arguments: hashpartitioning(COMM#263, Population#1332L, Total Income#1334, 1000), ENSURE_REQUIREMENTS, [plan_id=4076]\n",
      "\n",
      "(31) HashAggregate\n",
      "Input [4]: [COMM#263, Population#1332L, Total Income#1334, count#1413L]\n",
      "Keys [3]: [COMM#263, Population#1332L, Total Income#1334]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#1380L]\n",
      "Results [5]: [COMM#263, Population#1332L, Total Income#1334, count(1)#1380L AS Total Crimes#1381L, (Total Income#1334 / cast(Population#1332L as double)) AS Income per Person#1386]\n",
      "\n",
      "(32) Project\n",
      "Output [6]: [COMM#263, Population#1332L, Total Income#1334, Total Crimes#1381L, Income per Person#1386, (cast(Total Crimes#1381L as double) / cast(Population#1332L as double)) AS Crimes per Person#1392]\n",
      "Input [5]: [COMM#263, Population#1332L, Total Income#1334, Total Crimes#1381L, Income per Person#1386]\n",
      "\n",
      "(33) Exchange\n",
      "Input [6]: [COMM#263, Population#1332L, Total Income#1334, Total Crimes#1381L, Income per Person#1386, Crimes per Person#1392]\n",
      "Arguments: rangepartitioning(COMM#263 ASC NULLS FIRST, 1000), ENSURE_REQUIREMENTS, [plan_id=4080]\n",
      "\n",
      "(34) Sort\n",
      "Input [6]: [COMM#263, Population#1332L, Total Income#1334, Total Crimes#1381L, Income per Person#1386, Crimes per Person#1392]\n",
      "Arguments: [COMM#263 ASC NULLS FIRST], true, 0\n",
      "\n",
      "(35) AdaptiveSparkPlan\n",
      "Output [6]: [COMM#263, Population#1332L, Total Income#1334, Total Crimes#1381L, Income per Person#1386, Crimes per Person#1392]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "result.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. SHUFFLE REPLICATE NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------+-------------------+\n",
      "|COMM                   |Income per Person |Crimes per Person  |\n",
      "+-----------------------+------------------+-------------------+\n",
      "|Adams-Normandie        |8791.458301453711 |0.7148686559551135 |\n",
      "|Alsace                 |11239.50136425648 |0.5416098226466576 |\n",
      "|Angeles National Forest|33079.58823529412 |0.4117647058823529 |\n",
      "|Angelino Heights       |18427.059814658805|0.5732940185341197 |\n",
      "|Arleta                 |12110.779474388612|0.4264509064363061 |\n",
      "|Atwater Village        |28481.236967160792|0.5288318320448259 |\n",
      "|Baldwin Hills          |17303.906408241663|0.9950061114021302 |\n",
      "|Bel Air                |63041.33942621959 |0.39922527539038855|\n",
      "|Beverly Crest          |60947.48978754819 |0.3689607087195472 |\n",
      "|Beverlywood            |29267.82118405155 |0.5084977849375755 |\n",
      "|Boyle Heights          |8494.108286861341 |0.6171887393378809 |\n",
      "|Brentwood              |60846.854461055365|0.4058638814936173 |\n",
      "|Brookside              |18138.626409017714|0.8856682769726248 |\n",
      "|Cadillac-Corning       |19572.784696174043|0.581695423855964  |\n",
      "|Canoga Park            |19660.29173807544 |0.5506083179203503 |\n",
      "|Carthay                |49848.73592646053 |0.7628959963534149 |\n",
      "|Central                |6973.305663786775 |0.6593822350217403 |\n",
      "|Century City           |45617.760134566866|0.632968881412952  |\n",
      "|Century Palms/Cove     |8610.314655031003 |1.1446474853187232 |\n",
      "|Chatsworth             |30694.606443388573|0.5281029087959207 |\n",
      "+-----------------------+------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, col\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Για κάθε περιοχή του στο Census, υπάρχουν πολλαπλές\n",
    "# εγγραφές για COMM, ZCTA10. Συνεπώς, πριν κάνουμε το\n",
    "# join με το Income, πρέπει να κάνουμε group by για να\n",
    "# υπολογίσουμε το συνολικό πληθυσμό ανά COMM, ZIP Code\n",
    "result = blocks_data \\\n",
    "            .groupBy(\"COMM\", \"ZCTA10\") \\\n",
    "            .agg(\n",
    "                sum(\"POP_2010\").alias(\"Population\"),\n",
    "                sum(\"HOUSING10\").alias(\"Households\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "            )\n",
    "\n",
    "# Έπειτα κάνουμε join με το Income με βάση το ZIP Code,\n",
    "# οπότε έχουμε το μέσο ετήσιο εισόδημα ανά νοικοκυριό\n",
    "# για κάθε COMM, ZIP Code. Κάνουμε group by με το\n",
    "# COMM για να βρούμε το συνολικό εισόδημα και το\n",
    "# συνολικό πληθυσμό ανά COMM.\n",
    "result = result \\\n",
    "            .hint(\"SHUFFLE_REPLICATE_NL\") \\\n",
    "            .join(income_data, blocks_data[\"ZCTA10\"] == income_data[\"Zip Code\"]) \\\n",
    "            .groupBy(\"COMM\") \\\n",
    "            .agg(\n",
    "                sum(\"Population\").alias(\"Population\"),\n",
    "                sum(col(\"Estimated Median Income\") * col(\"Households\")).alias(\"Total Income\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "            )\n",
    "\n",
    "# Τέλος, κάνουμε join με τα εγκλήματα με βάση το geometry, δηλαδή\n",
    "# το POINT του εγκλήματος βρίσκεται εντός του POLYGON της περιοχής.\n",
    "# Κάνουμε group by με το COMM (και τα Population, Total Income τα οποία είναι\n",
    "# ίδια ανά εγγραφή) και υπολογίζουμε το συνολικό πλήθος των εγκλημάτων.\n",
    "# Μετά διαιρούμε με τον πληθυσμό για να βρούμε το εισόδημα ανά άτομο\n",
    "# και το πλήθος εγκλημάτων ανά άτομο.\n",
    "result = result \\\n",
    "            .hint(\"SHUFFLE_REPLICATE_NL\") \\\n",
    "            .join(crime_data, ST_Within(crime_data[\"geom\"], result[\"geometry\"]), \"inner\") \\\n",
    "            .groupBy(\"COMM\", \"Population\", \"Total Income\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"Total Crimes\")\n",
    "            ) \\\n",
    "            .withColumn(\"Income per Person\", col(\"Total Income\") / col(\"Population\")) \\\n",
    "            .withColumn(\"Crimes per Person\", col(\"Total Crimes\") / col(\"Population\")) \\\n",
    "            .orderBy(\"COMM\")\n",
    "\n",
    "result \\\n",
    "    .select(\"COMM\", \"Income per Person\", \"Crimes per Person\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 16.40 seconds"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (33)\n",
      "+- Sort (32)\n",
      "   +- Exchange (31)\n",
      "      +- Project (30)\n",
      "         +- HashAggregate (29)\n",
      "            +- Exchange (28)\n",
      "               +- HashAggregate (27)\n",
      "                  +- Project (26)\n",
      "                     +- RangeJoin (25)\n",
      "                        :- Filter (17)\n",
      "                        :  +- ObjectHashAggregate (16)\n",
      "                        :     +- Exchange (15)\n",
      "                        :        +- ObjectHashAggregate (14)\n",
      "                        :           +- Project (13)\n",
      "                        :              +- CartesianProduct Inner (12)\n",
      "                        :                 :- ObjectHashAggregate (8)\n",
      "                        :                 :  +- Exchange (7)\n",
      "                        :                 :     +- ObjectHashAggregate (6)\n",
      "                        :                 :        +- Project (5)\n",
      "                        :                 :           +- Filter (4)\n",
      "                        :                 :              +- Generate (3)\n",
      "                        :                 :                 +- Filter (2)\n",
      "                        :                 :                    +- Scan geojson  (1)\n",
      "                        :                 +- Project (11)\n",
      "                        :                    +- Filter (10)\n",
      "                        :                       +- Scan csv  (9)\n",
      "                        +- Union (24)\n",
      "                           :- Project (20)\n",
      "                           :  +- Filter (19)\n",
      "                           :     +- Scan csv  (18)\n",
      "                           +- Project (23)\n",
      "                              +- Filter (22)\n",
      "                                 +- Scan csv  (21)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#239]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#239]\n",
      "Condition : ((size(features#239, true) > 0) AND isnotnull(features#239))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#239]\n",
      "Arguments: explode(features#239), false, [features#247]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : ((((isnotnull(features#247.properties.CITY) AND isnotnull(features#247.properties.HOUSING10)) AND isnotnull(features#247.properties.POP_2010)) AND (((isnotnull(features#247.properties.COMM) AND (features#247.properties.CITY = Los Angeles)) AND (features#247.properties.HOUSING10 > 0)) AND (features#247.properties.POP_2010 > 0))) AND isnotnull(features#247.properties.ZCTA10))\n",
      "\n",
      "(5) Project\n",
      "Output [5]: [features#247.properties.COMM AS COMM#263, features#247.properties.ZCTA10 AS ZCTA10#280, features#247.properties.HOUSING10 AS HOUSING10#269L, features#247.properties.POP_2010 AS POP_2010#272L, features#247.geometry AS geometry#250]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(6) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, ZCTA10#280, HOUSING10#269L, POP_2010#272L, geometry#250]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [partial_sum(POP_2010#272L), partial_sum(HOUSING10#269L), partial_st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6c0dd3f6, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#1634L, sum#1636L, buf#1638]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum#1635L, sum#1637L, buf#1639]\n",
      "\n",
      "(7) Exchange\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#1635L, sum#1637L, buf#1639]\n",
      "Arguments: hashpartitioning(COMM#263, ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=4755]\n",
      "\n",
      "(8) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#1635L, sum#1637L, buf#1639]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [sum(POP_2010#272L), sum(HOUSING10#269L), st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6c0dd3f6, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(POP_2010#272L)#1498L, sum(HOUSING10#269L)#1500L, ST_Union_Aggr(geometry#250)#1505]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum(POP_2010#272L)#1498L AS Population#1499L, sum(HOUSING10#269L)#1500L AS Households#1501L, ST_Union_Aggr(geometry#250)#1505 AS geometry#1506]\n",
      "\n",
      "(9) Scan csv \n",
      "Output [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "(10) Filter\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Condition : isnotnull(Zip Code#24)\n",
      "\n",
      "(11) Project\n",
      "Output [2]: [Zip Code#24, cast(regexp_replace(Estimated Median Income#26, [$,], , 1) as float) AS Estimated Median Income#30]\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "\n",
      "(12) CartesianProduct\n",
      "Join type: Inner\n",
      "Join condition: (ZCTA10#280 = Zip Code#24)\n",
      "\n",
      "(13) Project\n",
      "Output [5]: [COMM#263, Population#1499L, Households#1501L, geometry#1506, Estimated Median Income#30]\n",
      "Input [7]: [COMM#263, ZCTA10#280, Population#1499L, Households#1501L, geometry#1506, Zip Code#24, Estimated Median Income#30]\n",
      "\n",
      "(14) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, Population#1499L, Households#1501L, geometry#1506, Estimated Median Income#30]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [partial_sum(Population#1499L), partial_sum((Estimated Median Income#30 * cast(Households#1501L as float))), partial_st_union_aggr(geometry#1506, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6380055e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#1628L, sum#1630, buf#1632]\n",
      "Results [4]: [COMM#263, sum#1629L, sum#1631, buf#1633]\n",
      "\n",
      "(15) Exchange\n",
      "Input [4]: [COMM#263, sum#1629L, sum#1631, buf#1633]\n",
      "Arguments: hashpartitioning(COMM#263, 1000), ENSURE_REQUIREMENTS, [plan_id=4761]\n",
      "\n",
      "(16) ObjectHashAggregate\n",
      "Input [4]: [COMM#263, sum#1629L, sum#1631, buf#1633]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [sum(Population#1499L), sum((Estimated Median Income#30 * cast(Households#1501L as float))), st_union_aggr(geometry#1506, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6380055e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(Population#1499L)#1545L, sum((Estimated Median Income#30 * cast(Households#1501L as float)))#1547, ST_Union_Aggr(geometry#1506)#1552]\n",
      "Results [4]: [COMM#263, sum(Population#1499L)#1545L AS Population#1546L, sum((Estimated Median Income#30 * cast(Households#1501L as float)))#1547 AS Total Income#1548, ST_Union_Aggr(geometry#1506)#1552 AS geometry#1553]\n",
      "\n",
      "(17) Filter\n",
      "Input [4]: [COMM#263, Population#1546L, Total Income#1548, geometry#1553]\n",
      "Condition : isnotnull(geometry#1553)\n",
      "\n",
      "(18) Scan csv \n",
      "Output [2]: [LAT#74, LON#75]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(19) Filter\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(20) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#188]\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "\n",
      "(21) Scan csv \n",
      "Output [2]: [LAT#130, LON#131]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(22) Filter\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(23) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#1685]\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "\n",
      "(24) Union\n",
      "\n",
      "(25) RangeJoin\n",
      "Arguments: geometry#1553: geometry, geom#188: geometry, CONTAINS\n",
      "\n",
      "(26) Project\n",
      "Output [3]: [COMM#263, Population#1546L, Total Income#1548]\n",
      "Input [5]: [COMM#263, Population#1546L, Total Income#1548, geometry#1553, geom#188]\n",
      "\n",
      "(27) HashAggregate\n",
      "Input [3]: [COMM#263, Population#1546L, Total Income#1548]\n",
      "Keys [3]: [COMM#263, Population#1546L, knownfloatingpointnormalized(normalizenanandzero(Total Income#1548)) AS Total Income#1548]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#1626L]\n",
      "Results [4]: [COMM#263, Population#1546L, Total Income#1548, count#1627L]\n",
      "\n",
      "(28) Exchange\n",
      "Input [4]: [COMM#263, Population#1546L, Total Income#1548, count#1627L]\n",
      "Arguments: hashpartitioning(COMM#263, Population#1546L, Total Income#1548, 1000), ENSURE_REQUIREMENTS, [plan_id=4768]\n",
      "\n",
      "(29) HashAggregate\n",
      "Input [4]: [COMM#263, Population#1546L, Total Income#1548, count#1627L]\n",
      "Keys [3]: [COMM#263, Population#1546L, Total Income#1548]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#1594L]\n",
      "Results [5]: [COMM#263, Population#1546L, Total Income#1548, count(1)#1594L AS Total Crimes#1595L, (Total Income#1548 / cast(Population#1546L as double)) AS Income per Person#1600]\n",
      "\n",
      "(30) Project\n",
      "Output [6]: [COMM#263, Population#1546L, Total Income#1548, Total Crimes#1595L, Income per Person#1600, (cast(Total Crimes#1595L as double) / cast(Population#1546L as double)) AS Crimes per Person#1606]\n",
      "Input [5]: [COMM#263, Population#1546L, Total Income#1548, Total Crimes#1595L, Income per Person#1600]\n",
      "\n",
      "(31) Exchange\n",
      "Input [6]: [COMM#263, Population#1546L, Total Income#1548, Total Crimes#1595L, Income per Person#1600, Crimes per Person#1606]\n",
      "Arguments: rangepartitioning(COMM#263 ASC NULLS FIRST, 1000), ENSURE_REQUIREMENTS, [plan_id=4772]\n",
      "\n",
      "(32) Sort\n",
      "Input [6]: [COMM#263, Population#1546L, Total Income#1548, Total Crimes#1595L, Income per Person#1600, Crimes per Person#1606]\n",
      "Arguments: [COMM#263 ASC NULLS FIRST], true, 0\n",
      "\n",
      "(33) AdaptiveSparkPlan\n",
      "Output [6]: [COMM#263, Population#1546L, Total Income#1548, Total Crimes#1595L, Income per Person#1600, Crimes per Person#1606]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "result.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Συμπεράσματα\n",
    "\n",
    "Η στρατηγική join που πετυχαίνει την καλύτερη επίδοση είναι η **SHUFFLE REPLICATE NL** (*Shuffle Replicate Nested Loop*) με χρόνο 16.40 δευτερόλεπτα.\n",
    "\n",
    "Η δεύτερη καλύτερη στρατηγική με λίγο μεγαλύτερο χρόνο (που μπορεί να οφείλεται και στην τυχαιότητα) είναι η **SHUFFLE HASH** με 22.81 δευτερόλεπτα, ενώ οι στρατηγικές **MERGE** και **BROADCAST** είναι λίγο πιο αργές, με χρόνους 25.10 και 26.90 δευτερόλεπτα αντίστοιχα.\n",
    "\n",
    "Συνεπώς η καταλληλότερη στρατηγική για την περίπτωσή μας είναι η **SHUFFLE_REPLICATE_NL**."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "name": "SparkLab - Introduction to RDDs and DataFrames"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
