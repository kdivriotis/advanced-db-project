{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Προχωρημένα Θέματα Βάσεων Δεδομένων\n",
    "\n",
    "**Ονοματεπώνυμο:** Κωνσταντίνος Διβριώτης\n",
    "\n",
    "**ΑΜ:** 03114140\n",
    "\n",
    "## Query 3: \n",
    "\n",
    "Χρησιμοποιώντας ως αναφορά τα δεδομένα της απογραφής 2010 για τον πληθυσμό και εκείνα της απογραφής του 2015 για το εισόδημα ανα νοικοκυριό, να υπολογίσετε για κάθε περιοχή του Los Angeles τα παρακάτω:\n",
    "- Το μέσο ετήσιο εισόδημα ανά άτομο\n",
    "- Την αναλογία συνολικού αριθμού εγκλημάτων ανά άτομο"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2uaG6GoZthpT",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1041</td><td>application_1732639283265_1008</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1008/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-80.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1008_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.spark import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"CensusDataAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_BUCKET = \"s3://initial-notebook-data-bucket-dblab-905418150721\"\n",
    "GROUP_BUCKET = \"s3://groups-bucket-dblab-905418150721/group15\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Διάβασμα και Επισκόπηση αρχείων εισόδου"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+\n",
      "|Zip Code|Estimated Median Income|\n",
      "+--------+-----------------------+\n",
      "|   90001|                33887.0|\n",
      "|   90002|                30413.0|\n",
      "|   90003|                30805.0|\n",
      "|   90004|                40612.0|\n",
      "|   90005|                31142.0|\n",
      "+--------+-----------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "income_schema = StructType([\n",
    "    StructField(\"Zip Code\", StringType()),\n",
    "    StructField(\"Community\", StringType()),\n",
    "    StructField(\"Estimated Median Income\", StringType())\n",
    "])\n",
    "\n",
    "income_data = spark.read.csv(f\"{DATA_BUCKET}/LA_income_2015.csv\", header=True, schema=income_schema)\n",
    "\n",
    "# Μετατροπή του Estimated Median Income σε αριθμητική μορφή\n",
    "income_data = income_data \\\n",
    "    .withColumn(\n",
    "        \"Estimated Median Income\",\n",
    "        regexp_replace(col(\"Estimated Median Income\"), \"[$,]\", \"\").cast(\"float\")\n",
    "    ) \\\n",
    "    .select(\"Zip Code\", \"Estimated Median Income\")\n",
    "\n",
    "income_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|    DR_NO|                geom|\n",
      "+---------+--------------------+\n",
      "|001307355|POINT (-118.2695 ...|\n",
      "|011401303|POINT (-118.3962 ...|\n",
      "|070309629|POINT (-118.2524 ...|\n",
      "|090631215|POINT (-118.3295 ...|\n",
      "|100100501|POINT (-118.2488 ...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, DoubleType\n",
    "\n",
    "# Ορισμός του schema των dataset\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", IntegerType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", IntegerType()),\n",
    "    StructField(\"Crm Cd\", IntegerType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", StringType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", IntegerType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", IntegerType()),\n",
    "    StructField(\"Crm Cd 2\", IntegerType()),\n",
    "    StructField(\"Crm Cd 3\", IntegerType()),\n",
    "    StructField(\"Crm Cd 4\", IntegerType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", DoubleType()),\n",
    "    StructField(\"LON\", DoubleType())\n",
    "])\n",
    "\n",
    "# Διαβάζουμε τα 2 datasets (2010-2019 και 2020-σήμερα) και τα συνενώνουμε σε 1\n",
    "crime_data_2010_2019 = spark.read.csv(f\"{DATA_BUCKET}/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, schema=crimes_schema)\n",
    "crime_data_2020_present = spark.read.csv(f\"{DATA_BUCKET}/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", header=True, schema=crimes_schema)\n",
    "crime_data = crime_data_2010_2019.union(crime_data_2020_present)\n",
    "\n",
    "# Μετατρέπουμε τις στήλες LAT, LON σε geometry με το ST_POINT\n",
    "crime_data = crime_data \\\n",
    "                .withColumn(\"geom\", ST_Point(\"LON\", \"LAT\")) \\\n",
    "                .filter(col(\"geom\") != ST_Point(0, 0)) \\\n",
    "                .select(\"DR_NO\", \"geom\")\n",
    "\n",
    "crime_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------+--------------------+\n",
      "|                COMM|ZCTA10|POP_2010|            geometry|\n",
      "+--------------------+------+--------+--------------------+\n",
      "|West Antelope Valley| 93243|       5|POLYGON ((-118.70...|\n",
      "|           San Pedro| 90732|      69|POLYGON ((-118.31...|\n",
      "|         Diamond Bar| 91789|      79|POLYGON ((-117.84...|\n",
      "|           San Pedro| 90731|     120|POLYGON ((-118.28...|\n",
      "|           San Pedro| 90731|     240|POLYGON ((-118.29...|\n",
      "+--------------------+------+--------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\") \\\n",
    "            .load(f\"{DATA_BUCKET}/2010_Census_Blocks.geojson\") \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "blocks_data = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                    blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\") \\\n",
    "            .filter(col(\"COMM\").isNotNull() & (col(\"POP_2010\") > 0)) \\\n",
    "            .select(\"COMM\", \"ZCTA10\", \"POP_2010\", \"geometry\")\n",
    "\n",
    "blocks_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- COMM: string (nullable = true)\n",
      " |-- ZCTA10: string (nullable = true)\n",
      " |-- POP_2010: long (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)"
     ]
    }
   ],
   "source": [
    "blocks_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------------------------------------------------------------------------+\n",
      "|field   |type    |meaning                                                                  |\n",
      "+--------+--------+-------------------------------------------------------------------------+\n",
      "|COMM    |string  |Unincorporated area community name and LA City neighborhood              |\n",
      "|POP_2010|long    |Population (PL 94-171 Redistricting Data Summary File - Total Population)|\n",
      "|ZCTA10  |string  |Zip Code Tabulation Area                                                 |\n",
      "|geometry|geometry|Geometry of the block                                                    |\n",
      "+--------+--------+-------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "blocks_data_description_schema = StructType([\n",
    "    StructField(\"field\", StringType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"meaning\", StringType())\n",
    "])\n",
    "\n",
    "blocks_data_description = spark.read.csv(f\"{DATA_BUCKET}/2010_Census_Blocks_fields.csv\", header=True, schema=blocks_data_description_schema)\n",
    "\n",
    "blocks_data_description \\\n",
    "        .filter(col(\"field\").isin(\"COMM\", \"ZCTA10\", \"POP_2010\", \"geometry\")) \\\n",
    "        .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Υλοποίηση με DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+\n",
      "|              COMM| Income per Person|   Crimes per Person|\n",
      "+------------------+------------------+--------------------+\n",
      "|        South Park| 119.3436899785335|  0.8312902496893007|\n",
      "|   Exposition Park|159.45045784492265|  0.9741195783209788|\n",
      "|           Central|135.04158432612502|  0.6675512393427814|\n",
      "|       West Vernon|142.88127236580516|  1.0376739562624255|\n",
      "|    Vernon Central|116.06856533595784|   0.663406532570585|\n",
      "|Wholesale District|183.52424911185273|  1.6380665303046613|\n",
      "|          Downtown|183.67133547731888|   2.974018280297901|\n",
      "|   University Park|102.57116979309234|  0.7612511069962161|\n",
      "|        Pico-Union|115.16710209003216|   0.706842845659164|\n",
      "|        West Adams|136.08198018309912|  0.7499905813208756|\n",
      "|   Wilshire Center| 92.47700602600698|  0.8269584522676816|\n",
      "|    Temple-Beaudry|180.82272665116915|  0.6359086558184056|\n",
      "|          Westlake| 50.96431055309492|  0.8648925656136754|\n",
      "|       Pico Rivera| 456.6046995646786|3.177528518318452E-5|\n",
      "|        Montebello|        395.805744|              9.6E-5|\n",
      "|     Boyle Heights|  169.583309101483|  0.6253271299796452|\n",
      "|     Monterey Park| 411.5216280343128|8.296138976920142E-5|\n",
      "|            Walnut| 656.5997531879884|6.855889208830385E-5|\n",
      "|          Commerce| 501.5481556578024|2.339546128051158...|\n",
      "|         Chinatown|135.73114497850722|  0.8569753810082064|\n",
      "+------------------+------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, col\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Σύνδεση Εισοδήματος / Περιοχών με βάση το ZIP Code\n",
    "income_per_block = blocks_data \\\n",
    "                        .join(income_data, blocks_data[\"ZCTA10\"] == income_data[\"Zip Code\"]) \\\n",
    "                        .groupBy(\"COMM\") \\\n",
    "                        .agg( \\\n",
    "                            sum(\"POP_2010\").alias(\"Population\"), \\\n",
    "                            sum(\"Estimated Median Income\").alias(\"Total Income\") \\\n",
    "                        )\n",
    "\n",
    "# Σύνδεση Εγκλημάτων / Περιοχών με βάση το geometry, δηλαδή\n",
    "# το POINT του εγκλήματος βρίσκεται εντός του POLYGON της περιοχής\n",
    "crimes_per_block = crime_data \\\n",
    "                        .join(blocks_data, ST_Within(crime_data[\"geom\"], blocks_data[\"geometry\"]), \"inner\") \\\n",
    "                        .groupBy(\"COMM\") \\\n",
    "                        .agg(count(\"*\").alias(\"Total Crimes\"))\n",
    "\n",
    "# Aναλογία συνολικού αριθμού εγκλημάτων ανά άτομο\n",
    "result = income_per_block \\\n",
    "                .join(crimes_per_block, on=[\"COMM\"]) \\\n",
    "                .withColumn(\"Income per Person\", col(\"Total Income\") / col(\"Population\")) \\\n",
    "                .withColumn(\"Crimes per Person\", col(\"Total Crimes\") / col(\"Population\")) \\\n",
    "                .select(\"COMM\", \"Income per Person\", \"Crimes per Person\")\n",
    "\n",
    "result.show()\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 32.24 seconds"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (36)\n",
      "+- Project (35)\n",
      "   +- SortMergeJoin Inner (34)\n",
      "      :- Sort (15)\n",
      "      :  +- HashAggregate (14)\n",
      "      :     +- Exchange (13)\n",
      "      :        +- HashAggregate (12)\n",
      "      :           +- Project (11)\n",
      "      :              +- BroadcastHashJoin Inner BuildRight (10)\n",
      "      :                 :- Project (5)\n",
      "      :                 :  +- Filter (4)\n",
      "      :                 :     +- Generate (3)\n",
      "      :                 :        +- Filter (2)\n",
      "      :                 :           +- Scan geojson  (1)\n",
      "      :                 +- BroadcastExchange (9)\n",
      "      :                    +- Project (8)\n",
      "      :                       +- Filter (7)\n",
      "      :                          +- Scan csv  (6)\n",
      "      +- Sort (33)\n",
      "         +- HashAggregate (32)\n",
      "            +- Exchange (31)\n",
      "               +- HashAggregate (30)\n",
      "                  +- Project (29)\n",
      "                     +- RangeJoin (28)\n",
      "                        :- Union (22)\n",
      "                        :  :- Project (18)\n",
      "                        :  :  +- Filter (17)\n",
      "                        :  :     +- Scan csv  (16)\n",
      "                        :  +- Project (21)\n",
      "                        :     +- Filter (20)\n",
      "                        :        +- Scan csv  (19)\n",
      "                        +- Project (27)\n",
      "                           +- Filter (26)\n",
      "                              +- Generate (25)\n",
      "                                 +- Filter (24)\n",
      "                                    +- Scan geojson  (23)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#239]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#239]\n",
      "Condition : ((size(features#239, true) > 0) AND isnotnull(features#239))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#239]\n",
      "Arguments: explode(features#239), false, [features#247]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : ((isnotnull(features#247.properties.POP_2010) AND (isnotnull(features#247.properties.COMM) AND (features#247.properties.POP_2010 > 0))) AND isnotnull(features#247.properties.ZCTA10))\n",
      "\n",
      "(5) Project\n",
      "Output [3]: [features#247.properties.COMM AS COMM#263, features#247.properties.ZCTA10 AS ZCTA10#280, features#247.properties.POP_2010 AS POP_2010#272L]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(6) Scan csv \n",
      "Output [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "(7) Filter\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Condition : isnotnull(Zip Code#24)\n",
      "\n",
      "(8) Project\n",
      "Output [2]: [Zip Code#24, cast(regexp_replace(Estimated Median Income#26, [$,], , 1) as float) AS Estimated Median Income#30]\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "\n",
      "(9) BroadcastExchange\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#30]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=1370]\n",
      "\n",
      "(10) BroadcastHashJoin\n",
      "Left keys [1]: [ZCTA10#280]\n",
      "Right keys [1]: [Zip Code#24]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(11) Project\n",
      "Output [3]: [COMM#263, POP_2010#272L, Estimated Median Income#30]\n",
      "Input [5]: [COMM#263, ZCTA10#280, POP_2010#272L, Zip Code#24, Estimated Median Income#30]\n",
      "\n",
      "(12) HashAggregate\n",
      "Input [3]: [COMM#263, POP_2010#272L, Estimated Median Income#30]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [2]: [partial_sum(POP_2010#272L), partial_sum(Estimated Median Income#30)]\n",
      "Aggregate Attributes [2]: [sum#691L, sum#693]\n",
      "Results [3]: [COMM#263, sum#692L, sum#694]\n",
      "\n",
      "(13) Exchange\n",
      "Input [3]: [COMM#263, sum#692L, sum#694]\n",
      "Arguments: hashpartitioning(COMM#263, 1000), ENSURE_REQUIREMENTS, [plan_id=1375]\n",
      "\n",
      "(14) HashAggregate\n",
      "Input [3]: [COMM#263, sum#692L, sum#694]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [2]: [sum(POP_2010#272L), sum(Estimated Median Income#30)]\n",
      "Aggregate Attributes [2]: [sum(POP_2010#272L)#601L, sum(Estimated Median Income#30)#603]\n",
      "Results [3]: [COMM#263, sum(POP_2010#272L)#601L AS Population#602L, sum(Estimated Median Income#30)#603 AS Total Income#604]\n",
      "\n",
      "(15) Sort\n",
      "Input [3]: [COMM#263, Population#602L, Total Income#604]\n",
      "Arguments: [COMM#263 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(16) Scan csv \n",
      "Output [2]: [LAT#74, LON#75]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(17) Filter\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(18) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#188]\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "\n",
      "(19) Scan csv \n",
      "Output [2]: [LAT#130, LON#131]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(20) Filter\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(21) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#730]\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "\n",
      "(22) Union\n",
      "\n",
      "(23) Scan geojson \n",
      "Output [1]: [features#631]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(24) Filter\n",
      "Input [1]: [features#631]\n",
      "Condition : ((size(features#631, true) > 0) AND isnotnull(features#631))\n",
      "\n",
      "(25) Generate\n",
      "Input [1]: [features#631]\n",
      "Arguments: explode(features#631), false, [features#247]\n",
      "\n",
      "(26) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : (((isnotnull(features#247.properties.POP_2010) AND isnotnull(features#247.properties.COMM)) AND (features#247.properties.POP_2010 > 0)) AND isnotnull(features#247.geometry))\n",
      "\n",
      "(27) Project\n",
      "Output [2]: [features#247.properties.COMM AS COMM#641, features#247.geometry AS geometry#250]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(28) RangeJoin\n",
      "Arguments: geom#188: geometry, geometry#250: geometry, WITHIN\n",
      "\n",
      "(29) Project\n",
      "Output [1]: [COMM#641]\n",
      "Input [3]: [geom#188, COMM#641, geometry#250]\n",
      "\n",
      "(30) HashAggregate\n",
      "Input [1]: [COMM#641]\n",
      "Keys [1]: [COMM#641]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#695L]\n",
      "Results [2]: [COMM#641, count#696L]\n",
      "\n",
      "(31) Exchange\n",
      "Input [2]: [COMM#641, count#696L]\n",
      "Arguments: hashpartitioning(COMM#641, 1000), ENSURE_REQUIREMENTS, [plan_id=1448]\n",
      "\n",
      "(32) HashAggregate\n",
      "Input [2]: [COMM#641, count#696L]\n",
      "Keys [1]: [COMM#641]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#626L]\n",
      "Results [2]: [COMM#641, count(1)#626L AS Total Crimes#627L]\n",
      "\n",
      "(33) Sort\n",
      "Input [2]: [COMM#641, Total Crimes#627L]\n",
      "Arguments: [COMM#641 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(34) SortMergeJoin\n",
      "Left keys [1]: [COMM#263]\n",
      "Right keys [1]: [COMM#641]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(35) Project\n",
      "Output [3]: [COMM#263, (Total Income#604 / cast(Population#602L as double)) AS Income per Person#664, (cast(Total Crimes#627L as double) / cast(Population#602L as double)) AS Crimes per Person#670]\n",
      "Input [5]: [COMM#263, Population#602L, Total Income#604, COMM#641, Total Crimes#627L]\n",
      "\n",
      "(36) AdaptiveSparkPlan\n",
      "Output [3]: [COMM#263, Income per Person#664, Crimes per Person#670]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "result.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Από προεπιλογή, το Spark χρησιμοποιεί τη στρατηγική **Sort Merge Join** (δηλαδή Merge).\n",
    "\n",
    "Θα δοκιμάσουμε να αναγκάσουμε το Spark να χρησιμοποιήσει διαφορετικές στρατηγικές, ώστε να συγκρίνουμε την απόδοσή τους."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. BROADCAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24c5a6f41af454c97dc49fac79c680d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '400' from http://ec2-35-159-120-182.eu-central-1.compute.amazonaws.com:8998/sessions/914/statements/27 with error payload: {\"msg\":\"requirement failed: Session isn't active.\"}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, col\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Σύνδεση Εισοδήματος / Περιοχών\n",
    "income_per_block = blocks_data  \\\n",
    "                        .hint(\"BROADCAST\") \\\n",
    "                        .join(income_data, blocks_data[\"ZCTA10\"] == income_data[\"Zip Code\"]) \\\n",
    "                        .groupBy(\"COMM\") \\\n",
    "                        .agg( \\\n",
    "                            sum(\"POP_2010\").alias(\"Population\"), \\\n",
    "                            sum(\"Estimated Median Income\").alias(\"Total Income\") \\\n",
    "                        )\n",
    "\n",
    "# Σύνδεση Εγκλημάτων / Περιοχών\n",
    "crimes_per_block = crime_data \\\n",
    "                        .hint(\"BROADCAST\") \\\n",
    "                        .join(blocks_data, ST_Within(crime_data[\"geom\"], blocks_data[\"geometry\"]), \"inner\") \\\n",
    "                        .groupBy(\"COMM\") \\\n",
    "                        .agg(count(\"*\").alias(\"Total Crimes\"))\n",
    "\n",
    "# Aναλογία συνολικού αριθμού εγκλημάτων ανά άτομο\n",
    "result = income_per_block \\\n",
    "                .hint(\"BROADCAST\") \\\n",
    "                .join(crimes_per_block, on=[\"COMM\"]) \\\n",
    "                .withColumn(\"Income per Person\", col(\"Total Income\") / col(\"Population\")) \\\n",
    "                .withColumn(\"Crimes per Person\", col(\"Total Crimes\") / col(\"Population\")) \\\n",
    "                .select(\"COMM\", \"Income per Person\", \"Crimes per Person\")\n",
    "\n",
    "result.show()\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Το query δε μπορεί να εκτελεστεί με τη στρατηγική Broadcast στα join, λόγω του μεγάλου όγκου δεδομένων."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+\n",
      "|                COMM| Income per Person|   Crimes per Person|\n",
      "+--------------------+------------------+--------------------+\n",
      "|                    | 649.5344341988502|1.543269416258343...|\n",
      "|               Acton|3226.7076469141875|1.229407425620850...|\n",
      "|     Adams-Normandie|160.62637082376943|  0.7148686559551135|\n",
      "|        Agoura Hills| 787.9183472700443|2.951303492375799...|\n",
      "|            Alhambra| 452.1307272924214|6.258349480677346E-4|\n",
      "|              Alsace|186.29007503410642|  0.5416098226466576|\n",
      "|            Altadena| 814.0934146854618|2.337704841386726...|\n",
      "|            Anaverde|2450.7256874580817|0.002682763246143...|\n",
      "|Angeles National ...| 4089.636842105263| 0.10300751879699248|\n",
      "|    Angelino Heights| 241.8594276094276|  0.5989057239057239|\n",
      "|             Arcadia|  592.959316784759|3.128274912799336...|\n",
      "|              Arleta|312.64241391896826|  0.4264509064363061|\n",
      "|      Athens Village| 364.7324789747697|0.002002402883460152|\n",
      "|     Athens-Westmont|272.68700409048347|0.004607954265437879|\n",
      "|     Atwater Village| 520.4056449897171|  0.5319480887880292|\n",
      "|              Avalon| 663.5335300429184|0.001609442060085837|\n",
      "|               Azusa| 505.0034097152645|4.847936395074496E-5|\n",
      "|       Baldwin Hills| 146.9312427977791|  0.9974508502985648|\n",
      "|             Bel Air|1013.9382641326716| 0.39922527539038855|\n",
      "|        Bell Gardens|186.50099828864802|2.139189960068454...|\n",
      "+--------------------+------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, col\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Σύνδεση Εισοδήματος / Περιοχών με βάση το ZIP Code\n",
    "income_per_block = blocks_data  \\\n",
    "                        .hint(\"MERGE\") \\\n",
    "                        .join(income_data, blocks_data[\"ZCTA10\"] == income_data[\"Zip Code\"]) \\\n",
    "                        .groupBy(\"COMM\") \\\n",
    "                        .agg( \\\n",
    "                            sum(\"POP_2010\").alias(\"Population\"), \\\n",
    "                            sum(\"Estimated Median Income\").alias(\"Total Income\") \\\n",
    "                        )\n",
    "\n",
    "# Σύνδεση Εγκλημάτων / Περιοχών με βάση το geometry, δηλαδή\n",
    "# το POINT του εγκλήματος βρίσκεται εντός του POLYGON της περιοχής\n",
    "crimes_per_block = crime_data \\\n",
    "                        .hint(\"MERGE\") \\\n",
    "                        .join(blocks_data, ST_Within(crime_data[\"geom\"], blocks_data[\"geometry\"]), \"inner\") \\\n",
    "                        .groupBy(\"COMM\") \\\n",
    "                        .agg(count(\"*\").alias(\"Total Crimes\"))\n",
    "\n",
    "# Aναλογία συνολικού αριθμού εγκλημάτων ανά άτομο\n",
    "result = income_per_block \\\n",
    "                .hint(\"MERGE\") \\\n",
    "                .join(crimes_per_block, on=[\"COMM\"]) \\\n",
    "                .withColumn(\"Income per Person\", col(\"Total Income\") / col(\"Population\")) \\\n",
    "                .withColumn(\"Crimes per Person\", col(\"Total Crimes\") / col(\"Population\")) \\\n",
    "                .select(\"COMM\", \"Income per Person\", \"Crimes per Person\")\n",
    "\n",
    "result.show()\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 31.77 seconds"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (39)\n",
      "+- Project (38)\n",
      "   +- SortMergeJoin Inner (37)\n",
      "      :- Sort (18)\n",
      "      :  +- HashAggregate (17)\n",
      "      :     +- Exchange (16)\n",
      "      :        +- HashAggregate (15)\n",
      "      :           +- Project (14)\n",
      "      :              +- SortMergeJoin Inner (13)\n",
      "      :                 :- Sort (7)\n",
      "      :                 :  +- Exchange (6)\n",
      "      :                 :     +- Project (5)\n",
      "      :                 :        +- Filter (4)\n",
      "      :                 :           +- Generate (3)\n",
      "      :                 :              +- Filter (2)\n",
      "      :                 :                 +- Scan geojson  (1)\n",
      "      :                 +- Sort (12)\n",
      "      :                    +- Exchange (11)\n",
      "      :                       +- Project (10)\n",
      "      :                          +- Filter (9)\n",
      "      :                             +- Scan csv  (8)\n",
      "      +- Sort (36)\n",
      "         +- HashAggregate (35)\n",
      "            +- Exchange (34)\n",
      "               +- HashAggregate (33)\n",
      "                  +- Project (32)\n",
      "                     +- RangeJoin (31)\n",
      "                        :- Union (25)\n",
      "                        :  :- Project (21)\n",
      "                        :  :  +- Filter (20)\n",
      "                        :  :     +- Scan csv  (19)\n",
      "                        :  +- Project (24)\n",
      "                        :     +- Filter (23)\n",
      "                        :        +- Scan csv  (22)\n",
      "                        +- Project (30)\n",
      "                           +- Filter (29)\n",
      "                              +- Generate (28)\n",
      "                                 +- Filter (27)\n",
      "                                    +- Scan geojson  (26)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#239]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#239]\n",
      "Condition : ((size(features#239, true) > 0) AND isnotnull(features#239))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#239]\n",
      "Arguments: explode(features#239), false, [features#247]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : ((isnotnull(features#247.properties.POP_2010) AND (isnotnull(features#247.properties.COMM) AND (features#247.properties.POP_2010 > 0))) AND isnotnull(features#247.properties.ZCTA10))\n",
      "\n",
      "(5) Project\n",
      "Output [3]: [features#247.properties.COMM AS COMM#263, features#247.properties.ZCTA10 AS ZCTA10#280, features#247.properties.POP_2010 AS POP_2010#272L]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(6) Exchange\n",
      "Input [3]: [COMM#263, ZCTA10#280, POP_2010#272L]\n",
      "Arguments: hashpartitioning(ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=3008]\n",
      "\n",
      "(7) Sort\n",
      "Input [3]: [COMM#263, ZCTA10#280, POP_2010#272L]\n",
      "Arguments: [ZCTA10#280 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(8) Scan csv \n",
      "Output [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "(9) Filter\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Condition : isnotnull(Zip Code#24)\n",
      "\n",
      "(10) Project\n",
      "Output [2]: [Zip Code#24, cast(regexp_replace(Estimated Median Income#26, [$,], , 1) as float) AS Estimated Median Income#30]\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "\n",
      "(11) Exchange\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#30]\n",
      "Arguments: hashpartitioning(Zip Code#24, 1000), ENSURE_REQUIREMENTS, [plan_id=3009]\n",
      "\n",
      "(12) Sort\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#30]\n",
      "Arguments: [Zip Code#24 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(13) SortMergeJoin\n",
      "Left keys [1]: [ZCTA10#280]\n",
      "Right keys [1]: [Zip Code#24]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(14) Project\n",
      "Output [3]: [COMM#263, POP_2010#272L, Estimated Median Income#30]\n",
      "Input [5]: [COMM#263, ZCTA10#280, POP_2010#272L, Zip Code#24, Estimated Median Income#30]\n",
      "\n",
      "(15) HashAggregate\n",
      "Input [3]: [COMM#263, POP_2010#272L, Estimated Median Income#30]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [2]: [partial_sum(POP_2010#272L), partial_sum(Estimated Median Income#30)]\n",
      "Aggregate Attributes [2]: [sum#993L, sum#995]\n",
      "Results [3]: [COMM#263, sum#994L, sum#996]\n",
      "\n",
      "(16) Exchange\n",
      "Input [3]: [COMM#263, sum#994L, sum#996]\n",
      "Arguments: hashpartitioning(COMM#263, 1000), ENSURE_REQUIREMENTS, [plan_id=3016]\n",
      "\n",
      "(17) HashAggregate\n",
      "Input [3]: [COMM#263, sum#994L, sum#996]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [2]: [sum(POP_2010#272L), sum(Estimated Median Income#30)]\n",
      "Aggregate Attributes [2]: [sum(POP_2010#272L)#903L, sum(Estimated Median Income#30)#905]\n",
      "Results [3]: [COMM#263, sum(POP_2010#272L)#903L AS Population#904L, sum(Estimated Median Income#30)#905 AS Total Income#906]\n",
      "\n",
      "(18) Sort\n",
      "Input [3]: [COMM#263, Population#904L, Total Income#906]\n",
      "Arguments: [COMM#263 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(19) Scan csv \n",
      "Output [2]: [LAT#74, LON#75]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(20) Filter\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(21) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#188]\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "\n",
      "(22) Scan csv \n",
      "Output [2]: [LAT#130, LON#131]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(23) Filter\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(24) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#1032]\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "\n",
      "(25) Union\n",
      "\n",
      "(26) Scan geojson \n",
      "Output [1]: [features#933]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(27) Filter\n",
      "Input [1]: [features#933]\n",
      "Condition : ((size(features#933, true) > 0) AND isnotnull(features#933))\n",
      "\n",
      "(28) Generate\n",
      "Input [1]: [features#933]\n",
      "Arguments: explode(features#933), false, [features#247]\n",
      "\n",
      "(29) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : ((((isnotnull(features#247.properties.POP_2010) AND isnotnull(features#247.properties.COMM)) AND (features#247.properties.POP_2010 > 0)) AND isnotnull(features#247.geometry)) AND bloomfilter#1033 of [bf1033 COMM#263 estimatedNumRows=294857] filtering [features#247.properties.COMM])\n",
      "\n",
      "(30) Project\n",
      "Output [2]: [features#247.properties.COMM AS COMM#943, features#247.geometry AS geometry#250]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(31) RangeJoin\n",
      "Arguments: geom#188: geometry, geometry#250: geometry, WITHIN\n",
      "\n",
      "(32) Project\n",
      "Output [1]: [COMM#943]\n",
      "Input [3]: [geom#188, COMM#943, geometry#250]\n",
      "\n",
      "(33) HashAggregate\n",
      "Input [1]: [COMM#943]\n",
      "Keys [1]: [COMM#943]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#997L]\n",
      "Results [2]: [COMM#943, count#998L]\n",
      "\n",
      "(34) Exchange\n",
      "Input [2]: [COMM#943, count#998L]\n",
      "Arguments: hashpartitioning(COMM#943, 1000), ENSURE_REQUIREMENTS, [plan_id=3104]\n",
      "\n",
      "(35) HashAggregate\n",
      "Input [2]: [COMM#943, count#998L]\n",
      "Keys [1]: [COMM#943]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#928L]\n",
      "Results [2]: [COMM#943, count(1)#928L AS Total Crimes#929L]\n",
      "\n",
      "(36) Sort\n",
      "Input [2]: [COMM#943, Total Crimes#929L]\n",
      "Arguments: [COMM#943 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(37) SortMergeJoin\n",
      "Left keys [1]: [COMM#263]\n",
      "Right keys [1]: [COMM#943]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(38) Project\n",
      "Output [3]: [COMM#263, (Total Income#906 / cast(Population#904L as double)) AS Income per Person#966, (cast(Total Crimes#929L as double) / cast(Population#904L as double)) AS Crimes per Person#972]\n",
      "Input [5]: [COMM#263, Population#904L, Total Income#906, COMM#943, Total Crimes#929L]\n",
      "\n",
      "(39) AdaptiveSparkPlan\n",
      "Output [3]: [COMM#263, Income per Person#966, Crimes per Person#972]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "===== Subqueries =====\n",
      "\n",
      "Subquery:1 Hosting operator id = 29 Hosting Expression = bloomfilter#1033 of [bf1033 COMM#263 estimatedNumRows=294857] filtering [features#247.properties.COMM]\n",
      "OutputAdapter (47)\n",
      "+- AdaptiveSparkPlan (46)\n",
      "   +- Exchange (45)\n",
      "      +- Project (44)\n",
      "         +- Filter (43)\n",
      "            +- Generate (42)\n",
      "               +- Filter (41)\n",
      "                  +- Scan geojson  (40)\n",
      "\n",
      "\n",
      "(40) Scan geojson \n",
      "Output [1]: [features#239]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(41) Filter\n",
      "Input [1]: [features#239]\n",
      "Condition : ((size(features#239, true) > 0) AND isnotnull(features#239))\n",
      "\n",
      "(42) Generate\n",
      "Input [1]: [features#239]\n",
      "Arguments: explode(features#239), false, [features#247]\n",
      "\n",
      "(43) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : ((isnotnull(features#247.properties.POP_2010) AND (isnotnull(features#247.properties.COMM) AND (features#247.properties.POP_2010 > 0))) AND isnotnull(features#247.properties.ZCTA10))\n",
      "\n",
      "(44) Project\n",
      "Output [3]: [features#247.properties.COMM AS COMM#263, features#247.properties.ZCTA10 AS ZCTA10#280, features#247.properties.POP_2010 AS POP_2010#272L]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(45) Exchange\n",
      "Input [3]: [COMM#263, ZCTA10#280, POP_2010#272L]\n",
      "Arguments: hashpartitioning(ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=3095]\n",
      "\n",
      "(46) AdaptiveSparkPlan\n",
      "Output [3]: [COMM#263, ZCTA10#280, POP_2010#272L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "(47) OutputAdapter\n",
      "Output [3]: [COMM#263, ZCTA10#280, POP_2010#272L]"
     ]
    }
   ],
   "source": [
    "result.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SHUFFLE HASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+\n",
      "|              COMM| Income per Person|   Crimes per Person|\n",
      "+------------------+------------------+--------------------+\n",
      "|    Vermont Knolls|208.44291881520567|  1.0672142942798897|\n",
      "|     Vermont Vista|146.19928095872172|   1.256750998668442|\n",
      "|      Harvard Park| 260.5980739155486|  1.0106455949735231|\n",
      "|Florence-Firestone|213.46142986696086| 0.45668047232168735|\n",
      "|       Pico Rivera| 456.6046995646786|3.177528518318452E-5|\n",
      "|        Montebello|        395.805744|              9.6E-5|\n",
      "|   Rowland Heights|384.75918600383346|4.078137106969536...|\n",
      "|     Boyle Heights|  169.583309101483|  0.6253271299796452|\n",
      "|           Maywood|167.33027924803795|3.650301149844862E-5|\n",
      "|            Walnut| 656.5997531879884|6.855889208830385E-5|\n",
      "|          Commerce| 501.5481556578024|2.339546128051158...|\n",
      "|           Central|135.04158432612502|  0.6675512393427814|\n",
      "|  East Los Angeles|258.15686367517316|0.002759067767131518|\n",
      "|       Diamond Bar|501.40065533630997|1.800374477891401...|\n",
      "|    Vernon Central|116.06856533595784|   0.663406532570585|\n",
      "|Wholesale District|183.52424911185273|  1.6380665303046613|\n",
      "|      Little Tokyo|188.61754282339044|   3.617838157117543|\n",
      "|   Huntington Park|197.63943283890285|1.548680180335203...|\n",
      "|         La Puente| 359.2414607193088| 7.53465943339361E-5|\n",
      "|            Encino| 569.3943422512929|  0.6293655104016623|\n",
      "+------------------+------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, col\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Σύνδεση Εισοδήματος / Περιοχών με βάση το ZIP Code\n",
    "income_per_block = blocks_data  \\\n",
    "                        .hint(\"SHUFFLE_HASH\") \\\n",
    "                        .join(income_data, blocks_data[\"ZCTA10\"] == income_data[\"Zip Code\"]) \\\n",
    "                        .groupBy(\"COMM\") \\\n",
    "                        .agg( \\\n",
    "                            sum(\"POP_2010\").alias(\"Population\"), \\\n",
    "                            sum(\"Estimated Median Income\").alias(\"Total Income\") \\\n",
    "                        )\n",
    "\n",
    "# Σύνδεση Εγκλημάτων / Περιοχών με βάση το geometry, δηλαδή\n",
    "# το POINT του εγκλήματος βρίσκεται εντός του POLYGON της περιοχής\n",
    "crimes_per_block = crime_data \\\n",
    "                        .hint(\"SHUFFLE_HASH\") \\\n",
    "                        .join(blocks_data, ST_Within(crime_data[\"geom\"], blocks_data[\"geometry\"]), \"inner\") \\\n",
    "                        .groupBy(\"COMM\") \\\n",
    "                        .agg(count(\"*\").alias(\"Total Crimes\"))\n",
    "\n",
    "# Aναλογία συνολικού αριθμού εγκλημάτων ανά άτομο\n",
    "result = income_per_block \\\n",
    "                .hint(\"SHUFFLE_HASH\") \\\n",
    "                .join(crimes_per_block, on=[\"COMM\"]) \\\n",
    "                .withColumn(\"Income per Person\", col(\"Total Income\") / col(\"Population\")) \\\n",
    "                .withColumn(\"Crimes per Person\", col(\"Total Crimes\") / col(\"Population\")) \\\n",
    "                .select(\"COMM\", \"Income per Person\", \"Crimes per Person\")\n",
    "\n",
    "result.show()\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 27.28 seconds"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (35)\n",
      "+- Project (34)\n",
      "   +- ShuffledHashJoin Inner BuildLeft (33)\n",
      "      :- HashAggregate (15)\n",
      "      :  +- Exchange (14)\n",
      "      :     +- HashAggregate (13)\n",
      "      :        +- Project (12)\n",
      "      :           +- ShuffledHashJoin Inner BuildLeft (11)\n",
      "      :              :- Exchange (6)\n",
      "      :              :  +- Project (5)\n",
      "      :              :     +- Filter (4)\n",
      "      :              :        +- Generate (3)\n",
      "      :              :           +- Filter (2)\n",
      "      :              :              +- Scan geojson  (1)\n",
      "      :              +- Exchange (10)\n",
      "      :                 +- Project (9)\n",
      "      :                    +- Filter (8)\n",
      "      :                       +- Scan csv  (7)\n",
      "      +- HashAggregate (32)\n",
      "         +- Exchange (31)\n",
      "            +- HashAggregate (30)\n",
      "               +- Project (29)\n",
      "                  +- RangeJoin (28)\n",
      "                     :- Union (22)\n",
      "                     :  :- Project (18)\n",
      "                     :  :  +- Filter (17)\n",
      "                     :  :     +- Scan csv  (16)\n",
      "                     :  +- Project (21)\n",
      "                     :     +- Filter (20)\n",
      "                     :        +- Scan csv  (19)\n",
      "                     +- Project (27)\n",
      "                        +- Filter (26)\n",
      "                           +- Generate (25)\n",
      "                              +- Filter (24)\n",
      "                                 +- Scan geojson  (23)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#239]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#239]\n",
      "Condition : ((size(features#239, true) > 0) AND isnotnull(features#239))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#239]\n",
      "Arguments: explode(features#239), false, [features#247]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : ((isnotnull(features#247.properties.POP_2010) AND (isnotnull(features#247.properties.COMM) AND (features#247.properties.POP_2010 > 0))) AND isnotnull(features#247.properties.ZCTA10))\n",
      "\n",
      "(5) Project\n",
      "Output [3]: [features#247.properties.COMM AS COMM#263, features#247.properties.ZCTA10 AS ZCTA10#280, features#247.properties.POP_2010 AS POP_2010#272L]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(6) Exchange\n",
      "Input [3]: [COMM#263, ZCTA10#280, POP_2010#272L]\n",
      "Arguments: hashpartitioning(ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=4506]\n",
      "\n",
      "(7) Scan csv \n",
      "Output [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "(8) Filter\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Condition : isnotnull(Zip Code#24)\n",
      "\n",
      "(9) Project\n",
      "Output [2]: [Zip Code#24, cast(regexp_replace(Estimated Median Income#26, [$,], , 1) as float) AS Estimated Median Income#30]\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "\n",
      "(10) Exchange\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#30]\n",
      "Arguments: hashpartitioning(Zip Code#24, 1000), ENSURE_REQUIREMENTS, [plan_id=4507]\n",
      "\n",
      "(11) ShuffledHashJoin\n",
      "Left keys [1]: [ZCTA10#280]\n",
      "Right keys [1]: [Zip Code#24]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(12) Project\n",
      "Output [3]: [COMM#263, POP_2010#272L, Estimated Median Income#30]\n",
      "Input [5]: [COMM#263, ZCTA10#280, POP_2010#272L, Zip Code#24, Estimated Median Income#30]\n",
      "\n",
      "(13) HashAggregate\n",
      "Input [3]: [COMM#263, POP_2010#272L, Estimated Median Income#30]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [2]: [partial_sum(POP_2010#272L), partial_sum(Estimated Median Income#30)]\n",
      "Aggregate Attributes [2]: [sum#1295L, sum#1297]\n",
      "Results [3]: [COMM#263, sum#1296L, sum#1298]\n",
      "\n",
      "(14) Exchange\n",
      "Input [3]: [COMM#263, sum#1296L, sum#1298]\n",
      "Arguments: hashpartitioning(COMM#263, 1000), ENSURE_REQUIREMENTS, [plan_id=4512]\n",
      "\n",
      "(15) HashAggregate\n",
      "Input [3]: [COMM#263, sum#1296L, sum#1298]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [2]: [sum(POP_2010#272L), sum(Estimated Median Income#30)]\n",
      "Aggregate Attributes [2]: [sum(POP_2010#272L)#1205L, sum(Estimated Median Income#30)#1207]\n",
      "Results [3]: [COMM#263, sum(POP_2010#272L)#1205L AS Population#1206L, sum(Estimated Median Income#30)#1207 AS Total Income#1208]\n",
      "\n",
      "(16) Scan csv \n",
      "Output [2]: [LAT#74, LON#75]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(17) Filter\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(18) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#188]\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "\n",
      "(19) Scan csv \n",
      "Output [2]: [LAT#130, LON#131]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(20) Filter\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(21) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#1334]\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "\n",
      "(22) Union\n",
      "\n",
      "(23) Scan geojson \n",
      "Output [1]: [features#1235]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(24) Filter\n",
      "Input [1]: [features#1235]\n",
      "Condition : ((size(features#1235, true) > 0) AND isnotnull(features#1235))\n",
      "\n",
      "(25) Generate\n",
      "Input [1]: [features#1235]\n",
      "Arguments: explode(features#1235), false, [features#247]\n",
      "\n",
      "(26) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : ((((isnotnull(features#247.properties.POP_2010) AND isnotnull(features#247.properties.COMM)) AND (features#247.properties.POP_2010 > 0)) AND isnotnull(features#247.geometry)) AND bloomfilter#1335 of [bf1335 COMM#263 estimatedNumRows=294857] filtering [features#247.properties.COMM])\n",
      "\n",
      "(27) Project\n",
      "Output [2]: [features#247.properties.COMM AS COMM#1245, features#247.geometry AS geometry#250]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(28) RangeJoin\n",
      "Arguments: geom#188: geometry, geometry#250: geometry, WITHIN\n",
      "\n",
      "(29) Project\n",
      "Output [1]: [COMM#1245]\n",
      "Input [3]: [geom#188, COMM#1245, geometry#250]\n",
      "\n",
      "(30) HashAggregate\n",
      "Input [1]: [COMM#1245]\n",
      "Keys [1]: [COMM#1245]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#1299L]\n",
      "Results [2]: [COMM#1245, count#1300L]\n",
      "\n",
      "(31) Exchange\n",
      "Input [2]: [COMM#1245, count#1300L]\n",
      "Arguments: hashpartitioning(COMM#1245, 1000), ENSURE_REQUIREMENTS, [plan_id=4593]\n",
      "\n",
      "(32) HashAggregate\n",
      "Input [2]: [COMM#1245, count#1300L]\n",
      "Keys [1]: [COMM#1245]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#1230L]\n",
      "Results [2]: [COMM#1245, count(1)#1230L AS Total Crimes#1231L]\n",
      "\n",
      "(33) ShuffledHashJoin\n",
      "Left keys [1]: [COMM#263]\n",
      "Right keys [1]: [COMM#1245]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(34) Project\n",
      "Output [3]: [COMM#263, (Total Income#1208 / cast(Population#1206L as double)) AS Income per Person#1268, (cast(Total Crimes#1231L as double) / cast(Population#1206L as double)) AS Crimes per Person#1274]\n",
      "Input [5]: [COMM#263, Population#1206L, Total Income#1208, COMM#1245, Total Crimes#1231L]\n",
      "\n",
      "(35) AdaptiveSparkPlan\n",
      "Output [3]: [COMM#263, Income per Person#1268, Crimes per Person#1274]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "===== Subqueries =====\n",
      "\n",
      "Subquery:1 Hosting operator id = 26 Hosting Expression = bloomfilter#1335 of [bf1335 COMM#263 estimatedNumRows=294857] filtering [features#247.properties.COMM]\n",
      "OutputAdapter (43)\n",
      "+- AdaptiveSparkPlan (42)\n",
      "   +- Exchange (41)\n",
      "      +- Project (40)\n",
      "         +- Filter (39)\n",
      "            +- Generate (38)\n",
      "               +- Filter (37)\n",
      "                  +- Scan geojson  (36)\n",
      "\n",
      "\n",
      "(36) Scan geojson \n",
      "Output [1]: [features#239]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(37) Filter\n",
      "Input [1]: [features#239]\n",
      "Condition : ((size(features#239, true) > 0) AND isnotnull(features#239))\n",
      "\n",
      "(38) Generate\n",
      "Input [1]: [features#239]\n",
      "Arguments: explode(features#239), false, [features#247]\n",
      "\n",
      "(39) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : ((isnotnull(features#247.properties.POP_2010) AND (isnotnull(features#247.properties.COMM) AND (features#247.properties.POP_2010 > 0))) AND isnotnull(features#247.properties.ZCTA10))\n",
      "\n",
      "(40) Project\n",
      "Output [3]: [features#247.properties.COMM AS COMM#263, features#247.properties.ZCTA10 AS ZCTA10#280, features#247.properties.POP_2010 AS POP_2010#272L]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(41) Exchange\n",
      "Input [3]: [COMM#263, ZCTA10#280, POP_2010#272L]\n",
      "Arguments: hashpartitioning(ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=4584]\n",
      "\n",
      "(42) AdaptiveSparkPlan\n",
      "Output [3]: [COMM#263, ZCTA10#280, POP_2010#272L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "(43) OutputAdapter\n",
      "Output [3]: [COMM#263, ZCTA10#280, POP_2010#272L]"
     ]
    }
   ],
   "source": [
    "result.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. SHUFFLE REPLICATE NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+\n",
      "|                COMM| Income per Person|   Crimes per Person|\n",
      "+--------------------+------------------+--------------------+\n",
      "|         Culver City| 792.7922742586735| 0.00887277216264177|\n",
      "|Rosewood/East Gar...|366.36426116838487| 0.08676975945017182|\n",
      "|      Toluca Terrace| 260.9477325134512| 0.22213681783243658|\n",
      "|        Elysian Park| 189.6804632618189|  0.6058477311562559|\n",
      "|            Longwood|209.40380047505937|  0.7273159144893112|\n",
      "|         Pico Rivera| 456.6046995646786|3.177528518318452E-5|\n",
      "|              Malibu|1807.5768287860815|7.908264136022143E-5|\n",
      "|       Green Meadows|246.80379395590535|  1.1079662983704153|\n",
      "|    Cadillac-Corning|223.08117029257315|   0.581695423855964|\n",
      "|          Montebello|        395.805744|              9.6E-5|\n",
      "|            Mid-city|396.23837087663014|  0.7106492781923426|\n",
      "|     Lincoln Heights|207.09009761109684|  0.5137105060364757|\n",
      "|    Westlake Village| 835.2990326481257|1.209189842805320...|\n",
      "|            Van Nuys|148.04583871005244|   0.787558562643137|\n",
      "|              Carson|480.76148679590904|0.002496892513683...|\n",
      "|     Rowland Heights|384.75918600383346|4.078137106969536...|\n",
      "|        Agoura Hills| 787.9183472700443|2.951303492375799...|\n",
      "|            Glendale| 435.6755146855554|4.277093037205493...|\n",
      "|      Gramercy Place| 488.9200849338867|  1.0647620886014864|\n",
      "|   Faircrest Heights| 624.6238745280278|  0.7290153935521347|\n",
      "+--------------------+------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, col\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Σύνδεση Εισοδήματος / Περιοχών\n",
    "income_per_block = blocks_data  \\\n",
    "                        .hint(\"SHUFFLE_REPLICATE_NL\") \\\n",
    "                        .join(income_data, blocks_data[\"ZCTA10\"] == income_data[\"Zip Code\"]) \\\n",
    "                        .groupBy(\"COMM\") \\\n",
    "                        .agg( \\\n",
    "                            sum(\"POP_2010\").alias(\"Population\"), \\\n",
    "                            sum(\"Estimated Median Income\").alias(\"Total Income\") \\\n",
    "                        )\n",
    "\n",
    "# Σύνδεση Εγκλημάτων / Περιοχών με βάση το geometry, δηλαδή\n",
    "# το POINT του εγκλήματος βρίσκεται εντός του POLYGON της περιοχής\n",
    "crimes_per_block = crime_data \\\n",
    "                        .hint(\"SHUFFLE_REPLICATE_NL\") \\\n",
    "                        .join(blocks_data, ST_Within(crime_data[\"geom\"], blocks_data[\"geometry\"]), \"inner\") \\\n",
    "                        .groupBy(\"COMM\") \\\n",
    "                        .agg(count(\"*\").alias(\"Total Crimes\"))\n",
    "\n",
    "# Aναλογία συνολικού αριθμού εγκλημάτων ανά άτομο\n",
    "result = income_per_block \\\n",
    "                .hint(\"SHUFFLE_REPLICATE_NL\") \\\n",
    "                .join(crimes_per_block, on=[\"COMM\"]) \\\n",
    "                .withColumn(\"Income per Person\", col(\"Total Income\") / col(\"Population\")) \\\n",
    "                .withColumn(\"Crimes per Person\", col(\"Total Crimes\") / col(\"Population\")) \\\n",
    "                .select(\"COMM\", \"Income per Person\", \"Crimes per Person\")\n",
    "\n",
    "result.show()\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 18.54 seconds"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (33)\n",
      "+- Project (32)\n",
      "   +- CartesianProduct Inner (31)\n",
      "      :- HashAggregate (13)\n",
      "      :  +- Exchange (12)\n",
      "      :     +- HashAggregate (11)\n",
      "      :        +- Project (10)\n",
      "      :           +- CartesianProduct Inner (9)\n",
      "      :              :- Project (5)\n",
      "      :              :  +- Filter (4)\n",
      "      :              :     +- Generate (3)\n",
      "      :              :        +- Filter (2)\n",
      "      :              :           +- Scan geojson  (1)\n",
      "      :              +- Project (8)\n",
      "      :                 +- Filter (7)\n",
      "      :                    +- Scan csv  (6)\n",
      "      +- HashAggregate (30)\n",
      "         +- Exchange (29)\n",
      "            +- HashAggregate (28)\n",
      "               +- Project (27)\n",
      "                  +- RangeJoin (26)\n",
      "                     :- Union (20)\n",
      "                     :  :- Project (16)\n",
      "                     :  :  +- Filter (15)\n",
      "                     :  :     +- Scan csv  (14)\n",
      "                     :  +- Project (19)\n",
      "                     :     +- Filter (18)\n",
      "                     :        +- Scan csv  (17)\n",
      "                     +- Project (25)\n",
      "                        +- Filter (24)\n",
      "                           +- Generate (23)\n",
      "                              +- Filter (22)\n",
      "                                 +- Scan geojson  (21)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#239]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#239]\n",
      "Condition : ((size(features#239, true) > 0) AND isnotnull(features#239))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#239]\n",
      "Arguments: explode(features#239), false, [features#247]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : ((isnotnull(features#247.properties.POP_2010) AND (isnotnull(features#247.properties.COMM) AND (features#247.properties.POP_2010 > 0))) AND isnotnull(features#247.properties.ZCTA10))\n",
      "\n",
      "(5) Project\n",
      "Output [3]: [features#247.properties.COMM AS COMM#263, features#247.properties.ZCTA10 AS ZCTA10#280, features#247.properties.POP_2010 AS POP_2010#272L]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(6) Scan csv \n",
      "Output [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "(7) Filter\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Condition : isnotnull(Zip Code#24)\n",
      "\n",
      "(8) Project\n",
      "Output [2]: [Zip Code#24, cast(regexp_replace(Estimated Median Income#26, [$,], , 1) as float) AS Estimated Median Income#30]\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "\n",
      "(9) CartesianProduct\n",
      "Join type: Inner\n",
      "Join condition: (ZCTA10#280 = Zip Code#24)\n",
      "\n",
      "(10) Project\n",
      "Output [3]: [COMM#263, POP_2010#272L, Estimated Median Income#30]\n",
      "Input [5]: [COMM#263, ZCTA10#280, POP_2010#272L, Zip Code#24, Estimated Median Income#30]\n",
      "\n",
      "(11) HashAggregate\n",
      "Input [3]: [COMM#263, POP_2010#272L, Estimated Median Income#30]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [2]: [partial_sum(POP_2010#272L), partial_sum(Estimated Median Income#30)]\n",
      "Aggregate Attributes [2]: [sum#1450L, sum#1452]\n",
      "Results [3]: [COMM#263, sum#1451L, sum#1453]\n",
      "\n",
      "(12) Exchange\n",
      "Input [3]: [COMM#263, sum#1451L, sum#1453]\n",
      "Arguments: hashpartitioning(COMM#263, 1000), ENSURE_REQUIREMENTS, [plan_id=5206]\n",
      "\n",
      "(13) HashAggregate\n",
      "Input [3]: [COMM#263, sum#1451L, sum#1453]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [2]: [sum(POP_2010#272L), sum(Estimated Median Income#30)]\n",
      "Aggregate Attributes [2]: [sum(POP_2010#272L)#1360L, sum(Estimated Median Income#30)#1362]\n",
      "Results [3]: [COMM#263, sum(POP_2010#272L)#1360L AS Population#1361L, sum(Estimated Median Income#30)#1362 AS Total Income#1363]\n",
      "\n",
      "(14) Scan csv \n",
      "Output [2]: [LAT#74, LON#75]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(15) Filter\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(16) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#188]\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "\n",
      "(17) Scan csv \n",
      "Output [2]: [LAT#130, LON#131]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(18) Filter\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(19) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#1487]\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "\n",
      "(20) Union\n",
      "\n",
      "(21) Scan geojson \n",
      "Output [1]: [features#1390]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(22) Filter\n",
      "Input [1]: [features#1390]\n",
      "Condition : ((size(features#1390, true) > 0) AND isnotnull(features#1390))\n",
      "\n",
      "(23) Generate\n",
      "Input [1]: [features#1390]\n",
      "Arguments: explode(features#1390), false, [features#247]\n",
      "\n",
      "(24) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : (((isnotnull(features#247.properties.POP_2010) AND isnotnull(features#247.properties.COMM)) AND (features#247.properties.POP_2010 > 0)) AND isnotnull(features#247.geometry))\n",
      "\n",
      "(25) Project\n",
      "Output [2]: [features#247.properties.COMM AS COMM#1400, features#247.geometry AS geometry#250]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(26) RangeJoin\n",
      "Arguments: geom#188: geometry, geometry#250: geometry, WITHIN\n",
      "\n",
      "(27) Project\n",
      "Output [1]: [COMM#1400]\n",
      "Input [3]: [geom#188, COMM#1400, geometry#250]\n",
      "\n",
      "(28) HashAggregate\n",
      "Input [1]: [COMM#1400]\n",
      "Keys [1]: [COMM#1400]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#1454L]\n",
      "Results [2]: [COMM#1400, count#1455L]\n",
      "\n",
      "(29) Exchange\n",
      "Input [2]: [COMM#1400, count#1455L]\n",
      "Arguments: hashpartitioning(COMM#1400, 1000), ENSURE_REQUIREMENTS, [plan_id=5274]\n",
      "\n",
      "(30) HashAggregate\n",
      "Input [2]: [COMM#1400, count#1455L]\n",
      "Keys [1]: [COMM#1400]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#1385L]\n",
      "Results [2]: [COMM#1400, count(1)#1385L AS Total Crimes#1386L]\n",
      "\n",
      "(31) CartesianProduct\n",
      "Join type: Inner\n",
      "Join condition: (COMM#263 = COMM#1400)\n",
      "\n",
      "(32) Project\n",
      "Output [3]: [COMM#263, (Total Income#1363 / cast(Population#1361L as double)) AS Income per Person#1423, (cast(Total Crimes#1386L as double) / cast(Population#1361L as double)) AS Crimes per Person#1429]\n",
      "Input [5]: [COMM#263, Population#1361L, Total Income#1363, COMM#1400, Total Crimes#1386L]\n",
      "\n",
      "(33) AdaptiveSparkPlan\n",
      "Output [3]: [COMM#263, Income per Person#1423, Crimes per Person#1429]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "result.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Συμπεράσματα\n",
    "\n",
    "Η στρατηγική join που πετυχαίνει την καλύτερη επίδοση είναι η **SHUFFLE REPLICATE NL** (*Shuffle Replicate Nested Loop*) με χρόνο 18.54 δευτερόλεπτα.\n",
    "\n",
    "H στρατηγική **SHUFFLE HASH** πέτυχε λίγο καλύτερο χρόνο από τη **MERGE** με 27.28 και 31.77 δευτερόλεπτα αντίστοιχα.\n",
    "\n",
    "Τέλος, η στρατηγική **BROADCAST** δε μπόρεσε να εκτελεστεί, καθώς εξαντλεί τους πόρους μνήμης. Αυτό συμβαίνει επειδή προσπαθεί να στείλει ολόκληρο το μικρότερο dataset σε όλους τους εκτελεστές (*executors*), για να εκτελεστεί το join τοπικά χωρίς shuffling. Έτσι η μέθοδος αποτυγχάνει, καθώς το μέγεθος του dataset που πρόκειται να μεταδοθεί (broadcast) είναι πολύ μεγάλο για να χωρέσει στη μνήμη των εκτελεστών.\n",
    "\n",
    "Συνεπώς η καταλληλότερη στρατηγική για την περίπτωσή μας είναι η **SHUFFLE_REPLICATE_NL**."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "name": "SparkLab - Introduction to RDDs and DataFrames"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
