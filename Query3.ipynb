{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Προχωρημένα Θέματα Βάσεων Δεδομένων\n",
    "\n",
    "**Ονοματεπώνυμο:** Κωνσταντίνος Διβριώτης\n",
    "\n",
    "**ΑΜ:** 03114140\n",
    "\n",
    "## Query 3: \n",
    "\n",
    "Χρησιμοποιώντας ως αναφορά τα δεδομένα της απογραφής 2010 για τον πληθυσμό και εκείνα της απογραφής του 2015 για το εισόδημα ανα νοικοκυριό, να υπολογίσετε για κάθε περιοχή του Los Angeles τα παρακάτω:\n",
    "- Το μέσο ετήσιο εισόδημα ανά άτομο\n",
    "- Την αναλογία συνολικού αριθμού εγκλημάτων ανά άτομο"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2uaG6GoZthpT",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>118</td><td>application_1738075734771_0119</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0119/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-245.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0119_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.spark import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"CensusDataAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_BUCKET = \"s3://initial-notebook-data-bucket-dblab-905418150721\"\n",
    "GROUP_BUCKET = \"s3://groups-bucket-dblab-905418150721/group15\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Διάβασμα και Επισκόπηση αρχείων εισόδου"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+\n",
      "|Zip Code|Estimated Median Income|\n",
      "+--------+-----------------------+\n",
      "|   90001|                33887.0|\n",
      "|   90002|                30413.0|\n",
      "|   90003|                30805.0|\n",
      "|   90004|                40612.0|\n",
      "|   90005|                31142.0|\n",
      "+--------+-----------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "income_schema = StructType([\n",
    "    StructField(\"Zip Code\", StringType()),\n",
    "    StructField(\"Community\", StringType()),\n",
    "    StructField(\"Estimated Median Income\", StringType())\n",
    "])\n",
    "\n",
    "income_data = spark.read.csv(f\"{DATA_BUCKET}/LA_income_2015.csv\", header=True, schema=income_schema)\n",
    "\n",
    "# Μετατροπή του Estimated Median Income σε αριθμητική μορφή\n",
    "income_data = income_data \\\n",
    "    .withColumn(\n",
    "        \"Estimated Median Income\",\n",
    "        regexp_replace(col(\"Estimated Median Income\"), \"[$,]\", \"\").cast(\"float\")\n",
    "    ) \\\n",
    "    .select(\"Zip Code\", \"Estimated Median Income\")\n",
    "\n",
    "income_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|    DR_NO|                geom|\n",
      "+---------+--------------------+\n",
      "|001307355|POINT (-118.2695 ...|\n",
      "|011401303|POINT (-118.3962 ...|\n",
      "|070309629|POINT (-118.2524 ...|\n",
      "|090631215|POINT (-118.3295 ...|\n",
      "|100100501|POINT (-118.2488 ...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, DoubleType\n",
    "\n",
    "# Ορισμός του schema των dataset\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", IntegerType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", IntegerType()),\n",
    "    StructField(\"Crm Cd\", IntegerType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", IntegerType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", StringType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", IntegerType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", IntegerType()),\n",
    "    StructField(\"Crm Cd 2\", IntegerType()),\n",
    "    StructField(\"Crm Cd 3\", IntegerType()),\n",
    "    StructField(\"Crm Cd 4\", IntegerType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", DoubleType()),\n",
    "    StructField(\"LON\", DoubleType())\n",
    "])\n",
    "\n",
    "# Διαβάζουμε τα 2 datasets (2010-2019 και 2020-σήμερα) και τα συνενώνουμε σε 1\n",
    "crime_data_2010_2019 = spark.read.csv(f\"{DATA_BUCKET}/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, schema=crimes_schema)\n",
    "crime_data_2020_present = spark.read.csv(f\"{DATA_BUCKET}/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", header=True, schema=crimes_schema)\n",
    "crime_data = crime_data_2010_2019.union(crime_data_2020_present)\n",
    "\n",
    "# Μετατρέπουμε τις στήλες LAT, LON σε geometry με το ST_POINT\n",
    "crime_data = crime_data \\\n",
    "                .withColumn(\"geom\", ST_Point(\"LON\", \"LAT\")) \\\n",
    "                .filter(col(\"geom\") != ST_Point(0, 0)) \\\n",
    "                .select(\"DR_NO\", \"geom\")\n",
    "\n",
    "crime_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+----------+----------+--------------------+\n",
      "|         COMM|ZCTA10|Population|Households|            geometry|\n",
      "+-------------+------+----------+----------+--------------------+\n",
      "|  West Vernon| 90037|     30161|      8556|POLYGON ((-118.29...|\n",
      "|        Palms| 90232|      1533|       917|MULTIPOLYGON (((-...|\n",
      "|   Silverlake| 90039|     10521|      4753|MULTIPOLYGON (((-...|\n",
      "|     Westwood| 90025|      2443|      1317|MULTIPOLYGON (((-...|\n",
      "|South Carthay| 90035|      9428|      5250|POLYGON ((-118.36...|\n",
      "+-------------+------+----------+----------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\") \\\n",
    "            .load(f\"{DATA_BUCKET}/2010_Census_Blocks.geojson\") \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "# Preprocessing των δεδομένων του Census:\n",
    "# Για κάθε περιοχή του στο Census, υπάρχουν πολλαπλές\n",
    "# εγγραφές για COMM, ZCTA10. Συνεπώς, πριν κάνουμε το\n",
    "# join με το Income, πρέπει να κάνουμε group by για να\n",
    "# υπολογίσουμε το συνολικό πληθυσμό, το πλήθος των\n",
    "# νοικοκυριών και την καλυπτόμενη περιοχή ανά COMM, ZIP Code\n",
    "blocks_data = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                    blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\") \\\n",
    "            .filter(\n",
    "                col(\"COMM\").isNotNull() & (col(\"CITY\") == \"Los Angeles\") &\n",
    "                (col(\"HOUSING10\") > 0) & (col(\"POP_2010\") > 0)\n",
    "            ) \\\n",
    "            .groupBy(\"COMM\", \"ZCTA10\") \\\n",
    "            .agg(\n",
    "                sum(\"POP_2010\").alias(\"Population\"),\n",
    "                sum(\"HOUSING10\").alias(\"Households\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "            ) \\\n",
    "            .select(\"COMM\", \"ZCTA10\", \"Population\", \"Households\", \"geometry\")\n",
    "\n",
    "blocks_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------------------------------------------------------------------+\n",
      "|field    |type    |meaning                                                                  |\n",
      "+---------+--------+-------------------------------------------------------------------------+\n",
      "|COMM     |string  |Unincorporated area community name and LA City neighborhood              |\n",
      "|HOUSING10|long    |2010 housing (PL 94-171 Redistricting Data Summary File - Total Housing) |\n",
      "|POP_2010 |long    |Population (PL 94-171 Redistricting Data Summary File - Total Population)|\n",
      "|ZCTA10   |string  |Zip Code Tabulation Area                                                 |\n",
      "|geometry |geometry|Geometry of the block                                                    |\n",
      "+---------+--------+-------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "blocks_data_description_schema = StructType([\n",
    "    StructField(\"field\", StringType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"meaning\", StringType())\n",
    "])\n",
    "\n",
    "blocks_data_description = spark.read.csv(f\"{DATA_BUCKET}/2010_Census_Blocks_fields.csv\", header=True, schema=blocks_data_description_schema)\n",
    "\n",
    "blocks_data_description \\\n",
    "        .filter(col(\"field\").isin(\"COMM\", \"ZCTA10\", \"HOUSING10\", \"POP_2010\", \"geometry\")) \\\n",
    "        .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Υλοποίηση με DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------+-------------------+\n",
      "|COMM                   |Income per Person |Crimes per Person  |\n",
      "+-----------------------+------------------+-------------------+\n",
      "|Adams-Normandie        |8791.458301453711 |0.7148686559551135 |\n",
      "|Alsace                 |11239.50136425648 |0.5416098226466576 |\n",
      "|Angeles National Forest|33079.58823529412 |0.4117647058823529 |\n",
      "|Angelino Heights       |18427.059814658805|0.5732940185341197 |\n",
      "|Arleta                 |12110.779474388612|0.4264509064363061 |\n",
      "|Atwater Village        |28481.236967160792|0.5288318320448259 |\n",
      "|Baldwin Hills          |17303.906408241663|0.9950061114021302 |\n",
      "|Bel Air                |63041.33942621959 |0.39922527539038855|\n",
      "|Beverly Crest          |60947.48978754819 |0.3689607087195472 |\n",
      "|Beverlywood            |29267.82118405155 |0.5084977849375755 |\n",
      "|Boyle Heights          |8494.108286861341 |0.6171887393378809 |\n",
      "|Brentwood              |60846.854461055365|0.4058638814936173 |\n",
      "|Brookside              |18138.626409017714|0.8856682769726248 |\n",
      "|Cadillac-Corning       |19572.784696174043|0.581695423855964  |\n",
      "|Canoga Park            |19660.29173807544 |0.5506083179203503 |\n",
      "|Carthay                |49848.73592646053 |0.7628959963534149 |\n",
      "|Central                |6973.305663786775 |0.6593822350217403 |\n",
      "|Century City           |45617.760134566866|0.632968881412952  |\n",
      "|Century Palms/Cove     |8610.314655031003 |1.1446474853187232 |\n",
      "|Chatsworth             |30694.606443388573|0.5281029087959207 |\n",
      "+-----------------------+------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, col\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Kάνουμε join με το Income με βάση το ZIP Code,\n",
    "# οπότε έχουμε το μέσο ετήσιο εισόδημα ανά νοικοκυριό\n",
    "# για κάθε COMM, ZIP Code.\n",
    "# Κάνουμε group by με το COMM για να βρούμε το συνολικό\n",
    "# εισόδημα και τον συνολικό πληθυσμό ανά COMM.\n",
    "result = blocks_data \\\n",
    "            .join(income_data, col(\"ZCTA10\") == col(\"Zip Code\")) \\\n",
    "            .groupBy(\"COMM\") \\\n",
    "            .agg(\n",
    "                sum(\"Population\").alias(\"Population\"),\n",
    "                sum(col(\"Estimated Median Income\") * col(\"Households\")).alias(\"Total Income\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "            ) \\\n",
    "            .select(\"COMM\", \"Population\", \"Total Income\", \"geometry\")\n",
    "\n",
    "# Τέλος, κάνουμε join με τα εγκλήματα με βάση το geometry, δηλαδή\n",
    "# το POINT του εγκλήματος βρίσκεται εντός του POLYGON της περιοχής.\n",
    "# Κάνουμε group by με το COMM (και τα Population, Total Income τα οποία είναι\n",
    "# ίδια ανά εγγραφή) και υπολογίζουμε το συνολικό πλήθος των εγκλημάτων.\n",
    "# Μετά διαιρούμε με τον πληθυσμό για να βρούμε το εισόδημα ανά άτομο\n",
    "# και το πλήθος εγκλημάτων ανά άτομο.\n",
    "result = crime_data \\\n",
    "            .join(\n",
    "                result,\n",
    "                ST_Within(col(\"geom\"), col(\"geometry\"))\n",
    "            ) \\\n",
    "            .groupBy(\"COMM\", \"Population\", \"Total Income\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"Total Crimes\")\n",
    "            ) \\\n",
    "            .withColumn(\"Income per Person\", col(\"Total Income\") / col(\"Population\")) \\\n",
    "            .withColumn(\"Crimes per Person\", col(\"Total Crimes\") / col(\"Population\")) \\\n",
    "            .orderBy(\"COMM\") \\\n",
    "            .select(\"COMM\", \"Income per Person\", \"Crimes per Person\")\n",
    "\n",
    "result.show(truncate=False)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 31.09 seconds"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (33)\n",
      "+- Sort (32)\n",
      "   +- Exchange (31)\n",
      "      +- HashAggregate (30)\n",
      "         +- Exchange (29)\n",
      "            +- HashAggregate (28)\n",
      "               +- Project (27)\n",
      "                  +- RangeJoin (26)\n",
      "                     :- Union (7)\n",
      "                     :  :- Project (3)\n",
      "                     :  :  +- Filter (2)\n",
      "                     :  :     +- Scan csv  (1)\n",
      "                     :  +- Project (6)\n",
      "                     :     +- Filter (5)\n",
      "                     :        +- Scan csv  (4)\n",
      "                     +- Filter (25)\n",
      "                        +- ObjectHashAggregate (24)\n",
      "                           +- Exchange (23)\n",
      "                              +- ObjectHashAggregate (22)\n",
      "                                 +- Project (21)\n",
      "                                    +- BroadcastHashJoin Inner BuildRight (20)\n",
      "                                       :- ObjectHashAggregate (15)\n",
      "                                       :  +- Exchange (14)\n",
      "                                       :     +- ObjectHashAggregate (13)\n",
      "                                       :        +- Project (12)\n",
      "                                       :           +- Filter (11)\n",
      "                                       :              +- Generate (10)\n",
      "                                       :                 +- Filter (9)\n",
      "                                       :                    +- Scan geojson  (8)\n",
      "                                       +- BroadcastExchange (19)\n",
      "                                          +- Project (18)\n",
      "                                             +- Filter (17)\n",
      "                                                +- Scan csv  (16)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [2]: [LAT#1829, LON#1830]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [2]: [LAT#1829, LON#1830]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(3) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#1943]\n",
      "Input [2]: [LAT#1829, LON#1830]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [2]: [LAT#1885, LON#1886]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(5) Filter\n",
      "Input [2]: [LAT#1885, LON#1886]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(6) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#2425]\n",
      "Input [2]: [LAT#1885, LON#1886]\n",
      "\n",
      "(7) Union\n",
      "\n",
      "(8) Scan geojson \n",
      "Output [1]: [features#1994]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(9) Filter\n",
      "Input [1]: [features#1994]\n",
      "Condition : ((size(features#1994, true) > 0) AND isnotnull(features#1994))\n",
      "\n",
      "(10) Generate\n",
      "Input [1]: [features#1994]\n",
      "Arguments: explode(features#1994), false, [features#2002]\n",
      "\n",
      "(11) Filter\n",
      "Input [1]: [features#2002]\n",
      "Condition : ((((isnotnull(features#2002.properties.CITY) AND isnotnull(features#2002.properties.HOUSING10)) AND isnotnull(features#2002.properties.POP_2010)) AND (((isnotnull(features#2002.properties.COMM) AND (features#2002.properties.CITY = Los Angeles)) AND (features#2002.properties.HOUSING10 > 0)) AND (features#2002.properties.POP_2010 > 0))) AND isnotnull(features#2002.properties.ZCTA10))\n",
      "\n",
      "(12) Project\n",
      "Output [5]: [features#2002.properties.COMM AS COMM#2018, features#2002.properties.HOUSING10 AS HOUSING10#2024L, features#2002.properties.POP_2010 AS POP_2010#2027L, features#2002.properties.ZCTA10 AS ZCTA10#2035, features#2002.geometry AS geometry#2005]\n",
      "Input [1]: [features#2002]\n",
      "\n",
      "(13) ObjectHashAggregate\n",
      "Input [5]: [COMM#2018, HOUSING10#2024L, POP_2010#2027L, ZCTA10#2035, geometry#2005]\n",
      "Keys [2]: [COMM#2018, ZCTA10#2035]\n",
      "Functions [3]: [partial_sum(POP_2010#2027L), partial_sum(HOUSING10#2024L), partial_st_union_aggr(geometry#2005, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@71e790a2, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#2211L, sum#2213L, buf#2215]\n",
      "Results [5]: [COMM#2018, ZCTA10#2035, sum#2212L, sum#2214L, buf#2216]\n",
      "\n",
      "(14) Exchange\n",
      "Input [5]: [COMM#2018, ZCTA10#2035, sum#2212L, sum#2214L, buf#2216]\n",
      "Arguments: hashpartitioning(COMM#2018, ZCTA10#2035, 1000), ENSURE_REQUIREMENTS, [plan_id=6656]\n",
      "\n",
      "(15) ObjectHashAggregate\n",
      "Input [5]: [COMM#2018, ZCTA10#2035, sum#2212L, sum#2214L, buf#2216]\n",
      "Keys [2]: [COMM#2018, ZCTA10#2035]\n",
      "Functions [3]: [sum(POP_2010#2027L), sum(HOUSING10#2024L), st_union_aggr(geometry#2005, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@71e790a2, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(POP_2010#2027L)#2165L, sum(HOUSING10#2024L)#2167L, ST_Union_Aggr(geometry#2005)#2172]\n",
      "Results [5]: [COMM#2018, ZCTA10#2035, sum(POP_2010#2027L)#2165L AS Population#2166L, sum(HOUSING10#2024L)#2167L AS Households#2168L, ST_Union_Aggr(geometry#2005)#2172 AS geometry#2173]\n",
      "\n",
      "(16) Scan csv \n",
      "Output [2]: [Zip Code#1779, Estimated Median Income#1781]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "(17) Filter\n",
      "Input [2]: [Zip Code#1779, Estimated Median Income#1781]\n",
      "Condition : isnotnull(Zip Code#1779)\n",
      "\n",
      "(18) Project\n",
      "Output [2]: [Zip Code#1779, cast(regexp_replace(Estimated Median Income#1781, [$,], , 1) as float) AS Estimated Median Income#1785]\n",
      "Input [2]: [Zip Code#1779, Estimated Median Income#1781]\n",
      "\n",
      "(19) BroadcastExchange\n",
      "Input [2]: [Zip Code#1779, Estimated Median Income#1785]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=6659]\n",
      "\n",
      "(20) BroadcastHashJoin\n",
      "Left keys [1]: [ZCTA10#2035]\n",
      "Right keys [1]: [Zip Code#1779]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(21) Project\n",
      "Output [5]: [COMM#2018, Population#2166L, Households#2168L, geometry#2173, Estimated Median Income#1785]\n",
      "Input [7]: [COMM#2018, ZCTA10#2035, Population#2166L, Households#2168L, geometry#2173, Zip Code#1779, Estimated Median Income#1785]\n",
      "\n",
      "(22) ObjectHashAggregate\n",
      "Input [5]: [COMM#2018, Population#2166L, Households#2168L, geometry#2173, Estimated Median Income#1785]\n",
      "Keys [1]: [COMM#2018]\n",
      "Functions [3]: [partial_sum(Population#2166L), partial_sum((Estimated Median Income#1785 * cast(Households#2168L as float))), partial_st_union_aggr(geometry#2173, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@10a1fb22, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#2374L, sum#2376, buf#2378]\n",
      "Results [4]: [COMM#2018, sum#2375L, sum#2377, buf#2379]\n",
      "\n",
      "(23) Exchange\n",
      "Input [4]: [COMM#2018, sum#2375L, sum#2377, buf#2379]\n",
      "Arguments: hashpartitioning(COMM#2018, 1000), ENSURE_REQUIREMENTS, [plan_id=6664]\n",
      "\n",
      "(24) ObjectHashAggregate\n",
      "Input [4]: [COMM#2018, sum#2375L, sum#2377, buf#2379]\n",
      "Keys [1]: [COMM#2018]\n",
      "Functions [3]: [sum(Population#2166L), sum((Estimated Median Income#1785 * cast(Households#2168L as float))), st_union_aggr(geometry#2173, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@10a1fb22, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(Population#2166L)#2287L, sum((Estimated Median Income#1785 * cast(Households#2168L as float)))#2289, ST_Union_Aggr(geometry#2173)#2294]\n",
      "Results [4]: [COMM#2018, sum(Population#2166L)#2287L AS Population#2288L, sum((Estimated Median Income#1785 * cast(Households#2168L as float)))#2289 AS Total Income#2290, ST_Union_Aggr(geometry#2173)#2294 AS geometry#2295]\n",
      "\n",
      "(25) Filter\n",
      "Input [4]: [COMM#2018, Population#2288L, Total Income#2290, geometry#2295]\n",
      "Condition : isnotnull(geometry#2295)\n",
      "\n",
      "(26) RangeJoin\n",
      "Arguments: geom#1943: geometry, geometry#2295: geometry, WITHIN\n",
      "\n",
      "(27) Project\n",
      "Output [3]: [COMM#2018, Population#2288L, Total Income#2290]\n",
      "Input [5]: [geom#1943, COMM#2018, Population#2288L, Total Income#2290, geometry#2295]\n",
      "\n",
      "(28) HashAggregate\n",
      "Input [3]: [COMM#2018, Population#2288L, Total Income#2290]\n",
      "Keys [3]: [COMM#2018, Population#2288L, knownfloatingpointnormalized(normalizenanandzero(Total Income#2290)) AS Total Income#2290]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#2372L]\n",
      "Results [4]: [COMM#2018, Population#2288L, Total Income#2290, count#2373L]\n",
      "\n",
      "(29) Exchange\n",
      "Input [4]: [COMM#2018, Population#2288L, Total Income#2290, count#2373L]\n",
      "Arguments: hashpartitioning(COMM#2018, Population#2288L, Total Income#2290, 1000), ENSURE_REQUIREMENTS, [plan_id=6671]\n",
      "\n",
      "(30) HashAggregate\n",
      "Input [4]: [COMM#2018, Population#2288L, Total Income#2290, count#2373L]\n",
      "Keys [3]: [COMM#2018, Population#2288L, Total Income#2290]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#2340L]\n",
      "Results [3]: [COMM#2018, (Total Income#2290 / cast(Population#2288L as double)) AS Income per Person#2346, (cast(count(1)#2340L as double) / cast(Population#2288L as double)) AS Crimes per Person#2352]\n",
      "\n",
      "(31) Exchange\n",
      "Input [3]: [COMM#2018, Income per Person#2346, Crimes per Person#2352]\n",
      "Arguments: rangepartitioning(COMM#2018 ASC NULLS FIRST, 1000), ENSURE_REQUIREMENTS, [plan_id=6674]\n",
      "\n",
      "(32) Sort\n",
      "Input [3]: [COMM#2018, Income per Person#2346, Crimes per Person#2352]\n",
      "Arguments: [COMM#2018 ASC NULLS FIRST], true, 0\n",
      "\n",
      "(33) AdaptiveSparkPlan\n",
      "Output [3]: [COMM#2018, Income per Person#2346, Crimes per Person#2352]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "result.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα δοκιμάσουμε να αναγκάσουμε το Spark να χρησιμοποιήσει διαφορετικές στρατηγικές, ώστε να συγκρίνουμε την απόδοσή τους."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. BROADCAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-------------------+\n",
      "|               COMM| Income per Person|  Crimes per Person|\n",
      "+-------------------+------------------+-------------------+\n",
      "|       Hancock Park|21538.792826380406| 0.7797133123352832|\n",
      "|        Playa Vista| 50264.47143177235| 0.5004481290611696|\n",
      "|    Hollywood Hills| 43023.70025098602|  0.747615632843313|\n",
      "|       Toluca Woods|24517.584657534248| 0.6301369863013698|\n",
      "|         West Adams|10768.299623210249| 0.7500753579502637|\n",
      "|       Century City|45617.760134566866|  0.632968881412952|\n",
      "|     Harbor Gateway|16152.767540362767|0.46035977675901935|\n",
      "|          Thai Town|26851.472085561498| 0.5305882352941177|\n",
      "|Palisades Highlands| 66867.44038612054| 0.1878424210800939|\n",
      "|            Sunland| 26184.01260332687| 0.4484131033778957|\n",
      "|    Harvard Heights| 11820.77328771164|  0.771694885257507|\n",
      "|    Atwater Village|28481.236967160792| 0.5288318320448259|\n",
      "|   West Los Angeles| 39729.36421669107|   0.61900439238653|\n",
      "|            Pacoima|    11699.46672729| 0.4793224663744079|\n",
      "|          Brookside|18138.626409017714| 0.8856682769726248|\n",
      "|        Rancho Park| 39037.73075076036|  1.008644149191612|\n",
      "|              Palms|30597.797911088597|  0.431748558747618|\n",
      "|          Hyde Park|14144.200637056656| 1.0300275580687879|\n",
      "|    University Park| 6877.686715882044| 0.6973254266005083|\n",
      "|       Porter Ranch| 35201.02200006368| 0.3803050081186921|\n",
      "+-------------------+------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, col\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Kάνουμε join με το Income με βάση το ZIP Code,\n",
    "# οπότε έχουμε το μέσο ετήσιο εισόδημα ανά νοικοκυριό\n",
    "# για κάθε COMM, ZIP Code.\n",
    "# Κάνουμε group by με το COMM για να βρούμε το συνολικό\n",
    "# εισόδημα και τον συνολικό πληθυσμό ανά COMM.\n",
    "result = blocks_data \\\n",
    "            .join(income_data.hint(\"BROADCAST\"), col(\"ZCTA10\") == col(\"Zip Code\")) \\\n",
    "            .groupBy(\"COMM\") \\\n",
    "            .agg(\n",
    "                sum(\"Population\").alias(\"Population\"),\n",
    "                sum(col(\"Estimated Median Income\") * col(\"Households\")).alias(\"Total Income\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "            ) \\\n",
    "            .select(\"COMM\", \"Population\", \"Total Income\", \"geometry\")\n",
    "\n",
    "# Τέλος, κάνουμε join με τα εγκλήματα με βάση το geometry, δηλαδή\n",
    "# το POINT του εγκλήματος βρίσκεται εντός του POLYGON της περιοχής.\n",
    "# Κάνουμε group by με το COMM (και τα Population, Total Income τα οποία είναι\n",
    "# ίδια ανά εγγραφή) και υπολογίζουμε το συνολικό πλήθος των εγκλημάτων.\n",
    "# Μετά διαιρούμε με τον πληθυσμό για να βρούμε το εισόδημα ανά άτομο\n",
    "# και το πλήθος εγκλημάτων ανά άτομο.\n",
    "result = crime_data \\\n",
    "            .join(\n",
    "                result,\n",
    "                ST_Within(col(\"geom\"), col(\"geometry\"))\n",
    "            ) \\\n",
    "            .groupBy(\"COMM\", \"Population\", \"Total Income\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"Total Crimes\")\n",
    "            ) \\\n",
    "            .withColumn(\"Income per Person\", col(\"Total Income\") / col(\"Population\")) \\\n",
    "            .withColumn(\"Crimes per Person\", col(\"Total Crimes\") / col(\"Population\")) \\\n",
    "            .select(\"COMM\", \"Income per Person\", \"Crimes per Person\")\n",
    "\n",
    "result.show()\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for strategy \"BROADCAST\": 33.36 seconds"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for strategy \\\"BROADCAST\\\": {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (31)\n",
      "+- HashAggregate (30)\n",
      "   +- Exchange (29)\n",
      "      +- HashAggregate (28)\n",
      "         +- Project (27)\n",
      "            +- RangeJoin (26)\n",
      "               :- Union (7)\n",
      "               :  :- Project (3)\n",
      "               :  :  +- Filter (2)\n",
      "               :  :     +- Scan csv  (1)\n",
      "               :  +- Project (6)\n",
      "               :     +- Filter (5)\n",
      "               :        +- Scan csv  (4)\n",
      "               +- Filter (25)\n",
      "                  +- ObjectHashAggregate (24)\n",
      "                     +- Exchange (23)\n",
      "                        +- ObjectHashAggregate (22)\n",
      "                           +- Project (21)\n",
      "                              +- BroadcastHashJoin Inner BuildRight (20)\n",
      "                                 :- ObjectHashAggregate (15)\n",
      "                                 :  +- Exchange (14)\n",
      "                                 :     +- ObjectHashAggregate (13)\n",
      "                                 :        +- Project (12)\n",
      "                                 :           +- Filter (11)\n",
      "                                 :              +- Generate (10)\n",
      "                                 :                 +- Filter (9)\n",
      "                                 :                    +- Scan geojson  (8)\n",
      "                                 +- BroadcastExchange (19)\n",
      "                                    +- Project (18)\n",
      "                                       +- Filter (17)\n",
      "                                          +- Scan csv  (16)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [2]: [LAT#74, LON#75]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(3) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#188]\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [2]: [LAT#130, LON#131]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(5) Filter\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(6) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#670]\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "\n",
      "(7) Union\n",
      "\n",
      "(8) Scan geojson \n",
      "Output [1]: [features#239]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(9) Filter\n",
      "Input [1]: [features#239]\n",
      "Condition : ((size(features#239, true) > 0) AND isnotnull(features#239))\n",
      "\n",
      "(10) Generate\n",
      "Input [1]: [features#239]\n",
      "Arguments: explode(features#239), false, [features#247]\n",
      "\n",
      "(11) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : ((((isnotnull(features#247.properties.CITY) AND isnotnull(features#247.properties.HOUSING10)) AND isnotnull(features#247.properties.POP_2010)) AND (((isnotnull(features#247.properties.COMM) AND (features#247.properties.CITY = Los Angeles)) AND (features#247.properties.HOUSING10 > 0)) AND (features#247.properties.POP_2010 > 0))) AND isnotnull(features#247.properties.ZCTA10))\n",
      "\n",
      "(12) Project\n",
      "Output [5]: [features#247.properties.COMM AS COMM#263, features#247.properties.HOUSING10 AS HOUSING10#269L, features#247.properties.POP_2010 AS POP_2010#272L, features#247.properties.ZCTA10 AS ZCTA10#280, features#247.geometry AS geometry#250]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(13) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, HOUSING10#269L, POP_2010#272L, ZCTA10#280, geometry#250]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [partial_sum(POP_2010#272L), partial_sum(HOUSING10#269L), partial_st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@4413df5b, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#456L, sum#458L, buf#460]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum#457L, sum#459L, buf#461]\n",
      "\n",
      "(14) Exchange\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#457L, sum#459L, buf#461]\n",
      "Arguments: hashpartitioning(COMM#263, ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=856]\n",
      "\n",
      "(15) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#457L, sum#459L, buf#461]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [sum(POP_2010#272L), sum(HOUSING10#269L), st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@4413df5b, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(POP_2010#272L)#410L, sum(HOUSING10#269L)#412L, ST_Union_Aggr(geometry#250)#417]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum(POP_2010#272L)#410L AS Population#411L, sum(HOUSING10#269L)#412L AS Households#413L, ST_Union_Aggr(geometry#250)#417 AS geometry#418]\n",
      "\n",
      "(16) Scan csv \n",
      "Output [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "(17) Filter\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Condition : isnotnull(Zip Code#24)\n",
      "\n",
      "(18) Project\n",
      "Output [2]: [Zip Code#24, cast(regexp_replace(Estimated Median Income#26, [$,], , 1) as float) AS Estimated Median Income#30]\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "\n",
      "(19) BroadcastExchange\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#30]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=859]\n",
      "\n",
      "(20) BroadcastHashJoin\n",
      "Left keys [1]: [ZCTA10#280]\n",
      "Right keys [1]: [Zip Code#24]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(21) Project\n",
      "Output [5]: [COMM#263, Population#411L, Households#413L, geometry#418, Estimated Median Income#30]\n",
      "Input [7]: [COMM#263, ZCTA10#280, Population#411L, Households#413L, geometry#418, Zip Code#24, Estimated Median Income#30]\n",
      "\n",
      "(22) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, Population#411L, Households#413L, geometry#418, Estimated Median Income#30]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [partial_sum(Population#411L), partial_sum((Estimated Median Income#30 * cast(Households#413L as float))), partial_st_union_aggr(geometry#418, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@2b6fc523, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#619L, sum#621, buf#623]\n",
      "Results [4]: [COMM#263, sum#620L, sum#622, buf#624]\n",
      "\n",
      "(23) Exchange\n",
      "Input [4]: [COMM#263, sum#620L, sum#622, buf#624]\n",
      "Arguments: hashpartitioning(COMM#263, 1000), ENSURE_REQUIREMENTS, [plan_id=864]\n",
      "\n",
      "(24) ObjectHashAggregate\n",
      "Input [4]: [COMM#263, sum#620L, sum#622, buf#624]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [sum(Population#411L), sum((Estimated Median Income#30 * cast(Households#413L as float))), st_union_aggr(geometry#418, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@2b6fc523, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(Population#411L)#532L, sum((Estimated Median Income#30 * cast(Households#413L as float)))#534, ST_Union_Aggr(geometry#418)#539]\n",
      "Results [4]: [COMM#263, sum(Population#411L)#532L AS Population#533L, sum((Estimated Median Income#30 * cast(Households#413L as float)))#534 AS Total Income#535, ST_Union_Aggr(geometry#418)#539 AS geometry#540]\n",
      "\n",
      "(25) Filter\n",
      "Input [4]: [COMM#263, Population#533L, Total Income#535, geometry#540]\n",
      "Condition : isnotnull(geometry#540)\n",
      "\n",
      "(26) RangeJoin\n",
      "Arguments: geom#188: geometry, geometry#540: geometry, WITHIN\n",
      "\n",
      "(27) Project\n",
      "Output [3]: [COMM#263, Population#533L, Total Income#535]\n",
      "Input [5]: [geom#188, COMM#263, Population#533L, Total Income#535, geometry#540]\n",
      "\n",
      "(28) HashAggregate\n",
      "Input [3]: [COMM#263, Population#533L, Total Income#535]\n",
      "Keys [3]: [COMM#263, Population#533L, knownfloatingpointnormalized(normalizenanandzero(Total Income#535)) AS Total Income#535]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#617L]\n",
      "Results [4]: [COMM#263, Population#533L, Total Income#535, count#618L]\n",
      "\n",
      "(29) Exchange\n",
      "Input [4]: [COMM#263, Population#533L, Total Income#535, count#618L]\n",
      "Arguments: hashpartitioning(COMM#263, Population#533L, Total Income#535, 1000), ENSURE_REQUIREMENTS, [plan_id=871]\n",
      "\n",
      "(30) HashAggregate\n",
      "Input [4]: [COMM#263, Population#533L, Total Income#535, count#618L]\n",
      "Keys [3]: [COMM#263, Population#533L, Total Income#535]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#585L]\n",
      "Results [3]: [COMM#263, (Total Income#535 / cast(Population#533L as double)) AS Income per Person#591, (cast(count(1)#585L as double) / cast(Population#533L as double)) AS Crimes per Person#597]\n",
      "\n",
      "(31) AdaptiveSparkPlan\n",
      "Output [3]: [COMM#263, Income per Person#591, Crimes per Person#597]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "result.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-------------------+\n",
      "|               COMM| Income per Person|  Crimes per Person|\n",
      "+-------------------+------------------+-------------------+\n",
      "|       Hancock Park|21538.792826380406| 0.7797133123352832|\n",
      "|        Playa Vista| 50264.47143177235| 0.5004481290611696|\n",
      "|    Hollywood Hills| 43023.70025098602|  0.747615632843313|\n",
      "|       Toluca Woods|24517.584657534248| 0.6301369863013698|\n",
      "|         West Adams|10768.299623210249| 0.7500753579502637|\n",
      "|       Century City|45617.760134566866|  0.632968881412952|\n",
      "|     Harbor Gateway|16152.767540362767|0.46035977675901935|\n",
      "|          Thai Town|26851.472085561498| 0.5305882352941177|\n",
      "|Palisades Highlands| 66867.44038612054| 0.1878424210800939|\n",
      "|            Sunland| 26184.01260332687| 0.4484131033778957|\n",
      "|    Harvard Heights| 11820.77328771164|  0.771694885257507|\n",
      "|    Atwater Village|28481.236967160792| 0.5288318320448259|\n",
      "|   West Los Angeles| 39729.36421669107|   0.61900439238653|\n",
      "|            Pacoima|    11699.46672729| 0.4793224663744079|\n",
      "|          Brookside|18138.626409017714| 0.8856682769726248|\n",
      "|        Rancho Park| 39037.73075076036|  1.008644149191612|\n",
      "|              Palms|30597.797911088597|  0.431748558747618|\n",
      "|          Hyde Park|14144.200637056656| 1.0300275580687879|\n",
      "|    University Park| 6877.686715882044| 0.6973254266005083|\n",
      "|       Porter Ranch| 35201.02200006368| 0.3803050081186921|\n",
      "+-------------------+------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, col\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Kάνουμε join με το Income με βάση το ZIP Code,\n",
    "# οπότε έχουμε το μέσο ετήσιο εισόδημα ανά νοικοκυριό\n",
    "# για κάθε COMM, ZIP Code.\n",
    "# Κάνουμε group by με το COMM για να βρούμε το συνολικό\n",
    "# εισόδημα και τον συνολικό πληθυσμό ανά COMM.\n",
    "result = blocks_data.hint(\"MERGE\") \\\n",
    "            .join(income_data.hint(\"MERGE\"), col(\"ZCTA10\") == col(\"Zip Code\")) \\\n",
    "            .groupBy(\"COMM\") \\\n",
    "            .agg(\n",
    "                sum(\"Population\").alias(\"Population\"),\n",
    "                sum(col(\"Estimated Median Income\") * col(\"Households\")).alias(\"Total Income\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "            ) \\\n",
    "            .select(\"COMM\", \"Population\", \"Total Income\", \"geometry\")\n",
    "\n",
    "# Τέλος, κάνουμε join με τα εγκλήματα με βάση το geometry, δηλαδή\n",
    "# το POINT του εγκλήματος βρίσκεται εντός του POLYGON της περιοχής.\n",
    "# Κάνουμε group by με το COMM (και τα Population, Total Income τα οποία είναι\n",
    "# ίδια ανά εγγραφή) και υπολογίζουμε το συνολικό πλήθος των εγκλημάτων.\n",
    "# Μετά διαιρούμε με τον πληθυσμό για να βρούμε το εισόδημα ανά άτομο\n",
    "# και το πλήθος εγκλημάτων ανά άτομο.\n",
    "result = result \\\n",
    "            .join(\n",
    "                crime_data,\n",
    "                ST_Within(col(\"geom\"), col(\"geometry\"))\n",
    "            ) \\\n",
    "            .groupBy(\"COMM\", \"Population\", \"Total Income\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"Total Crimes\")\n",
    "            ) \\\n",
    "            .withColumn(\"Income per Person\", col(\"Total Income\") / col(\"Population\")) \\\n",
    "            .withColumn(\"Crimes per Person\", col(\"Total Crimes\") / col(\"Population\")) \\\n",
    "            .select(\"COMM\", \"Income per Person\", \"Crimes per Person\")\n",
    "\n",
    "result.show()\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for strategy \"MERGE\": 32.95 seconds"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for strategy \\\"MERGE\\\": {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (34)\n",
      "+- HashAggregate (33)\n",
      "   +- Exchange (32)\n",
      "      +- HashAggregate (31)\n",
      "         +- Project (30)\n",
      "            +- RangeJoin (29)\n",
      "               :- Filter (21)\n",
      "               :  +- ObjectHashAggregate (20)\n",
      "               :     +- Exchange (19)\n",
      "               :        +- ObjectHashAggregate (18)\n",
      "               :           +- Project (17)\n",
      "               :              +- SortMergeJoin Inner (16)\n",
      "               :                 :- Sort (10)\n",
      "               :                 :  +- Exchange (9)\n",
      "               :                 :     +- ObjectHashAggregate (8)\n",
      "               :                 :        +- Exchange (7)\n",
      "               :                 :           +- ObjectHashAggregate (6)\n",
      "               :                 :              +- Project (5)\n",
      "               :                 :                 +- Filter (4)\n",
      "               :                 :                    +- Generate (3)\n",
      "               :                 :                       +- Filter (2)\n",
      "               :                 :                          +- Scan geojson  (1)\n",
      "               :                 +- Sort (15)\n",
      "               :                    +- Exchange (14)\n",
      "               :                       +- Project (13)\n",
      "               :                          +- Filter (12)\n",
      "               :                             +- Scan csv  (11)\n",
      "               +- Union (28)\n",
      "                  :- Project (24)\n",
      "                  :  +- Filter (23)\n",
      "                  :     +- Scan csv  (22)\n",
      "                  +- Project (27)\n",
      "                     +- Filter (26)\n",
      "                        +- Scan csv  (25)\n",
      "\n",
      "\n",
      "(1) Scan geojson \n",
      "Output [1]: [features#239]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [features#239]\n",
      "Condition : ((size(features#239, true) > 0) AND isnotnull(features#239))\n",
      "\n",
      "(3) Generate\n",
      "Input [1]: [features#239]\n",
      "Arguments: explode(features#239), false, [features#247]\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : ((((isnotnull(features#247.properties.CITY) AND isnotnull(features#247.properties.HOUSING10)) AND isnotnull(features#247.properties.POP_2010)) AND (((isnotnull(features#247.properties.COMM) AND (features#247.properties.CITY = Los Angeles)) AND (features#247.properties.HOUSING10 > 0)) AND (features#247.properties.POP_2010 > 0))) AND isnotnull(features#247.properties.ZCTA10))\n",
      "\n",
      "(5) Project\n",
      "Output [5]: [features#247.properties.COMM AS COMM#263, features#247.properties.HOUSING10 AS HOUSING10#269L, features#247.properties.POP_2010 AS POP_2010#272L, features#247.properties.ZCTA10 AS ZCTA10#280, features#247.geometry AS geometry#250]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(6) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, HOUSING10#269L, POP_2010#272L, ZCTA10#280, geometry#250]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [partial_sum(POP_2010#272L), partial_sum(HOUSING10#269L), partial_st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@374d2a53, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#456L, sum#458L, buf#460]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum#457L, sum#459L, buf#461]\n",
      "\n",
      "(7) Exchange\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#457L, sum#459L, buf#461]\n",
      "Arguments: hashpartitioning(COMM#263, ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=1031]\n",
      "\n",
      "(8) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#457L, sum#459L, buf#461]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [sum(POP_2010#272L), sum(HOUSING10#269L), st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@374d2a53, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(POP_2010#272L)#410L, sum(HOUSING10#269L)#412L, ST_Union_Aggr(geometry#250)#417]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum(POP_2010#272L)#410L AS Population#411L, sum(HOUSING10#269L)#412L AS Households#413L, ST_Union_Aggr(geometry#250)#417 AS geometry#418]\n",
      "\n",
      "(9) Exchange\n",
      "Input [5]: [COMM#263, ZCTA10#280, Population#411L, Households#413L, geometry#418]\n",
      "Arguments: hashpartitioning(ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=1035]\n",
      "\n",
      "(10) Sort\n",
      "Input [5]: [COMM#263, ZCTA10#280, Population#411L, Households#413L, geometry#418]\n",
      "Arguments: [ZCTA10#280 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(11) Scan csv \n",
      "Output [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "(12) Filter\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Condition : isnotnull(Zip Code#24)\n",
      "\n",
      "(13) Project\n",
      "Output [2]: [Zip Code#24, cast(regexp_replace(Estimated Median Income#26, [$,], , 1) as float) AS Estimated Median Income#30]\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "\n",
      "(14) Exchange\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#30]\n",
      "Arguments: hashpartitioning(Zip Code#24, 1000), ENSURE_REQUIREMENTS, [plan_id=1036]\n",
      "\n",
      "(15) Sort\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#30]\n",
      "Arguments: [Zip Code#24 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(16) SortMergeJoin\n",
      "Left keys [1]: [ZCTA10#280]\n",
      "Right keys [1]: [Zip Code#24]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(17) Project\n",
      "Output [5]: [COMM#263, Population#411L, Households#413L, geometry#418, Estimated Median Income#30]\n",
      "Input [7]: [COMM#263, ZCTA10#280, Population#411L, Households#413L, geometry#418, Zip Code#24, Estimated Median Income#30]\n",
      "\n",
      "(18) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, Population#411L, Households#413L, geometry#418, Estimated Median Income#30]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [partial_sum(Population#411L), partial_sum((Estimated Median Income#30 * cast(Households#413L as float))), partial_st_union_aggr(geometry#418, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@ef15ce4, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#619L, sum#621, buf#623]\n",
      "Results [4]: [COMM#263, sum#620L, sum#622, buf#624]\n",
      "\n",
      "(19) Exchange\n",
      "Input [4]: [COMM#263, sum#620L, sum#622, buf#624]\n",
      "Arguments: hashpartitioning(COMM#263, 1000), ENSURE_REQUIREMENTS, [plan_id=1043]\n",
      "\n",
      "(20) ObjectHashAggregate\n",
      "Input [4]: [COMM#263, sum#620L, sum#622, buf#624]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [sum(Population#411L), sum((Estimated Median Income#30 * cast(Households#413L as float))), st_union_aggr(geometry#418, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@ef15ce4, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(Population#411L)#532L, sum((Estimated Median Income#30 * cast(Households#413L as float)))#534, ST_Union_Aggr(geometry#418)#539]\n",
      "Results [4]: [COMM#263, sum(Population#411L)#532L AS Population#533L, sum((Estimated Median Income#30 * cast(Households#413L as float)))#534 AS Total Income#535, ST_Union_Aggr(geometry#418)#539 AS geometry#540]\n",
      "\n",
      "(21) Filter\n",
      "Input [4]: [COMM#263, Population#533L, Total Income#535, geometry#540]\n",
      "Condition : isnotnull(geometry#540)\n",
      "\n",
      "(22) Scan csv \n",
      "Output [2]: [LAT#74, LON#75]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(23) Filter\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(24) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#188]\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "\n",
      "(25) Scan csv \n",
      "Output [2]: [LAT#130, LON#131]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(26) Filter\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(27) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#673]\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "\n",
      "(28) Union\n",
      "\n",
      "(29) RangeJoin\n",
      "Arguments: geometry#540: geometry, geom#188: geometry, CONTAINS\n",
      "\n",
      "(30) Project\n",
      "Output [3]: [COMM#263, Population#533L, Total Income#535]\n",
      "Input [5]: [COMM#263, Population#533L, Total Income#535, geometry#540, geom#188]\n",
      "\n",
      "(31) HashAggregate\n",
      "Input [3]: [COMM#263, Population#533L, Total Income#535]\n",
      "Keys [3]: [COMM#263, Population#533L, knownfloatingpointnormalized(normalizenanandzero(Total Income#535)) AS Total Income#535]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#617L]\n",
      "Results [4]: [COMM#263, Population#533L, Total Income#535, count#618L]\n",
      "\n",
      "(32) Exchange\n",
      "Input [4]: [COMM#263, Population#533L, Total Income#535, count#618L]\n",
      "Arguments: hashpartitioning(COMM#263, Population#533L, Total Income#535, 1000), ENSURE_REQUIREMENTS, [plan_id=1050]\n",
      "\n",
      "(33) HashAggregate\n",
      "Input [4]: [COMM#263, Population#533L, Total Income#535, count#618L]\n",
      "Keys [3]: [COMM#263, Population#533L, Total Income#535]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#585L]\n",
      "Results [3]: [COMM#263, (Total Income#535 / cast(Population#533L as double)) AS Income per Person#591, (cast(count(1)#585L as double) / cast(Population#533L as double)) AS Crimes per Person#597]\n",
      "\n",
      "(34) AdaptiveSparkPlan\n",
      "Output [3]: [COMM#263, Income per Person#591, Crimes per Person#597]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "result.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SHUFFLE HASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-------------------+\n",
      "|               COMM| Income per Person|  Crimes per Person|\n",
      "+-------------------+------------------+-------------------+\n",
      "|       Hancock Park|21538.792826380406| 0.7797133123352832|\n",
      "|        Playa Vista| 50264.47143177235| 0.5004481290611696|\n",
      "|    Hollywood Hills| 43023.70025098602|  0.747615632843313|\n",
      "|       Toluca Woods|24517.584657534248| 0.6301369863013698|\n",
      "|         West Adams|10768.299623210249| 0.7500753579502637|\n",
      "|       Century City|45617.760134566866|  0.632968881412952|\n",
      "|     Harbor Gateway|16152.767540362767|0.46035977675901935|\n",
      "|          Thai Town|26851.472085561498| 0.5305882352941177|\n",
      "|Palisades Highlands| 66867.44038612054| 0.1878424210800939|\n",
      "|            Sunland| 26184.01260332687| 0.4484131033778957|\n",
      "|    Harvard Heights| 11820.77328771164|  0.771694885257507|\n",
      "|    Atwater Village|28481.236967160792| 0.5288318320448259|\n",
      "|   West Los Angeles| 39729.36421669107|   0.61900439238653|\n",
      "|            Pacoima|    11699.46672729| 0.4793224663744079|\n",
      "|          Brookside|18138.626409017714| 0.8856682769726248|\n",
      "|        Rancho Park| 39037.73075076036|  1.008644149191612|\n",
      "|              Palms|30597.797911088597|  0.431748558747618|\n",
      "|          Hyde Park|14144.200637056656| 1.0300275580687879|\n",
      "|    University Park| 6877.686715882044| 0.6973254266005083|\n",
      "|       Porter Ranch| 35201.02200006368| 0.3803050081186921|\n",
      "+-------------------+------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, col\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Kάνουμε join με το Income με βάση το ZIP Code,\n",
    "# οπότε έχουμε το μέσο ετήσιο εισόδημα ανά νοικοκυριό\n",
    "# για κάθε COMM, ZIP Code.\n",
    "# Κάνουμε group by με το COMM για να βρούμε το συνολικό\n",
    "# εισόδημα και τον συνολικό πληθυσμό ανά COMM.\n",
    "result = blocks_data \\\n",
    "            .join(income_data.hint(\"SHUFFLE_HASH\"), col(\"ZCTA10\") == col(\"Zip Code\")) \\\n",
    "            .groupBy(\"COMM\") \\\n",
    "            .agg(\n",
    "                sum(\"Population\").alias(\"Population\"),\n",
    "                sum(col(\"Estimated Median Income\") * col(\"Households\")).alias(\"Total Income\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "            ) \\\n",
    "            .select(\"COMM\", \"Population\", \"Total Income\", \"geometry\")\n",
    "\n",
    "# Τέλος, κάνουμε join με τα εγκλήματα με βάση το geometry, δηλαδή\n",
    "# το POINT του εγκλήματος βρίσκεται εντός του POLYGON της περιοχής.\n",
    "# Κάνουμε group by με το COMM (και τα Population, Total Income τα οποία είναι\n",
    "# ίδια ανά εγγραφή) και υπολογίζουμε το συνολικό πλήθος των εγκλημάτων.\n",
    "# Μετά διαιρούμε με τον πληθυσμό για να βρούμε το εισόδημα ανά άτομο\n",
    "# και το πλήθος εγκλημάτων ανά άτομο.\n",
    "result = crime_data \\\n",
    "            .join(\n",
    "                result,\n",
    "                ST_Within(col(\"geom\"), col(\"geometry\"))\n",
    "            ) \\\n",
    "            .groupBy(\"COMM\", \"Population\", \"Total Income\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"Total Crimes\")\n",
    "            ) \\\n",
    "            .withColumn(\"Income per Person\", col(\"Total Income\") / col(\"Population\")) \\\n",
    "            .withColumn(\"Crimes per Person\", col(\"Total Crimes\") / col(\"Population\")) \\\n",
    "            .select(\"COMM\", \"Income per Person\", \"Crimes per Person\")\n",
    "\n",
    "result.show()\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for strategy \"SHUFFLE_HASH\": 28.10 seconds"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for strategy \\\"SHUFFLE_HASH\\\": {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (32)\n",
      "+- HashAggregate (31)\n",
      "   +- Exchange (30)\n",
      "      +- HashAggregate (29)\n",
      "         +- Project (28)\n",
      "            +- RangeJoin (27)\n",
      "               :- Union (7)\n",
      "               :  :- Project (3)\n",
      "               :  :  +- Filter (2)\n",
      "               :  :     +- Scan csv  (1)\n",
      "               :  +- Project (6)\n",
      "               :     +- Filter (5)\n",
      "               :        +- Scan csv  (4)\n",
      "               +- Filter (26)\n",
      "                  +- ObjectHashAggregate (25)\n",
      "                     +- Exchange (24)\n",
      "                        +- ObjectHashAggregate (23)\n",
      "                           +- Project (22)\n",
      "                              +- ShuffledHashJoin Inner BuildRight (21)\n",
      "                                 :- Exchange (16)\n",
      "                                 :  +- ObjectHashAggregate (15)\n",
      "                                 :     +- Exchange (14)\n",
      "                                 :        +- ObjectHashAggregate (13)\n",
      "                                 :           +- Project (12)\n",
      "                                 :              +- Filter (11)\n",
      "                                 :                 +- Generate (10)\n",
      "                                 :                    +- Filter (9)\n",
      "                                 :                       +- Scan geojson  (8)\n",
      "                                 +- Exchange (20)\n",
      "                                    +- Project (19)\n",
      "                                       +- Filter (18)\n",
      "                                          +- Scan csv  (17)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [2]: [LAT#74, LON#75]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(3) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#188]\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [2]: [LAT#130, LON#131]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(5) Filter\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(6) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#673]\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "\n",
      "(7) Union\n",
      "\n",
      "(8) Scan geojson \n",
      "Output [1]: [features#239]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(9) Filter\n",
      "Input [1]: [features#239]\n",
      "Condition : ((size(features#239, true) > 0) AND isnotnull(features#239))\n",
      "\n",
      "(10) Generate\n",
      "Input [1]: [features#239]\n",
      "Arguments: explode(features#239), false, [features#247]\n",
      "\n",
      "(11) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : ((((isnotnull(features#247.properties.CITY) AND isnotnull(features#247.properties.HOUSING10)) AND isnotnull(features#247.properties.POP_2010)) AND (((isnotnull(features#247.properties.COMM) AND (features#247.properties.CITY = Los Angeles)) AND (features#247.properties.HOUSING10 > 0)) AND (features#247.properties.POP_2010 > 0))) AND isnotnull(features#247.properties.ZCTA10))\n",
      "\n",
      "(12) Project\n",
      "Output [5]: [features#247.properties.COMM AS COMM#263, features#247.properties.HOUSING10 AS HOUSING10#269L, features#247.properties.POP_2010 AS POP_2010#272L, features#247.properties.ZCTA10 AS ZCTA10#280, features#247.geometry AS geometry#250]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(13) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, HOUSING10#269L, POP_2010#272L, ZCTA10#280, geometry#250]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [partial_sum(POP_2010#272L), partial_sum(HOUSING10#269L), partial_st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@7c9d8fdc, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#456L, sum#458L, buf#460]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum#457L, sum#459L, buf#461]\n",
      "\n",
      "(14) Exchange\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#457L, sum#459L, buf#461]\n",
      "Arguments: hashpartitioning(COMM#263, ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=984]\n",
      "\n",
      "(15) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#457L, sum#459L, buf#461]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [sum(POP_2010#272L), sum(HOUSING10#269L), st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@7c9d8fdc, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(POP_2010#272L)#410L, sum(HOUSING10#269L)#412L, ST_Union_Aggr(geometry#250)#417]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum(POP_2010#272L)#410L AS Population#411L, sum(HOUSING10#269L)#412L AS Households#413L, ST_Union_Aggr(geometry#250)#417 AS geometry#418]\n",
      "\n",
      "(16) Exchange\n",
      "Input [5]: [COMM#263, ZCTA10#280, Population#411L, Households#413L, geometry#418]\n",
      "Arguments: hashpartitioning(ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=988]\n",
      "\n",
      "(17) Scan csv \n",
      "Output [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "(18) Filter\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Condition : isnotnull(Zip Code#24)\n",
      "\n",
      "(19) Project\n",
      "Output [2]: [Zip Code#24, cast(regexp_replace(Estimated Median Income#26, [$,], , 1) as float) AS Estimated Median Income#30]\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "\n",
      "(20) Exchange\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#30]\n",
      "Arguments: hashpartitioning(Zip Code#24, 1000), ENSURE_REQUIREMENTS, [plan_id=989]\n",
      "\n",
      "(21) ShuffledHashJoin\n",
      "Left keys [1]: [ZCTA10#280]\n",
      "Right keys [1]: [Zip Code#24]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(22) Project\n",
      "Output [5]: [COMM#263, Population#411L, Households#413L, geometry#418, Estimated Median Income#30]\n",
      "Input [7]: [COMM#263, ZCTA10#280, Population#411L, Households#413L, geometry#418, Zip Code#24, Estimated Median Income#30]\n",
      "\n",
      "(23) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, Population#411L, Households#413L, geometry#418, Estimated Median Income#30]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [partial_sum(Population#411L), partial_sum((Estimated Median Income#30 * cast(Households#413L as float))), partial_st_union_aggr(geometry#418, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@54ab06c6, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#619L, sum#621, buf#623]\n",
      "Results [4]: [COMM#263, sum#620L, sum#622, buf#624]\n",
      "\n",
      "(24) Exchange\n",
      "Input [4]: [COMM#263, sum#620L, sum#622, buf#624]\n",
      "Arguments: hashpartitioning(COMM#263, 1000), ENSURE_REQUIREMENTS, [plan_id=994]\n",
      "\n",
      "(25) ObjectHashAggregate\n",
      "Input [4]: [COMM#263, sum#620L, sum#622, buf#624]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [sum(Population#411L), sum((Estimated Median Income#30 * cast(Households#413L as float))), st_union_aggr(geometry#418, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@54ab06c6, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(Population#411L)#532L, sum((Estimated Median Income#30 * cast(Households#413L as float)))#534, ST_Union_Aggr(geometry#418)#539]\n",
      "Results [4]: [COMM#263, sum(Population#411L)#532L AS Population#533L, sum((Estimated Median Income#30 * cast(Households#413L as float)))#534 AS Total Income#535, ST_Union_Aggr(geometry#418)#539 AS geometry#540]\n",
      "\n",
      "(26) Filter\n",
      "Input [4]: [COMM#263, Population#533L, Total Income#535, geometry#540]\n",
      "Condition : isnotnull(geometry#540)\n",
      "\n",
      "(27) RangeJoin\n",
      "Arguments: geom#188: geometry, geometry#540: geometry, WITHIN\n",
      "\n",
      "(28) Project\n",
      "Output [3]: [COMM#263, Population#533L, Total Income#535]\n",
      "Input [5]: [geom#188, COMM#263, Population#533L, Total Income#535, geometry#540]\n",
      "\n",
      "(29) HashAggregate\n",
      "Input [3]: [COMM#263, Population#533L, Total Income#535]\n",
      "Keys [3]: [COMM#263, Population#533L, knownfloatingpointnormalized(normalizenanandzero(Total Income#535)) AS Total Income#535]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#617L]\n",
      "Results [4]: [COMM#263, Population#533L, Total Income#535, count#618L]\n",
      "\n",
      "(30) Exchange\n",
      "Input [4]: [COMM#263, Population#533L, Total Income#535, count#618L]\n",
      "Arguments: hashpartitioning(COMM#263, Population#533L, Total Income#535, 1000), ENSURE_REQUIREMENTS, [plan_id=1001]\n",
      "\n",
      "(31) HashAggregate\n",
      "Input [4]: [COMM#263, Population#533L, Total Income#535, count#618L]\n",
      "Keys [3]: [COMM#263, Population#533L, Total Income#535]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#585L]\n",
      "Results [3]: [COMM#263, (Total Income#535 / cast(Population#533L as double)) AS Income per Person#591, (cast(count(1)#585L as double) / cast(Population#533L as double)) AS Crimes per Person#597]\n",
      "\n",
      "(32) AdaptiveSparkPlan\n",
      "Output [3]: [COMM#263, Income per Person#591, Crimes per Person#597]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "result.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. SHUFFLE REPLICATE NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-------------------+\n",
      "|               COMM| Income per Person|  Crimes per Person|\n",
      "+-------------------+------------------+-------------------+\n",
      "|       Hancock Park|21538.792826380406| 0.7797133123352832|\n",
      "|        Playa Vista| 50264.47143177235| 0.5004481290611696|\n",
      "|    Hollywood Hills| 43023.70025098602|  0.747615632843313|\n",
      "|       Toluca Woods|24517.584657534248| 0.6301369863013698|\n",
      "|         West Adams|10768.299623210249| 0.7500753579502637|\n",
      "|       Century City|45617.760134566866|  0.632968881412952|\n",
      "|     Harbor Gateway|16152.767540362767|0.46035977675901935|\n",
      "|          Thai Town|26851.472085561498| 0.5305882352941177|\n",
      "|Palisades Highlands| 66867.44038612054| 0.1878424210800939|\n",
      "|            Sunland| 26184.01260332687| 0.4484131033778957|\n",
      "|    Harvard Heights| 11820.77328771164|  0.771694885257507|\n",
      "|    Atwater Village|28481.236967160792| 0.5288318320448259|\n",
      "|   West Los Angeles| 39729.36421669107|   0.61900439238653|\n",
      "|            Pacoima|    11699.46672729| 0.4793224663744079|\n",
      "|          Brookside|18138.626409017714| 0.8856682769726248|\n",
      "|        Rancho Park| 39037.73075076036|  1.008644149191612|\n",
      "|              Palms|30597.797911088597|  0.431748558747618|\n",
      "|          Hyde Park|14144.200637056656| 1.0300275580687879|\n",
      "|    University Park| 6877.686715882044| 0.6973254266005083|\n",
      "|       Porter Ranch| 35201.02200006368| 0.3803050081186921|\n",
      "+-------------------+------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, col\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Kάνουμε join με το Income με βάση το ZIP Code,\n",
    "# οπότε έχουμε το μέσο ετήσιο εισόδημα ανά νοικοκυριό\n",
    "# για κάθε COMM, ZIP Code.\n",
    "# Κάνουμε group by με το COMM για να βρούμε το συνολικό\n",
    "# εισόδημα και τον συνολικό πληθυσμό ανά COMM.\n",
    "result = income_data.hint(\"SHUFFLE_REPLICATE_NL\") \\\n",
    "            .join(blocks_data, col(\"ZCTA10\") == col(\"Zip Code\")) \\\n",
    "            .groupBy(\"COMM\") \\\n",
    "            .agg(\n",
    "                sum(\"Population\").alias(\"Population\"),\n",
    "                sum(col(\"Estimated Median Income\") * col(\"Households\")).alias(\"Total Income\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "            ) \\\n",
    "            .select(\"COMM\", \"Population\", \"Total Income\", \"geometry\")\n",
    "\n",
    "# Τέλος, κάνουμε join με τα εγκλήματα με βάση το geometry, δηλαδή\n",
    "# το POINT του εγκλήματος βρίσκεται εντός του POLYGON της περιοχής.\n",
    "# Κάνουμε group by με το COMM (και τα Population, Total Income τα οποία είναι\n",
    "# ίδια ανά εγγραφή) και υπολογίζουμε το συνολικό πλήθος των εγκλημάτων.\n",
    "# Μετά διαιρούμε με τον πληθυσμό για να βρούμε το εισόδημα ανά άτομο\n",
    "# και το πλήθος εγκλημάτων ανά άτομο.\n",
    "result = crime_data \\\n",
    "            .join(\n",
    "                result,\n",
    "                ST_Within(col(\"geom\"), col(\"geometry\"))\n",
    "            ) \\\n",
    "            .groupBy(\"COMM\", \"Population\", \"Total Income\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"Total Crimes\")\n",
    "            ) \\\n",
    "            .withColumn(\"Income per Person\", col(\"Total Income\") / col(\"Population\")) \\\n",
    "            .withColumn(\"Crimes per Person\", col(\"Total Crimes\") / col(\"Population\")) \\\n",
    "            .select(\"COMM\", \"Income per Person\", \"Crimes per Person\")\n",
    "\n",
    "result.show()\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for strategy \"SHUFFLE_REPLICATE_NL\": 27.93 seconds"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for strategy \\\"SHUFFLE_REPLICATE_NL\\\": {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (30)\n",
      "+- HashAggregate (29)\n",
      "   +- Exchange (28)\n",
      "      +- HashAggregate (27)\n",
      "         +- Project (26)\n",
      "            +- RangeJoin (25)\n",
      "               :- Union (7)\n",
      "               :  :- Project (3)\n",
      "               :  :  +- Filter (2)\n",
      "               :  :     +- Scan csv  (1)\n",
      "               :  +- Project (6)\n",
      "               :     +- Filter (5)\n",
      "               :        +- Scan csv  (4)\n",
      "               +- Filter (24)\n",
      "                  +- ObjectHashAggregate (23)\n",
      "                     +- Exchange (22)\n",
      "                        +- ObjectHashAggregate (21)\n",
      "                           +- Project (20)\n",
      "                              +- CartesianProduct Inner (19)\n",
      "                                 :- Project (10)\n",
      "                                 :  +- Filter (9)\n",
      "                                 :     +- Scan csv  (8)\n",
      "                                 +- ObjectHashAggregate (18)\n",
      "                                    +- Exchange (17)\n",
      "                                       +- ObjectHashAggregate (16)\n",
      "                                          +- Project (15)\n",
      "                                             +- Filter (14)\n",
      "                                                +- Generate (13)\n",
      "                                                   +- Filter (12)\n",
      "                                                      +- Scan geojson  (11)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [2]: [LAT#74, LON#75]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(3) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#188]\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [2]: [LAT#130, LON#131]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(5) Filter\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(6) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#670]\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "\n",
      "(7) Union\n",
      "\n",
      "(8) Scan csv \n",
      "Output [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "(9) Filter\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Condition : isnotnull(Zip Code#24)\n",
      "\n",
      "(10) Project\n",
      "Output [2]: [Zip Code#24, cast(regexp_replace(Estimated Median Income#26, [$,], , 1) as float) AS Estimated Median Income#30]\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "\n",
      "(11) Scan geojson \n",
      "Output [1]: [features#239]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(12) Filter\n",
      "Input [1]: [features#239]\n",
      "Condition : ((size(features#239, true) > 0) AND isnotnull(features#239))\n",
      "\n",
      "(13) Generate\n",
      "Input [1]: [features#239]\n",
      "Arguments: explode(features#239), false, [features#247]\n",
      "\n",
      "(14) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : ((((isnotnull(features#247.properties.CITY) AND isnotnull(features#247.properties.HOUSING10)) AND isnotnull(features#247.properties.POP_2010)) AND (((isnotnull(features#247.properties.COMM) AND (features#247.properties.CITY = Los Angeles)) AND (features#247.properties.HOUSING10 > 0)) AND (features#247.properties.POP_2010 > 0))) AND isnotnull(features#247.properties.ZCTA10))\n",
      "\n",
      "(15) Project\n",
      "Output [5]: [features#247.properties.COMM AS COMM#263, features#247.properties.HOUSING10 AS HOUSING10#269L, features#247.properties.POP_2010 AS POP_2010#272L, features#247.properties.ZCTA10 AS ZCTA10#280, features#247.geometry AS geometry#250]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(16) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, HOUSING10#269L, POP_2010#272L, ZCTA10#280, geometry#250]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [partial_sum(POP_2010#272L), partial_sum(HOUSING10#269L), partial_st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@39121bca, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#456L, sum#458L, buf#460]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum#457L, sum#459L, buf#461]\n",
      "\n",
      "(17) Exchange\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#457L, sum#459L, buf#461]\n",
      "Arguments: hashpartitioning(COMM#263, ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=789]\n",
      "\n",
      "(18) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#457L, sum#459L, buf#461]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [sum(POP_2010#272L), sum(HOUSING10#269L), st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@39121bca, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(POP_2010#272L)#410L, sum(HOUSING10#269L)#412L, ST_Union_Aggr(geometry#250)#417]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum(POP_2010#272L)#410L AS Population#411L, sum(HOUSING10#269L)#412L AS Households#413L, ST_Union_Aggr(geometry#250)#417 AS geometry#418]\n",
      "\n",
      "(19) CartesianProduct\n",
      "Join type: Inner\n",
      "Join condition: (ZCTA10#280 = Zip Code#24)\n",
      "\n",
      "(20) Project\n",
      "Output [5]: [Estimated Median Income#30, COMM#263, Population#411L, Households#413L, geometry#418]\n",
      "Input [7]: [Zip Code#24, Estimated Median Income#30, COMM#263, ZCTA10#280, Population#411L, Households#413L, geometry#418]\n",
      "\n",
      "(21) ObjectHashAggregate\n",
      "Input [5]: [Estimated Median Income#30, COMM#263, Population#411L, Households#413L, geometry#418]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [partial_sum(Population#411L), partial_sum((Estimated Median Income#30 * cast(Households#413L as float))), partial_st_union_aggr(geometry#418, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@4417e0f5, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#619L, sum#621, buf#623]\n",
      "Results [4]: [COMM#263, sum#620L, sum#622, buf#624]\n",
      "\n",
      "(22) Exchange\n",
      "Input [4]: [COMM#263, sum#620L, sum#622, buf#624]\n",
      "Arguments: hashpartitioning(COMM#263, 1000), ENSURE_REQUIREMENTS, [plan_id=795]\n",
      "\n",
      "(23) ObjectHashAggregate\n",
      "Input [4]: [COMM#263, sum#620L, sum#622, buf#624]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [sum(Population#411L), sum((Estimated Median Income#30 * cast(Households#413L as float))), st_union_aggr(geometry#418, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@4417e0f5, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(Population#411L)#532L, sum((Estimated Median Income#30 * cast(Households#413L as float)))#534, ST_Union_Aggr(geometry#418)#539]\n",
      "Results [4]: [COMM#263, sum(Population#411L)#532L AS Population#533L, sum((Estimated Median Income#30 * cast(Households#413L as float)))#534 AS Total Income#535, ST_Union_Aggr(geometry#418)#539 AS geometry#540]\n",
      "\n",
      "(24) Filter\n",
      "Input [4]: [COMM#263, Population#533L, Total Income#535, geometry#540]\n",
      "Condition : isnotnull(geometry#540)\n",
      "\n",
      "(25) RangeJoin\n",
      "Arguments: geom#188: geometry, geometry#540: geometry, WITHIN\n",
      "\n",
      "(26) Project\n",
      "Output [3]: [COMM#263, Population#533L, Total Income#535]\n",
      "Input [5]: [geom#188, COMM#263, Population#533L, Total Income#535, geometry#540]\n",
      "\n",
      "(27) HashAggregate\n",
      "Input [3]: [COMM#263, Population#533L, Total Income#535]\n",
      "Keys [3]: [COMM#263, Population#533L, knownfloatingpointnormalized(normalizenanandzero(Total Income#535)) AS Total Income#535]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#617L]\n",
      "Results [4]: [COMM#263, Population#533L, Total Income#535, count#618L]\n",
      "\n",
      "(28) Exchange\n",
      "Input [4]: [COMM#263, Population#533L, Total Income#535, count#618L]\n",
      "Arguments: hashpartitioning(COMM#263, Population#533L, Total Income#535, 1000), ENSURE_REQUIREMENTS, [plan_id=802]\n",
      "\n",
      "(29) HashAggregate\n",
      "Input [4]: [COMM#263, Population#533L, Total Income#535, count#618L]\n",
      "Keys [3]: [COMM#263, Population#533L, Total Income#535]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#585L]\n",
      "Results [3]: [COMM#263, (Total Income#535 / cast(Population#533L as double)) AS Income per Person#591, (cast(count(1)#585L as double) / cast(Population#533L as double)) AS Crimes per Person#597]\n",
      "\n",
      "(30) AdaptiveSparkPlan\n",
      "Output [3]: [COMM#263, Income per Person#591, Crimes per Person#597]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "result.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. SHUFFLE_HASH + BROADCAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-------------------+\n",
      "|               COMM| Income per Person|  Crimes per Person|\n",
      "+-------------------+------------------+-------------------+\n",
      "|       Hancock Park|21538.792826380406| 0.7797133123352832|\n",
      "|        Playa Vista| 50264.47143177235| 0.5004481290611696|\n",
      "|    Hollywood Hills| 43023.70025098602|  0.747615632843313|\n",
      "|       Toluca Woods|24517.584657534248| 0.6301369863013698|\n",
      "|         West Adams|10768.299623210249| 0.7500753579502637|\n",
      "|       Century City|45617.760134566866|  0.632968881412952|\n",
      "|     Harbor Gateway|16152.767540362767|0.46035977675901935|\n",
      "|          Thai Town|26851.472085561498| 0.5305882352941177|\n",
      "|Palisades Highlands| 66867.44038612054| 0.1878424210800939|\n",
      "|            Sunland| 26184.01260332687| 0.4484131033778957|\n",
      "|    Harvard Heights| 11820.77328771164|  0.771694885257507|\n",
      "|    Atwater Village|28481.236967160792| 0.5288318320448259|\n",
      "|   West Los Angeles| 39729.36421669107|   0.61900439238653|\n",
      "|            Pacoima|    11699.46672729| 0.4793224663744079|\n",
      "|          Brookside|18138.626409017714| 0.8856682769726248|\n",
      "|        Rancho Park| 39037.73075076036|  1.008644149191612|\n",
      "|              Palms|30597.797911088597|  0.431748558747618|\n",
      "|          Hyde Park|14144.200637056656| 1.0300275580687879|\n",
      "|    University Park| 6877.686715882044| 0.6973254266005083|\n",
      "|       Porter Ranch| 35201.02200006368| 0.3803050081186921|\n",
      "+-------------------+------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, col\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Kάνουμε join με το Income με βάση το ZIP Code,\n",
    "# οπότε έχουμε το μέσο ετήσιο εισόδημα ανά νοικοκυριό\n",
    "# για κάθε COMM, ZIP Code.\n",
    "# Κάνουμε group by με το COMM για να βρούμε το συνολικό\n",
    "# εισόδημα και τον συνολικό πληθυσμό ανά COMM.\n",
    "result = blocks_data \\\n",
    "            .join(income_data.hint(\"SHUFFLE_HASH\"), col(\"ZCTA10\") == col(\"Zip Code\")) \\\n",
    "            .groupBy(\"COMM\") \\\n",
    "            .agg(\n",
    "                sum(\"Population\").alias(\"Population\"),\n",
    "                sum(col(\"Estimated Median Income\") * col(\"Households\")).alias(\"Total Income\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "            ) \\\n",
    "            .select(\"COMM\", \"Population\", \"Total Income\", \"geometry\")\n",
    "\n",
    "# Τέλος, κάνουμε join με τα εγκλήματα με βάση το geometry, δηλαδή\n",
    "# το POINT του εγκλήματος βρίσκεται εντός του POLYGON της περιοχής.\n",
    "# Κάνουμε group by με το COMM (και τα Population, Total Income τα οποία είναι\n",
    "# ίδια ανά εγγραφή) και υπολογίζουμε το συνολικό πλήθος των εγκλημάτων.\n",
    "# Μετά διαιρούμε με τον πληθυσμό για να βρούμε το εισόδημα ανά άτομο\n",
    "# και το πλήθος εγκλημάτων ανά άτομο.\n",
    "result = crime_data \\\n",
    "            .join(\n",
    "                result.hint(\"BROADCAST\"),\n",
    "                ST_Within(col(\"geom\"), col(\"geometry\"))\n",
    "            ) \\\n",
    "            .groupBy(\"COMM\", \"Population\", \"Total Income\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"Total Crimes\")\n",
    "            ) \\\n",
    "            .withColumn(\"Income per Person\", col(\"Total Income\") / col(\"Population\")) \\\n",
    "            .withColumn(\"Crimes per Person\", col(\"Total Crimes\") / col(\"Population\")) \\\n",
    "            .select(\"COMM\", \"Income per Person\", \"Crimes per Person\")\n",
    "\n",
    "result.show()\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for strategy \"SHUFFLE_HASH\": 28.80 seconds"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for strategy \\\"SHUFFLE_HASH\\\": {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (33)\n",
      "+- HashAggregate (32)\n",
      "   +- Exchange (31)\n",
      "      +- HashAggregate (30)\n",
      "         +- Project (29)\n",
      "            +- BroadcastIndexJoin (28)\n",
      "               :- Union (7)\n",
      "               :  :- Project (3)\n",
      "               :  :  +- Filter (2)\n",
      "               :  :     +- Scan csv  (1)\n",
      "               :  +- Project (6)\n",
      "               :     +- Filter (5)\n",
      "               :        +- Scan csv  (4)\n",
      "               +- SpatialIndex (27)\n",
      "                  +- Filter (26)\n",
      "                     +- ObjectHashAggregate (25)\n",
      "                        +- Exchange (24)\n",
      "                           +- ObjectHashAggregate (23)\n",
      "                              +- Project (22)\n",
      "                                 +- ShuffledHashJoin Inner BuildRight (21)\n",
      "                                    :- Exchange (16)\n",
      "                                    :  +- ObjectHashAggregate (15)\n",
      "                                    :     +- Exchange (14)\n",
      "                                    :        +- ObjectHashAggregate (13)\n",
      "                                    :           +- Project (12)\n",
      "                                    :              +- Filter (11)\n",
      "                                    :                 +- Generate (10)\n",
      "                                    :                    +- Filter (9)\n",
      "                                    :                       +- Scan geojson  (8)\n",
      "                                    +- Exchange (20)\n",
      "                                       +- Project (19)\n",
      "                                          +- Filter (18)\n",
      "                                             +- Scan csv  (17)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [2]: [LAT#74, LON#75]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(3) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#188]\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [2]: [LAT#130, LON#131]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(5) Filter\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(6) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#673]\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "\n",
      "(7) Union\n",
      "\n",
      "(8) Scan geojson \n",
      "Output [1]: [features#239]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(9) Filter\n",
      "Input [1]: [features#239]\n",
      "Condition : ((size(features#239, true) > 0) AND isnotnull(features#239))\n",
      "\n",
      "(10) Generate\n",
      "Input [1]: [features#239]\n",
      "Arguments: explode(features#239), false, [features#247]\n",
      "\n",
      "(11) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : ((((isnotnull(features#247.properties.CITY) AND isnotnull(features#247.properties.HOUSING10)) AND isnotnull(features#247.properties.POP_2010)) AND (((isnotnull(features#247.properties.COMM) AND (features#247.properties.CITY = Los Angeles)) AND (features#247.properties.HOUSING10 > 0)) AND (features#247.properties.POP_2010 > 0))) AND isnotnull(features#247.properties.ZCTA10))\n",
      "\n",
      "(12) Project\n",
      "Output [5]: [features#247.properties.COMM AS COMM#263, features#247.properties.HOUSING10 AS HOUSING10#269L, features#247.properties.POP_2010 AS POP_2010#272L, features#247.properties.ZCTA10 AS ZCTA10#280, features#247.geometry AS geometry#250]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(13) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, HOUSING10#269L, POP_2010#272L, ZCTA10#280, geometry#250]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [partial_sum(POP_2010#272L), partial_sum(HOUSING10#269L), partial_st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@56ceb4d, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#456L, sum#458L, buf#460]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum#457L, sum#459L, buf#461]\n",
      "\n",
      "(14) Exchange\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#457L, sum#459L, buf#461]\n",
      "Arguments: hashpartitioning(COMM#263, ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=1005]\n",
      "\n",
      "(15) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#457L, sum#459L, buf#461]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [sum(POP_2010#272L), sum(HOUSING10#269L), st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@56ceb4d, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(POP_2010#272L)#410L, sum(HOUSING10#269L)#412L, ST_Union_Aggr(geometry#250)#417]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum(POP_2010#272L)#410L AS Population#411L, sum(HOUSING10#269L)#412L AS Households#413L, ST_Union_Aggr(geometry#250)#417 AS geometry#418]\n",
      "\n",
      "(16) Exchange\n",
      "Input [5]: [COMM#263, ZCTA10#280, Population#411L, Households#413L, geometry#418]\n",
      "Arguments: hashpartitioning(ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=1009]\n",
      "\n",
      "(17) Scan csv \n",
      "Output [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "(18) Filter\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Condition : isnotnull(Zip Code#24)\n",
      "\n",
      "(19) Project\n",
      "Output [2]: [Zip Code#24, cast(regexp_replace(Estimated Median Income#26, [$,], , 1) as float) AS Estimated Median Income#30]\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "\n",
      "(20) Exchange\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#30]\n",
      "Arguments: hashpartitioning(Zip Code#24, 1000), ENSURE_REQUIREMENTS, [plan_id=1010]\n",
      "\n",
      "(21) ShuffledHashJoin\n",
      "Left keys [1]: [ZCTA10#280]\n",
      "Right keys [1]: [Zip Code#24]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(22) Project\n",
      "Output [5]: [COMM#263, Population#411L, Households#413L, geometry#418, Estimated Median Income#30]\n",
      "Input [7]: [COMM#263, ZCTA10#280, Population#411L, Households#413L, geometry#418, Zip Code#24, Estimated Median Income#30]\n",
      "\n",
      "(23) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, Population#411L, Households#413L, geometry#418, Estimated Median Income#30]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [partial_sum(Population#411L), partial_sum((Estimated Median Income#30 * cast(Households#413L as float))), partial_st_union_aggr(geometry#418, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@d40438e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#619L, sum#621, buf#623]\n",
      "Results [4]: [COMM#263, sum#620L, sum#622, buf#624]\n",
      "\n",
      "(24) Exchange\n",
      "Input [4]: [COMM#263, sum#620L, sum#622, buf#624]\n",
      "Arguments: hashpartitioning(COMM#263, 1000), ENSURE_REQUIREMENTS, [plan_id=1015]\n",
      "\n",
      "(25) ObjectHashAggregate\n",
      "Input [4]: [COMM#263, sum#620L, sum#622, buf#624]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [sum(Population#411L), sum((Estimated Median Income#30 * cast(Households#413L as float))), st_union_aggr(geometry#418, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@d40438e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(Population#411L)#532L, sum((Estimated Median Income#30 * cast(Households#413L as float)))#534, ST_Union_Aggr(geometry#418)#539]\n",
      "Results [4]: [COMM#263, sum(Population#411L)#532L AS Population#533L, sum((Estimated Median Income#30 * cast(Households#413L as float)))#534 AS Total Income#535, ST_Union_Aggr(geometry#418)#539 AS geometry#540]\n",
      "\n",
      "(26) Filter\n",
      "Input [4]: [COMM#263, Population#533L, Total Income#535, geometry#540]\n",
      "Condition : isnotnull(geometry#540)\n",
      "\n",
      "(27) SpatialIndex\n",
      "Arguments: geometry#540: geometry, RTREE, false, false\n",
      "\n",
      "(28) BroadcastIndexJoin\n",
      "Arguments: geom#188: geometry, RightSide, LeftSide, Inner, WITHIN\n",
      "\n",
      "(29) Project\n",
      "Output [3]: [COMM#263, Population#533L, Total Income#535]\n",
      "Input [5]: [geom#188, COMM#263, Population#533L, Total Income#535, geometry#540]\n",
      "\n",
      "(30) HashAggregate\n",
      "Input [3]: [COMM#263, Population#533L, Total Income#535]\n",
      "Keys [3]: [COMM#263, Population#533L, knownfloatingpointnormalized(normalizenanandzero(Total Income#535)) AS Total Income#535]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#617L]\n",
      "Results [4]: [COMM#263, Population#533L, Total Income#535, count#618L]\n",
      "\n",
      "(31) Exchange\n",
      "Input [4]: [COMM#263, Population#533L, Total Income#535, count#618L]\n",
      "Arguments: hashpartitioning(COMM#263, Population#533L, Total Income#535, 1000), ENSURE_REQUIREMENTS, [plan_id=1023]\n",
      "\n",
      "(32) HashAggregate\n",
      "Input [4]: [COMM#263, Population#533L, Total Income#535, count#618L]\n",
      "Keys [3]: [COMM#263, Population#533L, Total Income#535]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#585L]\n",
      "Results [3]: [COMM#263, (Total Income#535 / cast(Population#533L as double)) AS Income per Person#591, (cast(count(1)#585L as double) / cast(Population#533L as double)) AS Crimes per Person#597]\n",
      "\n",
      "(33) AdaptiveSparkPlan\n",
      "Output [3]: [COMM#263, Income per Person#591, Crimes per Person#597]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "result.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. SHUFFLE_REPLICATE_NL + BROADCAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-------------------+\n",
      "|               COMM| Income per Person|  Crimes per Person|\n",
      "+-------------------+------------------+-------------------+\n",
      "|       Hancock Park|21538.792826380406| 0.7797133123352832|\n",
      "|        Playa Vista| 50264.47143177235| 0.5004481290611696|\n",
      "|    Hollywood Hills| 43023.70025098602|  0.747615632843313|\n",
      "|       Toluca Woods|24517.584657534248| 0.6301369863013698|\n",
      "|         West Adams|10768.299623210249| 0.7500753579502637|\n",
      "|       Century City|45617.760134566866|  0.632968881412952|\n",
      "|     Harbor Gateway|16152.767540362767|0.46035977675901935|\n",
      "|          Thai Town|26851.472085561498| 0.5305882352941177|\n",
      "|Palisades Highlands| 66867.44038612054| 0.1878424210800939|\n",
      "|            Sunland| 26184.01260332687| 0.4484131033778957|\n",
      "|    Harvard Heights| 11820.77328771164|  0.771694885257507|\n",
      "|    Atwater Village|28481.236967160792| 0.5288318320448259|\n",
      "|   West Los Angeles| 39729.36421669107|   0.61900439238653|\n",
      "|            Pacoima|    11699.46672729| 0.4793224663744079|\n",
      "|          Brookside|18138.626409017714| 0.8856682769726248|\n",
      "|        Rancho Park| 39037.73075076036|  1.008644149191612|\n",
      "|              Palms|30597.797911088597|  0.431748558747618|\n",
      "|          Hyde Park|14144.200637056656| 1.0300275580687879|\n",
      "|    University Park| 6877.686715882044| 0.6973254266005083|\n",
      "|       Porter Ranch| 35201.02200006368| 0.3803050081186921|\n",
      "+-------------------+------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, col\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Kάνουμε join με το Income με βάση το ZIP Code,\n",
    "# οπότε έχουμε το μέσο ετήσιο εισόδημα ανά νοικοκυριό\n",
    "# για κάθε COMM, ZIP Code.\n",
    "# Κάνουμε group by με το COMM για να βρούμε το συνολικό\n",
    "# εισόδημα και τον συνολικό πληθυσμό ανά COMM.\n",
    "result = income_data.hint(\"SHUFFLE_REPLICATE_NL\") \\\n",
    "            .join(blocks_data, col(\"ZCTA10\") == col(\"Zip Code\")) \\\n",
    "            .groupBy(\"COMM\") \\\n",
    "            .agg(\n",
    "                sum(\"Population\").alias(\"Population\"),\n",
    "                sum(col(\"Estimated Median Income\") * col(\"Households\")).alias(\"Total Income\"),\n",
    "                ST_Union_Aggr(\"geometry\").alias(\"geometry\")\n",
    "            ) \\\n",
    "            .select(\"COMM\", \"Population\", \"Total Income\", \"geometry\")\n",
    "\n",
    "\n",
    "# Τέλος, κάνουμε join με τα εγκλήματα με βάση το geometry, δηλαδή\n",
    "# το POINT του εγκλήματος βρίσκεται εντός του POLYGON της περιοχής.\n",
    "# Κάνουμε group by με το COMM (και τα Population, Total Income τα οποία είναι\n",
    "# ίδια ανά εγγραφή) και υπολογίζουμε το συνολικό πλήθος των εγκλημάτων.\n",
    "# Μετά διαιρούμε με τον πληθυσμό για να βρούμε το εισόδημα ανά άτομο\n",
    "# και το πλήθος εγκλημάτων ανά άτομο.\n",
    "result = crime_data \\\n",
    "            .join(\n",
    "                result.hint(\"BROADCAST\"),\n",
    "                ST_Within(col(\"geom\"), col(\"geometry\"))\n",
    "            ) \\\n",
    "            .groupBy(\"COMM\", \"Population\", \"Total Income\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"Total Crimes\")\n",
    "            ) \\\n",
    "            .withColumn(\"Income per Person\", col(\"Total Income\") / col(\"Population\")) \\\n",
    "            .withColumn(\"Crimes per Person\", col(\"Total Crimes\") / col(\"Population\")) \\\n",
    "            .select(\"COMM\", \"Income per Person\", \"Crimes per Person\")\n",
    "\n",
    "result.show()\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for strategy \"SHUFFLE_REPLICATE_NL\": 30.92 seconds"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for strategy \\\"SHUFFLE_REPLICATE_NL\\\": {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (31)\n",
      "+- HashAggregate (30)\n",
      "   +- Exchange (29)\n",
      "      +- HashAggregate (28)\n",
      "         +- Project (27)\n",
      "            +- BroadcastIndexJoin (26)\n",
      "               :- Union (7)\n",
      "               :  :- Project (3)\n",
      "               :  :  +- Filter (2)\n",
      "               :  :     +- Scan csv  (1)\n",
      "               :  +- Project (6)\n",
      "               :     +- Filter (5)\n",
      "               :        +- Scan csv  (4)\n",
      "               +- SpatialIndex (25)\n",
      "                  +- Filter (24)\n",
      "                     +- ObjectHashAggregate (23)\n",
      "                        +- Exchange (22)\n",
      "                           +- ObjectHashAggregate (21)\n",
      "                              +- Project (20)\n",
      "                                 +- CartesianProduct Inner (19)\n",
      "                                    :- Project (10)\n",
      "                                    :  +- Filter (9)\n",
      "                                    :     +- Scan csv  (8)\n",
      "                                    +- ObjectHashAggregate (18)\n",
      "                                       +- Exchange (17)\n",
      "                                          +- ObjectHashAggregate (16)\n",
      "                                             +- Project (15)\n",
      "                                                +- Filter (14)\n",
      "                                                   +- Generate (13)\n",
      "                                                      +- Filter (12)\n",
      "                                                         +- Scan geojson  (11)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [2]: [LAT#74, LON#75]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(3) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#188]\n",
      "Input [2]: [LAT#74, LON#75]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [2]: [LAT#130, LON#131]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv]\n",
      "ReadSchema: struct<LAT:double,LON:double>\n",
      "\n",
      "(5) Filter\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "Condition : NOT ( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   = 0x120000000100000000000000000000000000000000000000)\n",
      "\n",
      "(6) Project\n",
      "Output [1]: [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#670]\n",
      "Input [2]: [LAT#130, LON#131]\n",
      "\n",
      "(7) Union\n",
      "\n",
      "(8) Scan csv \n",
      "Output [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv]\n",
      "PushedFilters: [IsNotNull(Zip Code)]\n",
      "ReadSchema: struct<Zip Code:string,Estimated Median Income:string>\n",
      "\n",
      "(9) Filter\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "Condition : isnotnull(Zip Code#24)\n",
      "\n",
      "(10) Project\n",
      "Output [2]: [Zip Code#24, cast(regexp_replace(Estimated Median Income#26, [$,], , 1) as float) AS Estimated Median Income#30]\n",
      "Input [2]: [Zip Code#24, Estimated Median Income#26]\n",
      "\n",
      "(11) Scan geojson \n",
      "Output [1]: [features#239]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson]\n",
      "PushedFilters: [IsNotNull(features)]\n",
      "ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CITY:string,CITYCOM:string,COMM:string,CT10:string,CT12:string,CTCB10:string,HD_2012:bigint,HD_NAME:string,HOUSING10:bigint,LA_FIP10:string,OBJECTID:bigint,POP_2010:bigint,PUMA10:string,SPA_2012:bigint,SPA_NAME:string,SUP_DIST:string,SUP_LABEL:string,ShapeSTArea:double,ShapeSTLength:double,ZCTA10:string>,type:string>>>\n",
      "\n",
      "(12) Filter\n",
      "Input [1]: [features#239]\n",
      "Condition : ((size(features#239, true) > 0) AND isnotnull(features#239))\n",
      "\n",
      "(13) Generate\n",
      "Input [1]: [features#239]\n",
      "Arguments: explode(features#239), false, [features#247]\n",
      "\n",
      "(14) Filter\n",
      "Input [1]: [features#247]\n",
      "Condition : ((((isnotnull(features#247.properties.CITY) AND isnotnull(features#247.properties.HOUSING10)) AND isnotnull(features#247.properties.POP_2010)) AND (((isnotnull(features#247.properties.COMM) AND (features#247.properties.CITY = Los Angeles)) AND (features#247.properties.HOUSING10 > 0)) AND (features#247.properties.POP_2010 > 0))) AND isnotnull(features#247.properties.ZCTA10))\n",
      "\n",
      "(15) Project\n",
      "Output [5]: [features#247.properties.COMM AS COMM#263, features#247.properties.HOUSING10 AS HOUSING10#269L, features#247.properties.POP_2010 AS POP_2010#272L, features#247.properties.ZCTA10 AS ZCTA10#280, features#247.geometry AS geometry#250]\n",
      "Input [1]: [features#247]\n",
      "\n",
      "(16) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, HOUSING10#269L, POP_2010#272L, ZCTA10#280, geometry#250]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [partial_sum(POP_2010#272L), partial_sum(HOUSING10#269L), partial_st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@1fbb0a0, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#456L, sum#458L, buf#460]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum#457L, sum#459L, buf#461]\n",
      "\n",
      "(17) Exchange\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#457L, sum#459L, buf#461]\n",
      "Arguments: hashpartitioning(COMM#263, ZCTA10#280, 1000), ENSURE_REQUIREMENTS, [plan_id=799]\n",
      "\n",
      "(18) ObjectHashAggregate\n",
      "Input [5]: [COMM#263, ZCTA10#280, sum#457L, sum#459L, buf#461]\n",
      "Keys [2]: [COMM#263, ZCTA10#280]\n",
      "Functions [3]: [sum(POP_2010#272L), sum(HOUSING10#269L), st_union_aggr(geometry#250, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@1fbb0a0, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(POP_2010#272L)#410L, sum(HOUSING10#269L)#412L, ST_Union_Aggr(geometry#250)#417]\n",
      "Results [5]: [COMM#263, ZCTA10#280, sum(POP_2010#272L)#410L AS Population#411L, sum(HOUSING10#269L)#412L AS Households#413L, ST_Union_Aggr(geometry#250)#417 AS geometry#418]\n",
      "\n",
      "(19) CartesianProduct\n",
      "Join type: Inner\n",
      "Join condition: (ZCTA10#280 = Zip Code#24)\n",
      "\n",
      "(20) Project\n",
      "Output [5]: [Estimated Median Income#30, COMM#263, Population#411L, Households#413L, geometry#418]\n",
      "Input [7]: [Zip Code#24, Estimated Median Income#30, COMM#263, ZCTA10#280, Population#411L, Households#413L, geometry#418]\n",
      "\n",
      "(21) ObjectHashAggregate\n",
      "Input [5]: [Estimated Median Income#30, COMM#263, Population#411L, Households#413L, geometry#418]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [partial_sum(Population#411L), partial_sum((Estimated Median Income#30 * cast(Households#413L as float))), partial_st_union_aggr(geometry#418, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@713e48ab, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum#619L, sum#621, buf#623]\n",
      "Results [4]: [COMM#263, sum#620L, sum#622, buf#624]\n",
      "\n",
      "(22) Exchange\n",
      "Input [4]: [COMM#263, sum#620L, sum#622, buf#624]\n",
      "Arguments: hashpartitioning(COMM#263, 1000), ENSURE_REQUIREMENTS, [plan_id=805]\n",
      "\n",
      "(23) ObjectHashAggregate\n",
      "Input [4]: [COMM#263, sum#620L, sum#622, buf#624]\n",
      "Keys [1]: [COMM#263]\n",
      "Functions [3]: [sum(Population#411L), sum((Estimated Median Income#30 * cast(Households#413L as float))), st_union_aggr(geometry#418, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@713e48ab, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)]\n",
      "Aggregate Attributes [3]: [sum(Population#411L)#532L, sum((Estimated Median Income#30 * cast(Households#413L as float)))#534, ST_Union_Aggr(geometry#418)#539]\n",
      "Results [4]: [COMM#263, sum(Population#411L)#532L AS Population#533L, sum((Estimated Median Income#30 * cast(Households#413L as float)))#534 AS Total Income#535, ST_Union_Aggr(geometry#418)#539 AS geometry#540]\n",
      "\n",
      "(24) Filter\n",
      "Input [4]: [COMM#263, Population#533L, Total Income#535, geometry#540]\n",
      "Condition : isnotnull(geometry#540)\n",
      "\n",
      "(25) SpatialIndex\n",
      "Arguments: geometry#540: geometry, RTREE, false, false\n",
      "\n",
      "(26) BroadcastIndexJoin\n",
      "Arguments: geom#188: geometry, RightSide, LeftSide, Inner, WITHIN\n",
      "\n",
      "(27) Project\n",
      "Output [3]: [COMM#263, Population#533L, Total Income#535]\n",
      "Input [5]: [geom#188, COMM#263, Population#533L, Total Income#535, geometry#540]\n",
      "\n",
      "(28) HashAggregate\n",
      "Input [3]: [COMM#263, Population#533L, Total Income#535]\n",
      "Keys [3]: [COMM#263, Population#533L, knownfloatingpointnormalized(normalizenanandzero(Total Income#535)) AS Total Income#535]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#617L]\n",
      "Results [4]: [COMM#263, Population#533L, Total Income#535, count#618L]\n",
      "\n",
      "(29) Exchange\n",
      "Input [4]: [COMM#263, Population#533L, Total Income#535, count#618L]\n",
      "Arguments: hashpartitioning(COMM#263, Population#533L, Total Income#535, 1000), ENSURE_REQUIREMENTS, [plan_id=813]\n",
      "\n",
      "(30) HashAggregate\n",
      "Input [4]: [COMM#263, Population#533L, Total Income#535, count#618L]\n",
      "Keys [3]: [COMM#263, Population#533L, Total Income#535]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#585L]\n",
      "Results [3]: [COMM#263, (Total Income#535 / cast(Population#533L as double)) AS Income per Person#591, (cast(count(1)#585L as double) / cast(Population#533L as double)) AS Crimes per Person#597]\n",
      "\n",
      "(31) AdaptiveSparkPlan\n",
      "Output [3]: [COMM#263, Income per Person#591, Crimes per Person#597]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "result.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Συμπεράσματα\n",
    "\n",
    "Οι χρόνοι που πετύχαμε χρησιμοποιώντας τις 4 δοθείσες στρατηγικές για το πρώτο join είναι:\n",
    "\n",
    "| Join Strategy | Time (seconds) |\n",
    "|---|---|\n",
    "| BROADCAST | 33.36 |\n",
    "| MERGE | 32.95 |\n",
    "| SHUFFLE HASH | 28.10 |\n",
    "| SHUFFLE REPLICATE NL | 27.93 |\n",
    "\n",
    "Η στρατηγική join που πετυχαίνει την καλύτερη επίδοση είναι η **SHUFFLE REPLICATE NL** (*Shuffle Replicate Nested Loop*) με χρόνο 27.93 δευτερόλεπτα, έχοντας μικρή διαφορά από τη **SHUFFLE HASH** (*Shuffle Hash Join*) με 28.10 δευτερόλεπτα.\n",
    "\n",
    "Στη συνέχεια δοκιμάσαμε να χρησιμοποιήσουμε για το γεωχωρικό join (*ST_Within*) το **Broadcast Index Join** αντί του **Range Join**:\n",
    "\n",
    "| 1st Join Strategy | 2nd Join Strategy | Time (seconds) |\n",
    "|---|---|---|\n",
    "| SHUFFLE HASH | RANGE JOIN | 28.10 |\n",
    "| SHUFFLE HASH | BROADCAST INDEX JOIN | 28.80 |\n",
    "| SHUFFLE REPLICATE NL | RANGE JOIN | 27.93 |\n",
    "| SHUFFLE REPLICATE NL | BROADCAST INDEX JOIN | 30.92 |\n",
    "\n",
    "Συνεπώς η καταλληλότερη στρατηγική για την περίπτωσή μας είναι η **SHUFFLE_REPLICATE_NL**, με τη χρήση του optimized **Range Join** του Sedona για το 2ο join."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "name": "SparkLab - Introduction to RDDs and DataFrames"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
